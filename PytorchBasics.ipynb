{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Basics\n",
    "### Numpy Warm-up\n",
    "Using Numpy to implement a two-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 34852612.22357218)\n",
      "(1, 35694358.175756715)\n",
      "(2, 37878873.03228342)\n",
      "(3, 34980440.468109846)\n",
      "(4, 25588806.024137624)\n",
      "(5, 14643300.504955878)\n",
      "(6, 7223539.461158253)\n",
      "(7, 3575965.653405032)\n",
      "(8, 2018251.4719189126)\n",
      "(9, 1338402.9990590406)\n",
      "(10, 1001717.688989993)\n",
      "(11, 803666.2111883124)\n",
      "(12, 668852.7589523678)\n",
      "(13, 567693.1468488292)\n",
      "(14, 487651.8463333079)\n",
      "(15, 422288.1113857226)\n",
      "(16, 367993.9256755011)\n",
      "(17, 322422.9346949797)\n",
      "(18, 283879.5294934307)\n",
      "(19, 251091.52396492354)\n",
      "(20, 223000.3578602472)\n",
      "(21, 198790.00200137822)\n",
      "(22, 177799.58778063627)\n",
      "(23, 159514.0107787868)\n",
      "(24, 143527.20571636345)\n",
      "(25, 129474.72515934348)\n",
      "(26, 117076.46564780973)\n",
      "(27, 106101.30446656005)\n",
      "(28, 96350.47069664171)\n",
      "(29, 87667.3216849142)\n",
      "(30, 79914.38487452635)\n",
      "(31, 72982.22872424212)\n",
      "(32, 66757.0309093316)\n",
      "(33, 61165.607510859736)\n",
      "(34, 56151.92730739089)\n",
      "(35, 51618.59900633756)\n",
      "(36, 47512.017326508794)\n",
      "(37, 43786.525988571804)\n",
      "(38, 40400.030979480805)\n",
      "(39, 37317.65114821485)\n",
      "(40, 34508.96092819225)\n",
      "(41, 31944.53071966046)\n",
      "(42, 29601.862429712484)\n",
      "(43, 27458.686110871204)\n",
      "(44, 25493.234763376626)\n",
      "(45, 23688.256707361248)\n",
      "(46, 22029.332485115545)\n",
      "(47, 20503.08727992561)\n",
      "(48, 19097.154401948417)\n",
      "(49, 17801.343627879072)\n",
      "(50, 16605.658734762983)\n",
      "(51, 15502.119447132281)\n",
      "(52, 14482.398134322442)\n",
      "(53, 13538.32622061959)\n",
      "(54, 12664.04276462624)\n",
      "(55, 11853.897417474565)\n",
      "(56, 11102.836444462107)\n",
      "(57, 10405.9240488488)\n",
      "(58, 9758.378703864244)\n",
      "(59, 9156.345861760823)\n",
      "(60, 8595.84604811866)\n",
      "(61, 8074.019858777738)\n",
      "(62, 7587.8243004792785)\n",
      "(63, 7134.668898826714)\n",
      "(64, 6712.081170519499)\n",
      "(65, 6317.445676294872)\n",
      "(66, 5948.637227061528)\n",
      "(67, 5603.999190004938)\n",
      "(68, 5281.7213089511315)\n",
      "(69, 4980.217349087516)\n",
      "(70, 4697.977725809855)\n",
      "(71, 4433.686501032669)\n",
      "(72, 4185.91907657319)\n",
      "(73, 3953.6417921913194)\n",
      "(74, 3735.7051460820317)\n",
      "(75, 3531.2039419541034)\n",
      "(76, 3339.2519066415375)\n",
      "(77, 3158.9725906910044)\n",
      "(78, 2989.725051440537)\n",
      "(79, 2830.647769663352)\n",
      "(80, 2681.0541286172383)\n",
      "(81, 2540.216755790185)\n",
      "(82, 2407.6553798774057)\n",
      "(83, 2282.82124841886)\n",
      "(84, 2165.1317293459033)\n",
      "(85, 2054.210999077881)\n",
      "(86, 1949.5768175954313)\n",
      "(87, 1850.8305965671148)\n",
      "(88, 1757.6244509672692)\n",
      "(89, 1669.6276513460696)\n",
      "(90, 1586.5229519694776)\n",
      "(91, 1507.9814868840713)\n",
      "(92, 1433.7740126084188)\n",
      "(93, 1363.5905567203686)\n",
      "(94, 1297.1668370416874)\n",
      "(95, 1234.3089557730455)\n",
      "(96, 1174.826296493742)\n",
      "(97, 1118.5087960208778)\n",
      "(98, 1065.15754282318)\n",
      "(99, 1014.61092768135)\n",
      "(100, 966.6834413723843)\n",
      "(101, 921.2410311984994)\n",
      "(102, 878.1476426398137)\n",
      "(103, 837.2868299148654)\n",
      "(104, 798.5116406991607)\n",
      "(105, 761.6890567477698)\n",
      "(106, 726.7361350290813)\n",
      "(107, 693.5426000494015)\n",
      "(108, 661.9987564342653)\n",
      "(109, 632.020130330133)\n",
      "(110, 603.5354030833596)\n",
      "(111, 576.4543732789182)\n",
      "(112, 550.6913805725493)\n",
      "(113, 526.1814587668559)\n",
      "(114, 502.8662797666609)\n",
      "(115, 480.67144672797644)\n",
      "(116, 459.5412174106454)\n",
      "(117, 439.4285505311174)\n",
      "(118, 420.2763736902418)\n",
      "(119, 402.01798877615875)\n",
      "(120, 384.63414323171116)\n",
      "(121, 368.0588877502107)\n",
      "(122, 352.25259911692547)\n",
      "(123, 337.1854212454283)\n",
      "(124, 322.8223416601938)\n",
      "(125, 309.1243152731961)\n",
      "(126, 296.04778912599824)\n",
      "(127, 283.5660201595184)\n",
      "(128, 271.6548010416135)\n",
      "(129, 260.2827120448674)\n",
      "(130, 249.4234878249158)\n",
      "(131, 239.05402792181945)\n",
      "(132, 229.15440493911808)\n",
      "(133, 219.69334830846412)\n",
      "(134, 210.6508443129751)\n",
      "(135, 202.00618340231088)\n",
      "(136, 193.74575548545226)\n",
      "(137, 185.84760164879984)\n",
      "(138, 178.29305568678848)\n",
      "(139, 171.07339942841995)\n",
      "(140, 164.16136652250492)\n",
      "(141, 157.54913684270227)\n",
      "(142, 151.22278732902743)\n",
      "(143, 145.16722759252542)\n",
      "(144, 139.3703657493091)\n",
      "(145, 133.8209610247295)\n",
      "(146, 128.51942381771414)\n",
      "(147, 123.43127421515365)\n",
      "(148, 118.55735897584292)\n",
      "(149, 113.8888918485203)\n",
      "(150, 109.4158074447765)\n",
      "(151, 105.13044097155534)\n",
      "(152, 101.02382687783569)\n",
      "(153, 97.08990472293677)\n",
      "(154, 93.31742749173728)\n",
      "(155, 89.69957220634194)\n",
      "(156, 86.23095335510311)\n",
      "(157, 82.90461010215762)\n",
      "(158, 79.71587680566685)\n",
      "(159, 76.65581239289067)\n",
      "(160, 73.7211866106812)\n",
      "(161, 70.90446356684893)\n",
      "(162, 68.20202266735544)\n",
      "(163, 65.60813251192363)\n",
      "(164, 63.1180704267142)\n",
      "(165, 60.728222325676995)\n",
      "(166, 58.43356553209372)\n",
      "(167, 56.23157792433093)\n",
      "(168, 54.11648395065572)\n",
      "(169, 52.085208854000086)\n",
      "(170, 50.13547097449475)\n",
      "(171, 48.26079089834067)\n",
      "(172, 46.459860824802284)\n",
      "(173, 44.729783972739725)\n",
      "(174, 43.06861095972272)\n",
      "(175, 41.47181570071911)\n",
      "(176, 39.93673957913557)\n",
      "(177, 38.46113841741101)\n",
      "(178, 37.04257364331398)\n",
      "(179, 35.6790131169287)\n",
      "(180, 34.368215390784485)\n",
      "(181, 33.109147034280234)\n",
      "(182, 31.897662341617348)\n",
      "(183, 30.732179055733756)\n",
      "(184, 29.611002103156174)\n",
      "(185, 28.53251108213594)\n",
      "(186, 27.494946108170964)\n",
      "(187, 26.498622808124455)\n",
      "(188, 25.544999441673074)\n",
      "(189, 24.626707698801617)\n",
      "(190, 23.74266556274561)\n",
      "(191, 22.891787543941508)\n",
      "(192, 22.072800306311066)\n",
      "(193, 21.284857841391656)\n",
      "(194, 20.525939883691418)\n",
      "(195, 19.795690652173693)\n",
      "(196, 19.091810361029783)\n",
      "(197, 18.41390028627979)\n",
      "(198, 17.76115247596872)\n",
      "(199, 17.132343096269352)\n",
      "(200, 16.526680475781514)\n",
      "(201, 15.943648552797614)\n",
      "(202, 15.381749642318518)\n",
      "(203, 14.840302588783619)\n",
      "(204, 14.318963951696121)\n",
      "(205, 13.816188047245126)\n",
      "(206, 13.331804838588344)\n",
      "(207, 12.864967257013953)\n",
      "(208, 12.41530794629452)\n",
      "(209, 11.98161365518844)\n",
      "(210, 11.563637770707036)\n",
      "(211, 11.160750717606271)\n",
      "(212, 10.772424207781034)\n",
      "(213, 10.397946046563634)\n",
      "(214, 10.037104235802703)\n",
      "(215, 9.689315682718448)\n",
      "(216, 9.35362936044845)\n",
      "(217, 9.030017890143903)\n",
      "(218, 8.71800133187691)\n",
      "(219, 8.416981299497664)\n",
      "(220, 8.126767547253314)\n",
      "(221, 7.846935480605966)\n",
      "(222, 7.576892580047745)\n",
      "(223, 7.316405660959546)\n",
      "(224, 7.065095236178166)\n",
      "(225, 6.822831041501887)\n",
      "(226, 6.589043758313907)\n",
      "(227, 6.363560867928175)\n",
      "(228, 6.145926149470586)\n",
      "(229, 5.935919899936948)\n",
      "(230, 5.733219517167817)\n",
      "(231, 5.537612666139422)\n",
      "(232, 5.348905885194529)\n",
      "(233, 5.166824376580712)\n",
      "(234, 4.991066819865445)\n",
      "(235, 4.821413624705508)\n",
      "(236, 4.657752976221694)\n",
      "(237, 4.499729493834099)\n",
      "(238, 4.3471799487426654)\n",
      "(239, 4.199990882005087)\n",
      "(240, 4.057851748271981)\n",
      "(241, 3.920607008980973)\n",
      "(242, 3.7881308328826506)\n",
      "(243, 3.6601945772075126)\n",
      "(244, 3.5366839359026194)\n",
      "(245, 3.4174657735211302)\n",
      "(246, 3.302416297105073)\n",
      "(247, 3.1912622894390785)\n",
      "(248, 3.083900467761445)\n",
      "(249, 2.9802560872264565)\n",
      "(250, 2.8801366592200526)\n",
      "(251, 2.783489317993154)\n",
      "(252, 2.6901263045056485)\n",
      "(253, 2.5999312681965074)\n",
      "(254, 2.5128206739905896)\n",
      "(255, 2.4287034245275008)\n",
      "(256, 2.347464193455857)\n",
      "(257, 2.269054669625862)\n",
      "(258, 2.19324733563517)\n",
      "(259, 2.120016394596463)\n",
      "(260, 2.049281481605861)\n",
      "(261, 1.9809268381292897)\n",
      "(262, 1.9149007030627123)\n",
      "(263, 1.851143691465296)\n",
      "(264, 1.7895235149673856)\n",
      "(265, 1.7299766437679645)\n",
      "(266, 1.672449935043982)\n",
      "(267, 1.6168874733343144)\n",
      "(268, 1.5632020221014395)\n",
      "(269, 1.5113114094514668)\n",
      "(270, 1.46116176278876)\n",
      "(271, 1.4127107631029932)\n",
      "(272, 1.3658980179473703)\n",
      "(273, 1.3206419465492716)\n",
      "(274, 1.2769268643118856)\n",
      "(275, 1.2346672041495887)\n",
      "(276, 1.193832411636305)\n",
      "(277, 1.1543782196583345)\n",
      "(278, 1.116229517006731)\n",
      "(279, 1.0793652070647877)\n",
      "(280, 1.043741531024635)\n",
      "(281, 1.0092939781812307)\n",
      "(282, 0.9759989775654507)\n",
      "(283, 0.943826969456085)\n",
      "(284, 0.9127361629440068)\n",
      "(285, 0.8826784037617312)\n",
      "(286, 0.853612937721549)\n",
      "(287, 0.8255305172113654)\n",
      "(288, 0.7983751773997925)\n",
      "(289, 0.77212334465154)\n",
      "(290, 0.7467573215819193)\n",
      "(291, 0.7222216601336434)\n",
      "(292, 0.6985024386671345)\n",
      "(293, 0.6755661881086854)\n",
      "(294, 0.6533920812758234)\n",
      "(295, 0.6319645523553484)\n",
      "(296, 0.6112483475939645)\n",
      "(297, 0.5912205193491916)\n",
      "(298, 0.5718455440122925)\n",
      "(299, 0.5531150421221549)\n",
      "(300, 0.5350059228762785)\n",
      "(301, 0.5174936895499227)\n",
      "(302, 0.5005587540263928)\n",
      "(303, 0.48418685969124514)\n",
      "(304, 0.46835303594459676)\n",
      "(305, 0.4530446552098558)\n",
      "(306, 0.43824336117964385)\n",
      "(307, 0.4239329878206938)\n",
      "(308, 0.41008726197933837)\n",
      "(309, 0.39669924597594997)\n",
      "(310, 0.38376070045341726)\n",
      "(311, 0.3712401972423382)\n",
      "(312, 0.35913024106826136)\n",
      "(313, 0.34741919812029554)\n",
      "(314, 0.3360950055426538)\n",
      "(315, 0.3251453490356949)\n",
      "(316, 0.3145538614121331)\n",
      "(317, 0.3043133476506844)\n",
      "(318, 0.29440521758880156)\n",
      "(319, 0.2848208480952314)\n",
      "(320, 0.2755557577022286)\n",
      "(321, 0.2665916683992814)\n",
      "(322, 0.2579225544436585)\n",
      "(323, 0.24953540819764763)\n",
      "(324, 0.24142715809160115)\n",
      "(325, 0.2335832836224403)\n",
      "(326, 0.22599583004268975)\n",
      "(327, 0.2186560244964361)\n",
      "(328, 0.2115554013809945)\n",
      "(329, 0.20468776151086804)\n",
      "(330, 0.1980441264420015)\n",
      "(331, 0.19161763482676064)\n",
      "(332, 0.18540026591423914)\n",
      "(333, 0.17938592670461112)\n",
      "(334, 0.17357077620203493)\n",
      "(335, 0.16794329596584812)\n",
      "(336, 0.16250153868415684)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 0.15723526221488454)\n",
      "(338, 0.1521413960656664)\n",
      "(339, 0.14721523269500225)\n",
      "(340, 0.14244745165704895)\n",
      "(341, 0.13783516846711522)\n",
      "(342, 0.13337259565976767)\n",
      "(343, 0.1290565781867253)\n",
      "(344, 0.12488055753646082)\n",
      "(345, 0.12084075118411708)\n",
      "(346, 0.11693262575453021)\n",
      "(347, 0.11315146217269388)\n",
      "(348, 0.10949344267361355)\n",
      "(349, 0.10595326602896268)\n",
      "(350, 0.10252826582021153)\n",
      "(351, 0.09921526115096013)\n",
      "(352, 0.09601010575848781)\n",
      "(353, 0.09290862704584328)\n",
      "(354, 0.08990866569321997)\n",
      "(355, 0.08700676626325163)\n",
      "(356, 0.08419791160906691)\n",
      "(357, 0.08148015913355)\n",
      "(358, 0.07884990287085426)\n",
      "(359, 0.07630494933631345)\n",
      "(360, 0.07384311403512042)\n",
      "(361, 0.07146107207605956)\n",
      "(362, 0.06915569510319301)\n",
      "(363, 0.06692515940081419)\n",
      "(364, 0.06476747912328476)\n",
      "(365, 0.06268012179206714)\n",
      "(366, 0.06065935402961334)\n",
      "(367, 0.05870421089356504)\n",
      "(368, 0.05681210578319247)\n",
      "(369, 0.05498182537241103)\n",
      "(370, 0.053210476346396154)\n",
      "(371, 0.05149681641830071)\n",
      "(372, 0.049838045139056986)\n",
      "(373, 0.04823339777139101)\n",
      "(374, 0.04668077747858558)\n",
      "(375, 0.04517773895118545)\n",
      "(376, 0.04372339116891197)\n",
      "(377, 0.04231624744925991)\n",
      "(378, 0.0409542953350375)\n",
      "(379, 0.0396363611281577)\n",
      "(380, 0.038360988426083535)\n",
      "(381, 0.03712696045977816)\n",
      "(382, 0.03593270458543394)\n",
      "(383, 0.03477727364125403)\n",
      "(384, 0.033659119427250785)\n",
      "(385, 0.03257699285290665)\n",
      "(386, 0.03152968146554603)\n",
      "(387, 0.030515904793390655)\n",
      "(388, 0.029535394556240886)\n",
      "(389, 0.028586294803995778)\n",
      "(390, 0.02766749717314329)\n",
      "(391, 0.0267784029205152)\n",
      "(392, 0.025918090385982553)\n",
      "(393, 0.025085714405720518)\n",
      "(394, 0.02427986359187632)\n",
      "(395, 0.023499965142892484)\n",
      "(396, 0.0227451517019045)\n",
      "(397, 0.022014866038560593)\n",
      "(398, 0.02130791161711327)\n",
      "(399, 0.020623684540937076)\n",
      "(400, 0.019961566185148143)\n",
      "(401, 0.019320890931439228)\n",
      "(402, 0.018700891627777466)\n",
      "(403, 0.0181006572817943)\n",
      "(404, 0.017519776304517114)\n",
      "(405, 0.016957701718940357)\n",
      "(406, 0.016413720378118838)\n",
      "(407, 0.01588709525346147)\n",
      "(408, 0.015377418194541585)\n",
      "(409, 0.014884190505610849)\n",
      "(410, 0.01440675920081829)\n",
      "(411, 0.01394475089246222)\n",
      "(412, 0.013497661787698132)\n",
      "(413, 0.013064845741942815)\n",
      "(414, 0.012645993333226984)\n",
      "(415, 0.012240541166563229)\n",
      "(416, 0.01184819284104756)\n",
      "(417, 0.011468317802684846)\n",
      "(418, 0.011100708075572518)\n",
      "(419, 0.010744936830354244)\n",
      "(420, 0.010400643990782586)\n",
      "(421, 0.01006739719844024)\n",
      "(422, 0.00974475866303668)\n",
      "(423, 0.009432563724516518)\n",
      "(424, 0.00913046712725985)\n",
      "(425, 0.00883792787646184)\n",
      "(426, 0.00855477449563598)\n",
      "(427, 0.008280802179562652)\n",
      "(428, 0.008015538559364335)\n",
      "(429, 0.007758819951806473)\n",
      "(430, 0.007510432204268147)\n",
      "(431, 0.007269922798344662)\n",
      "(432, 0.0070370935318590545)\n",
      "(433, 0.0068117727263878)\n",
      "(434, 0.006593681386659616)\n",
      "(435, 0.00638256549219249)\n",
      "(436, 0.006178245727730419)\n",
      "(437, 0.005980457080579637)\n",
      "(438, 0.005789038064946413)\n",
      "(439, 0.00560380714453833)\n",
      "(440, 0.005424425675447338)\n",
      "(441, 0.005250835224501156)\n",
      "(442, 0.005082849401029818)\n",
      "(443, 0.00492023118571314)\n",
      "(444, 0.004762806587390932)\n",
      "(445, 0.0046104314730486436)\n",
      "(446, 0.004462910145357991)\n",
      "(447, 0.004320120295317029)\n",
      "(448, 0.004181972018727057)\n",
      "(449, 0.004048186784936757)\n",
      "(450, 0.003918691119520561)\n",
      "(451, 0.0037933433149825785)\n",
      "(452, 0.0036720139199395508)\n",
      "(453, 0.003554570649607491)\n",
      "(454, 0.00344088002114823)\n",
      "(455, 0.003330853373794034)\n",
      "(456, 0.0032243398659625878)\n",
      "(457, 0.0031212577257917857)\n",
      "(458, 0.003021457500221062)\n",
      "(459, 0.0029248586672714883)\n",
      "(460, 0.002831335320727573)\n",
      "(461, 0.002740834115699826)\n",
      "(462, 0.0026532376415965286)\n",
      "(463, 0.0025684112337292456)\n",
      "(464, 0.0024863038274791044)\n",
      "(465, 0.0024068385629853715)\n",
      "(466, 0.00232992777890123)\n",
      "(467, 0.002255458831609269)\n",
      "(468, 0.0021833734834998822)\n",
      "(469, 0.0021135928728554824)\n",
      "(470, 0.0020460466253560366)\n",
      "(471, 0.001980654404801759)\n",
      "(472, 0.0019173783002065744)\n",
      "(473, 0.0018561184059489135)\n",
      "(474, 0.0017968047130477716)\n",
      "(475, 0.0017394212022332398)\n",
      "(476, 0.0016838491482829687)\n",
      "(477, 0.0016300432154145237)\n",
      "(478, 0.0015779727293673924)\n",
      "(479, 0.0015275617778716204)\n",
      "(480, 0.0014787589982494438)\n",
      "(481, 0.0014315419355043285)\n",
      "(482, 0.0013858155027311571)\n",
      "(483, 0.0013415447444463044)\n",
      "(484, 0.0012987104864801578)\n",
      "(485, 0.0012572333404065055)\n",
      "(486, 0.001217074701296431)\n",
      "(487, 0.0011782049688338514)\n",
      "(488, 0.0011405792316426704)\n",
      "(489, 0.0011041513778609591)\n",
      "(490, 0.0010688914902538794)\n",
      "(491, 0.0010347582153971125)\n",
      "(492, 0.0010017162275382768)\n",
      "(493, 0.0009697398640258497)\n",
      "(494, 0.0009387741690723078)\n",
      "(495, 0.0009088015386392349)\n",
      "(496, 0.0008797850134015452)\n",
      "(497, 0.0008516938695727307)\n",
      "(498, 0.0008245055951505329)\n",
      "(499, 0.0007981802720215658)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N: batch size; D_in: input dimension\n",
    "# H: hidden dimension; D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(0, h)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y - y_pred).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Tensor\n",
    "\n",
    "Implemting a two-layer network with pytorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 31429802.0)\n",
      "(1, 29396530.0)\n",
      "(2, 32845976.0)\n",
      "(3, 35806252.0)\n",
      "(4, 33564848.0)\n",
      "(5, 24558668.0)\n",
      "(6, 14257989.0)\n",
      "(7, 7025729.5)\n",
      "(8, 3443714.75)\n",
      "(9, 1883885.375)\n",
      "(10, 1212178.375)\n",
      "(11, 889619.125)\n",
      "(12, 708059.9375)\n",
      "(13, 588472.8125)\n",
      "(14, 500231.4375)\n",
      "(15, 430676.0)\n",
      "(16, 373858.9375)\n",
      "(17, 326520.0625)\n",
      "(18, 286556.84375)\n",
      "(19, 252485.265625)\n",
      "(20, 223255.6875)\n",
      "(21, 198072.28125)\n",
      "(22, 176249.171875)\n",
      "(23, 157245.09375)\n",
      "(24, 140637.265625)\n",
      "(25, 126081.078125)\n",
      "(26, 113286.15625)\n",
      "(27, 101994.5078125)\n",
      "(28, 92006.1015625)\n",
      "(29, 83145.15625)\n",
      "(30, 75258.484375)\n",
      "(31, 68221.5703125)\n",
      "(32, 61929.73828125)\n",
      "(33, 56290.703125)\n",
      "(34, 51232.0390625)\n",
      "(35, 46679.72265625)\n",
      "(36, 42579.53515625)\n",
      "(37, 38880.48046875)\n",
      "(38, 35536.80859375)\n",
      "(39, 32513.37109375)\n",
      "(40, 29773.734375)\n",
      "(41, 27288.564453125)\n",
      "(42, 25033.09375)\n",
      "(43, 22982.80078125)\n",
      "(44, 21116.482421875)\n",
      "(45, 19415.56640625)\n",
      "(46, 17864.697265625)\n",
      "(47, 16448.609375)\n",
      "(48, 15154.025390625)\n",
      "(49, 13969.728515625)\n",
      "(50, 12885.7216796875)\n",
      "(51, 11893.1767578125)\n",
      "(52, 10982.7509765625)\n",
      "(53, 10147.447265625)\n",
      "(54, 9380.82421875)\n",
      "(55, 8676.4873046875)\n",
      "(56, 8029.0068359375)\n",
      "(57, 7433.27197265625)\n",
      "(58, 6885.0244140625)\n",
      "(59, 6381.13818359375)\n",
      "(60, 5917.9482421875)\n",
      "(61, 5490.60205078125)\n",
      "(62, 5096.44091796875)\n",
      "(63, 4732.41650390625)\n",
      "(64, 4396.21240234375)\n",
      "(65, 4085.510986328125)\n",
      "(66, 3798.220947265625)\n",
      "(67, 3532.4384765625)\n",
      "(68, 3286.491455078125)\n",
      "(69, 3058.762451171875)\n",
      "(70, 2847.791015625)\n",
      "(71, 2652.259521484375)\n",
      "(72, 2471.019287109375)\n",
      "(73, 2302.94970703125)\n",
      "(74, 2146.993896484375)\n",
      "(75, 2002.218017578125)\n",
      "(76, 1867.7471923828125)\n",
      "(77, 1742.90087890625)\n",
      "(78, 1626.8707275390625)\n",
      "(79, 1518.9599609375)\n",
      "(80, 1418.619140625)\n",
      "(81, 1325.2884521484375)\n",
      "(82, 1238.447998046875)\n",
      "(83, 1157.9959716796875)\n",
      "(84, 1083.107421875)\n",
      "(85, 1013.3403930664062)\n",
      "(86, 948.3297729492188)\n",
      "(87, 887.7381591796875)\n",
      "(88, 831.2164306640625)\n",
      "(89, 778.483642578125)\n",
      "(90, 729.2664794921875)\n",
      "(91, 683.320068359375)\n",
      "(92, 640.4315795898438)\n",
      "(93, 600.3635864257812)\n",
      "(94, 562.9344482421875)\n",
      "(95, 527.9439086914062)\n",
      "(96, 495.226806640625)\n",
      "(97, 464.64337158203125)\n",
      "(98, 436.03363037109375)\n",
      "(99, 409.2625732421875)\n",
      "(100, 384.21405029296875)\n",
      "(101, 360.7653503417969)\n",
      "(102, 338.8077392578125)\n",
      "(103, 318.2474365234375)\n",
      "(104, 298.9879455566406)\n",
      "(105, 280.9468994140625)\n",
      "(106, 264.0400085449219)\n",
      "(107, 248.1930389404297)\n",
      "(108, 233.34213256835938)\n",
      "(109, 219.41278076171875)\n",
      "(110, 206.34336853027344)\n",
      "(111, 194.082763671875)\n",
      "(112, 182.5785675048828)\n",
      "(113, 171.78126525878906)\n",
      "(114, 161.64784240722656)\n",
      "(115, 152.14097595214844)\n",
      "(116, 143.2065887451172)\n",
      "(117, 134.81787109375)\n",
      "(118, 126.93680572509766)\n",
      "(119, 119.53414154052734)\n",
      "(120, 112.57606506347656)\n",
      "(121, 106.03760528564453)\n",
      "(122, 99.89520263671875)\n",
      "(123, 94.1184310913086)\n",
      "(124, 88.68629455566406)\n",
      "(125, 83.57801055908203)\n",
      "(126, 78.77430725097656)\n",
      "(127, 74.2535171508789)\n",
      "(128, 70.00117492675781)\n",
      "(129, 66.000244140625)\n",
      "(130, 62.2344856262207)\n",
      "(131, 58.69075393676758)\n",
      "(132, 55.35320281982422)\n",
      "(133, 52.21112823486328)\n",
      "(134, 49.25408172607422)\n",
      "(135, 46.46854019165039)\n",
      "(136, 43.843292236328125)\n",
      "(137, 41.371768951416016)\n",
      "(138, 39.044403076171875)\n",
      "(139, 36.850582122802734)\n",
      "(140, 34.78453826904297)\n",
      "(141, 32.83608627319336)\n",
      "(142, 31.000185012817383)\n",
      "(143, 29.26873016357422)\n",
      "(144, 27.63646125793457)\n",
      "(145, 26.098155975341797)\n",
      "(146, 24.647052764892578)\n",
      "(147, 23.27904510498047)\n",
      "(148, 21.988636016845703)\n",
      "(149, 20.771032333374023)\n",
      "(150, 19.62224006652832)\n",
      "(151, 18.539043426513672)\n",
      "(152, 17.516521453857422)\n",
      "(153, 16.55239486694336)\n",
      "(154, 15.64183235168457)\n",
      "(155, 14.78276252746582)\n",
      "(156, 13.972290992736816)\n",
      "(157, 13.206893920898438)\n",
      "(158, 12.484243392944336)\n",
      "(159, 11.801749229431152)\n",
      "(160, 11.15733814239502)\n",
      "(161, 10.549341201782227)\n",
      "(162, 9.974658012390137)\n",
      "(163, 9.431876182556152)\n",
      "(164, 8.919421195983887)\n",
      "(165, 8.435250282287598)\n",
      "(166, 7.977830410003662)\n",
      "(167, 7.5457024574279785)\n",
      "(168, 7.137063980102539)\n",
      "(169, 6.751473426818848)\n",
      "(170, 6.387007713317871)\n",
      "(171, 6.04233980178833)\n",
      "(172, 5.716583251953125)\n",
      "(173, 5.409078598022461)\n",
      "(174, 5.118513107299805)\n",
      "(175, 4.843184947967529)\n",
      "(176, 4.583512306213379)\n",
      "(177, 4.337745189666748)\n",
      "(178, 4.105226516723633)\n",
      "(179, 3.885439872741699)\n",
      "(180, 3.677678108215332)\n",
      "(181, 3.4810283184051514)\n",
      "(182, 3.2953271865844727)\n",
      "(183, 3.1195156574249268)\n",
      "(184, 2.953319549560547)\n",
      "(185, 2.796039581298828)\n",
      "(186, 2.6473007202148438)\n",
      "(187, 2.5066545009613037)\n",
      "(188, 2.37341046333313)\n",
      "(189, 2.2475335597991943)\n",
      "(190, 2.128312587738037)\n",
      "(191, 2.015594959259033)\n",
      "(192, 1.9088963270187378)\n",
      "(193, 1.8080652952194214)\n",
      "(194, 1.7123664617538452)\n",
      "(195, 1.6218280792236328)\n",
      "(196, 1.5362598896026611)\n",
      "(197, 1.4552737474441528)\n",
      "(198, 1.3784080743789673)\n",
      "(199, 1.3058305978775024)\n",
      "(200, 1.2372031211853027)\n",
      "(201, 1.1720227003097534)\n",
      "(202, 1.1104750633239746)\n",
      "(203, 1.0521515607833862)\n",
      "(204, 0.9968467354774475)\n",
      "(205, 0.9446914196014404)\n",
      "(206, 0.8951265215873718)\n",
      "(207, 0.8482420444488525)\n",
      "(208, 0.8038440942764282)\n",
      "(209, 0.7617778778076172)\n",
      "(210, 0.7219900488853455)\n",
      "(211, 0.6842853426933289)\n",
      "(212, 0.6485024690628052)\n",
      "(213, 0.6145852208137512)\n",
      "(214, 0.5825787782669067)\n",
      "(215, 0.5522294044494629)\n",
      "(216, 0.5234445333480835)\n",
      "(217, 0.4962382912635803)\n",
      "(218, 0.4704035520553589)\n",
      "(219, 0.445951908826828)\n",
      "(220, 0.4227767586708069)\n",
      "(221, 0.40078386664390564)\n",
      "(222, 0.3800003230571747)\n",
      "(223, 0.36026620864868164)\n",
      "(224, 0.3416157364845276)\n",
      "(225, 0.32387658953666687)\n",
      "(226, 0.3071167469024658)\n",
      "(227, 0.2911894917488098)\n",
      "(228, 0.2761513292789459)\n",
      "(229, 0.26189669966697693)\n",
      "(230, 0.24835598468780518)\n",
      "(231, 0.235519140958786)\n",
      "(232, 0.22334089875221252)\n",
      "(233, 0.2118435502052307)\n",
      "(234, 0.20094142854213715)\n",
      "(235, 0.19055378437042236)\n",
      "(236, 0.18076619505882263)\n",
      "(237, 0.1714877188205719)\n",
      "(238, 0.16269247233867645)\n",
      "(239, 0.15434294939041138)\n",
      "(240, 0.14642515778541565)\n",
      "(241, 0.13890430331230164)\n",
      "(242, 0.13179150223731995)\n",
      "(243, 0.1250450611114502)\n",
      "(244, 0.11867813766002655)\n",
      "(245, 0.11258005350828171)\n",
      "(246, 0.10682160407304764)\n",
      "(247, 0.10136955976486206)\n",
      "(248, 0.09620773792266846)\n",
      "(249, 0.09130461513996124)\n",
      "(250, 0.08662842214107513)\n",
      "(251, 0.08218588680028915)\n",
      "(252, 0.07802106440067291)\n",
      "(253, 0.07406117022037506)\n",
      "(254, 0.07027148455381393)\n",
      "(255, 0.06670555472373962)\n",
      "(256, 0.06330829858779907)\n",
      "(257, 0.06009768322110176)\n",
      "(258, 0.05703461915254593)\n",
      "(259, 0.054149165749549866)\n",
      "(260, 0.05141289532184601)\n",
      "(261, 0.048800669610500336)\n",
      "(262, 0.04632231220602989)\n",
      "(263, 0.04396173730492592)\n",
      "(264, 0.04172796756029129)\n",
      "(265, 0.039610959589481354)\n",
      "(266, 0.03760998323559761)\n",
      "(267, 0.0357203371822834)\n",
      "(268, 0.033908337354660034)\n",
      "(269, 0.032187748700380325)\n",
      "(270, 0.030569860711693764)\n",
      "(271, 0.029031576588749886)\n",
      "(272, 0.02756781503558159)\n",
      "(273, 0.02618582919239998)\n",
      "(274, 0.024861300364136696)\n",
      "(275, 0.023617710918188095)\n",
      "(276, 0.02242477983236313)\n",
      "(277, 0.021296191960573196)\n",
      "(278, 0.02023567631840706)\n",
      "(279, 0.019224602729082108)\n",
      "(280, 0.018260348588228226)\n",
      "(281, 0.01734359748661518)\n",
      "(282, 0.01648559421300888)\n",
      "(283, 0.015662232413887978)\n",
      "(284, 0.014882072806358337)\n",
      "(285, 0.014152111485600471)\n",
      "(286, 0.013443371281027794)\n",
      "(287, 0.012779533863067627)\n",
      "(288, 0.01214921660721302)\n",
      "(289, 0.011538976803421974)\n",
      "(290, 0.010976316407322884)\n",
      "(291, 0.01043795794248581)\n",
      "(292, 0.009925696067512035)\n",
      "(293, 0.009435231797397137)\n",
      "(294, 0.00897944625467062)\n",
      "(295, 0.00853254646062851)\n",
      "(296, 0.00811945553869009)\n",
      "(297, 0.0077282278798520565)\n",
      "(298, 0.0073471968062222)\n",
      "(299, 0.00699402391910553)\n",
      "(300, 0.006654109340161085)\n",
      "(301, 0.0063367243856191635)\n",
      "(302, 0.0060285055078566074)\n",
      "(303, 0.005738766398280859)\n",
      "(304, 0.005467679817229509)\n",
      "(305, 0.0052069490775465965)\n",
      "(306, 0.004963207058608532)\n",
      "(307, 0.004729693289846182)\n",
      "(308, 0.004504631273448467)\n",
      "(309, 0.004288836382329464)\n",
      "(310, 0.00408979831263423)\n",
      "(311, 0.0039045396260917187)\n",
      "(312, 0.003725282149389386)\n",
      "(313, 0.003554917871952057)\n",
      "(314, 0.003391933161765337)\n",
      "(315, 0.0032381194178014994)\n",
      "(316, 0.0030909546185284853)\n",
      "(317, 0.0029528301674872637)\n",
      "(318, 0.0028231728356331587)\n",
      "(319, 0.0026947876904159784)\n",
      "(320, 0.002577617997303605)\n",
      "(321, 0.0024628096725791693)\n",
      "(322, 0.002353333169594407)\n",
      "(323, 0.00225217011757195)\n",
      "(324, 0.002157527254894376)\n",
      "(325, 0.002062578219920397)\n",
      "(326, 0.0019744064193218946)\n",
      "(327, 0.0018922274466603994)\n",
      "(328, 0.0018126512877643108)\n",
      "(329, 0.0017365154344588518)\n",
      "(330, 0.0016626124270260334)\n",
      "(331, 0.001594566972926259)\n",
      "(332, 0.0015285167610272765)\n",
      "(333, 0.0014673704281449318)\n",
      "(334, 0.001406315597705543)\n",
      "(335, 0.0013533806195482612)\n",
      "(336, 0.0012987989466637373)\n",
      "(337, 0.001246076775714755)\n",
      "(338, 0.001197214238345623)\n",
      "(339, 0.0011499287793412805)\n",
      "(340, 0.001107194577343762)\n",
      "(341, 0.0010640878463163972)\n",
      "(342, 0.001023513381369412)\n",
      "(343, 0.0009855588432401419)\n",
      "(344, 0.0009489675285294652)\n",
      "(345, 0.0009126114891842008)\n",
      "(346, 0.0008788739214651287)\n",
      "(347, 0.0008472844492644072)\n",
      "(348, 0.0008174970862455666)\n",
      "(349, 0.0007879173499532044)\n",
      "(350, 0.000759680406190455)\n",
      "(351, 0.0007324249017983675)\n",
      "(352, 0.0007078980561345816)\n",
      "(353, 0.0006834477535448968)\n",
      "(354, 0.000659897574223578)\n",
      "(355, 0.0006370694027282298)\n",
      "(356, 0.0006156583549454808)\n",
      "(357, 0.0005956560489721596)\n",
      "(358, 0.0005760226049460471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359, 0.00055618432816118)\n",
      "(360, 0.0005375434993766248)\n",
      "(361, 0.0005208392976783216)\n",
      "(362, 0.0005042726988904178)\n",
      "(363, 0.00048761782818473876)\n",
      "(364, 0.0004718530108220875)\n",
      "(365, 0.0004576892242766917)\n",
      "(366, 0.0004432285495568067)\n",
      "(367, 0.0004298833664506674)\n",
      "(368, 0.0004175696230959147)\n",
      "(369, 0.0004045344248879701)\n",
      "(370, 0.0003923288022633642)\n",
      "(371, 0.0003812439099419862)\n",
      "(372, 0.00036940016434527934)\n",
      "(373, 0.0003588602412492037)\n",
      "(374, 0.0003486204950604588)\n",
      "(375, 0.00033866986632347107)\n",
      "(376, 0.00032957489020191133)\n",
      "(377, 0.00031980720814317465)\n",
      "(378, 0.00031111238058656454)\n",
      "(379, 0.0003023817844223231)\n",
      "(380, 0.00029373663710430264)\n",
      "(381, 0.0002859430096577853)\n",
      "(382, 0.00027820796822197735)\n",
      "(383, 0.00027114481781609356)\n",
      "(384, 0.00026463018730282784)\n",
      "(385, 0.0002565123431850225)\n",
      "(386, 0.00024969613878056407)\n",
      "(387, 0.0002428637963021174)\n",
      "(388, 0.00023744440113659948)\n",
      "(389, 0.0002313067961949855)\n",
      "(390, 0.00022576918127015233)\n",
      "(391, 0.000219789901166223)\n",
      "(392, 0.00021475512767210603)\n",
      "(393, 0.0002098361583193764)\n",
      "(394, 0.0002045860601356253)\n",
      "(395, 0.0001998704537982121)\n",
      "(396, 0.0001955098268808797)\n",
      "(397, 0.00019072661234531552)\n",
      "(398, 0.0001864049700088799)\n",
      "(399, 0.00018114532576873899)\n",
      "(400, 0.00017719420429784805)\n",
      "(401, 0.00017319194739684463)\n",
      "(402, 0.00016906812379602343)\n",
      "(403, 0.00016582722309976816)\n",
      "(404, 0.000162035328685306)\n",
      "(405, 0.00015823205467313528)\n",
      "(406, 0.00015507232456002384)\n",
      "(407, 0.0001513814931968227)\n",
      "(408, 0.00014804334205109626)\n",
      "(409, 0.00014507074956782162)\n",
      "(410, 0.00014186307089403272)\n",
      "(411, 0.00013880390906706452)\n",
      "(412, 0.00013535117614082992)\n",
      "(413, 0.00013285150635056198)\n",
      "(414, 0.00013015573495067656)\n",
      "(415, 0.0001273454399779439)\n",
      "(416, 0.00012487403000704944)\n",
      "(417, 0.00012227399565745145)\n",
      "(418, 0.00011998655827483162)\n",
      "(419, 0.00011780985369114205)\n",
      "(420, 0.00011516892118379474)\n",
      "(421, 0.00011301863560220227)\n",
      "(422, 0.00011047212319681421)\n",
      "(423, 0.00010817880684044212)\n",
      "(424, 0.0001057124391081743)\n",
      "(425, 0.00010402304906165227)\n",
      "(426, 0.00010204582940787077)\n",
      "(427, 9.986765508074313e-05)\n",
      "(428, 9.822749416343868e-05)\n",
      "(429, 9.685396798886359e-05)\n",
      "(430, 9.503371984465048e-05)\n",
      "(431, 9.327233419753611e-05)\n",
      "(432, 9.174620936391875e-05)\n",
      "(433, 8.98858270375058e-05)\n",
      "(434, 8.8483102445025e-05)\n",
      "(435, 8.690560207469389e-05)\n",
      "(436, 8.519900438841432e-05)\n",
      "(437, 8.34631355246529e-05)\n",
      "(438, 8.220187010010704e-05)\n",
      "(439, 8.063633140409365e-05)\n",
      "(440, 7.901754725025967e-05)\n",
      "(441, 7.803189510013908e-05)\n",
      "(442, 7.668342732358724e-05)\n",
      "(443, 7.54958382458426e-05)\n",
      "(444, 7.401405309792608e-05)\n",
      "(445, 7.288358028745279e-05)\n",
      "(446, 7.212977652670816e-05)\n",
      "(447, 7.086413825163618e-05)\n",
      "(448, 6.934324483154342e-05)\n",
      "(449, 6.839851994300261e-05)\n",
      "(450, 6.711674359394237e-05)\n",
      "(451, 6.615163874812424e-05)\n",
      "(452, 6.531853432534263e-05)\n",
      "(453, 6.419115379685536e-05)\n",
      "(454, 6.310846220003441e-05)\n",
      "(455, 6.247960118344054e-05)\n",
      "(456, 6.165329978102818e-05)\n",
      "(457, 6.0350896092131734e-05)\n",
      "(458, 5.939267430221662e-05)\n",
      "(459, 5.816967677674256e-05)\n",
      "(460, 5.7534416555427015e-05)\n",
      "(461, 5.68155555811245e-05)\n",
      "(462, 5.5734206398483366e-05)\n",
      "(463, 5.479357787407935e-05)\n",
      "(464, 5.403178511187434e-05)\n",
      "(465, 5.327254984877072e-05)\n",
      "(466, 5.245765714789741e-05)\n",
      "(467, 5.182791210245341e-05)\n",
      "(468, 5.1080955017823726e-05)\n",
      "(469, 5.013484042137861e-05)\n",
      "(470, 4.9610611313255504e-05)\n",
      "(471, 4.918900958728045e-05)\n",
      "(472, 4.8342022637370974e-05)\n",
      "(473, 4.752319728140719e-05)\n",
      "(474, 4.686402462539263e-05)\n",
      "(475, 4.6275094064185396e-05)\n",
      "(476, 4.582815381581895e-05)\n",
      "(477, 4.510670623858459e-05)\n",
      "(478, 4.4640019041253254e-05)\n",
      "(479, 4.422412530402653e-05)\n",
      "(480, 4.3631240259855986e-05)\n",
      "(481, 4.319290746934712e-05)\n",
      "(482, 4.260772038833238e-05)\n",
      "(483, 4.205427831038833e-05)\n",
      "(484, 4.162985715083778e-05)\n",
      "(485, 4.114067633054219e-05)\n",
      "(486, 4.067771806148812e-05)\n",
      "(487, 4.0255697967950255e-05)\n",
      "(488, 3.9881229895399883e-05)\n",
      "(489, 3.9365870179608464e-05)\n",
      "(490, 3.883855606545694e-05)\n",
      "(491, 3.836303221760318e-05)\n",
      "(492, 3.793988798861392e-05)\n",
      "(493, 3.743554771062918e-05)\n",
      "(494, 3.703912443597801e-05)\n",
      "(495, 3.6703728255815804e-05)\n",
      "(496, 3.60587946488522e-05)\n",
      "(497, 3.559971082722768e-05)\n",
      "(498, 3.5274686524644494e-05)\n",
      "(499, 3.472469688858837e-05)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:0') # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch size; D_in: input dimension\n",
    "# H: hidden dimension; D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # Update weights using gradient decent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Autograd\n",
    "Each Tensor represents a node in a computational graph. If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value.\n",
    "Implementing a two-layer network with pytorch autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 30020064.0)\n",
      "(1, 26504782.0)\n",
      "(2, 30436430.0)\n",
      "(3, 37108648.0)\n",
      "(4, 40371504.0)\n",
      "(5, 34686188.0)\n",
      "(6, 21885936.0)\n",
      "(7, 10624503.0)\n",
      "(8, 4584433.0)\n",
      "(9, 2153656.0)\n",
      "(10, 1230077.875)\n",
      "(11, 847742.375)\n",
      "(12, 656420.375)\n",
      "(13, 539228.1875)\n",
      "(14, 455722.6875)\n",
      "(15, 390761.78125)\n",
      "(16, 338234.46875)\n",
      "(17, 294765.0)\n",
      "(18, 258297.359375)\n",
      "(19, 227412.875)\n",
      "(20, 201121.5)\n",
      "(21, 178555.890625)\n",
      "(22, 159081.875)\n",
      "(23, 142210.0625)\n",
      "(24, 127506.578125)\n",
      "(25, 114632.2890625)\n",
      "(26, 103324.2421875)\n",
      "(27, 93346.8125)\n",
      "(28, 84529.03125)\n",
      "(29, 76704.5078125)\n",
      "(30, 69744.4375)\n",
      "(31, 63535.3671875)\n",
      "(32, 57975.34765625)\n",
      "(33, 52993.1171875)\n",
      "(34, 48516.625)\n",
      "(35, 44483.265625)\n",
      "(36, 40842.9921875)\n",
      "(37, 37549.87109375)\n",
      "(38, 34567.375)\n",
      "(39, 31862.5390625)\n",
      "(40, 29403.9453125)\n",
      "(41, 27166.943359375)\n",
      "(42, 25126.09375)\n",
      "(43, 23262.287109375)\n",
      "(44, 21557.369140625)\n",
      "(45, 19995.703125)\n",
      "(46, 18563.84765625)\n",
      "(47, 17249.228515625)\n",
      "(48, 16041.212890625)\n",
      "(49, 14930.12890625)\n",
      "(50, 13906.5888671875)\n",
      "(51, 12962.7412109375)\n",
      "(52, 12091.20703125)\n",
      "(53, 11285.9677734375)\n",
      "(54, 10541.5419921875)\n",
      "(55, 9852.412109375)\n",
      "(56, 9213.9296875)\n",
      "(57, 8621.974609375)\n",
      "(58, 8072.6982421875)\n",
      "(59, 7562.646484375)\n",
      "(60, 7088.77734375)\n",
      "(61, 6648.01806640625)\n",
      "(62, 6237.7802734375)\n",
      "(63, 5856.1591796875)\n",
      "(64, 5500.7841796875)\n",
      "(65, 5169.3681640625)\n",
      "(66, 4860.01171875)\n",
      "(67, 4571.1171875)\n",
      "(68, 4301.0791015625)\n",
      "(69, 4048.56494140625)\n",
      "(70, 3812.314208984375)\n",
      "(71, 3591.18798828125)\n",
      "(72, 3384.083740234375)\n",
      "(73, 3190.035888671875)\n",
      "(74, 3008.111572265625)\n",
      "(75, 2837.550048828125)\n",
      "(76, 2677.44482421875)\n",
      "(77, 2527.153076171875)\n",
      "(78, 2386.010009765625)\n",
      "(79, 2253.38916015625)\n",
      "(80, 2128.77099609375)\n",
      "(81, 2011.5635986328125)\n",
      "(82, 1901.2740478515625)\n",
      "(83, 1797.5087890625)\n",
      "(84, 1699.81103515625)\n",
      "(85, 1607.832763671875)\n",
      "(86, 1521.184814453125)\n",
      "(87, 1439.5162353515625)\n",
      "(88, 1362.539794921875)\n",
      "(89, 1289.9405517578125)\n",
      "(90, 1221.46533203125)\n",
      "(91, 1156.884521484375)\n",
      "(92, 1095.912353515625)\n",
      "(93, 1038.348388671875)\n",
      "(94, 984.0010375976562)\n",
      "(95, 932.675537109375)\n",
      "(96, 884.177001953125)\n",
      "(97, 838.34033203125)\n",
      "(98, 795.0147094726562)\n",
      "(99, 754.051513671875)\n",
      "(100, 715.3184204101562)\n",
      "(101, 678.6672973632812)\n",
      "(102, 644.0023803710938)\n",
      "(103, 611.1952514648438)\n",
      "(104, 580.1398315429688)\n",
      "(105, 550.735107421875)\n",
      "(106, 522.8898315429688)\n",
      "(107, 496.5183410644531)\n",
      "(108, 471.5368347167969)\n",
      "(109, 447.8751220703125)\n",
      "(110, 425.4475402832031)\n",
      "(111, 404.191650390625)\n",
      "(112, 384.0399169921875)\n",
      "(113, 364.9386291503906)\n",
      "(114, 346.82672119140625)\n",
      "(115, 329.66204833984375)\n",
      "(116, 313.3868713378906)\n",
      "(117, 297.9480285644531)\n",
      "(118, 283.2956237792969)\n",
      "(119, 269.3908996582031)\n",
      "(120, 256.19781494140625)\n",
      "(121, 243.66836547851562)\n",
      "(122, 231.77557373046875)\n",
      "(123, 220.48231506347656)\n",
      "(124, 209.75665283203125)\n",
      "(125, 199.57302856445312)\n",
      "(126, 189.89988708496094)\n",
      "(127, 180.70816040039062)\n",
      "(128, 171.97525024414062)\n",
      "(129, 163.6775665283203)\n",
      "(130, 155.79226684570312)\n",
      "(131, 148.29640197753906)\n",
      "(132, 141.17347717285156)\n",
      "(133, 134.40182495117188)\n",
      "(134, 127.96372985839844)\n",
      "(135, 121.84236145019531)\n",
      "(136, 116.02275848388672)\n",
      "(137, 110.48725891113281)\n",
      "(138, 105.2244873046875)\n",
      "(139, 100.21910095214844)\n",
      "(140, 95.46024322509766)\n",
      "(141, 90.93211364746094)\n",
      "(142, 86.62376403808594)\n",
      "(143, 82.52403259277344)\n",
      "(144, 78.6233139038086)\n",
      "(145, 74.91213989257812)\n",
      "(146, 71.3801498413086)\n",
      "(147, 68.018310546875)\n",
      "(148, 64.81768798828125)\n",
      "(149, 61.771358489990234)\n",
      "(150, 58.8711051940918)\n",
      "(151, 56.10969924926758)\n",
      "(152, 53.48076248168945)\n",
      "(153, 50.97751235961914)\n",
      "(154, 48.593421936035156)\n",
      "(155, 46.323081970214844)\n",
      "(156, 44.16044235229492)\n",
      "(157, 42.10200119018555)\n",
      "(158, 40.140892028808594)\n",
      "(159, 38.272491455078125)\n",
      "(160, 36.49236297607422)\n",
      "(161, 34.79692459106445)\n",
      "(162, 33.1815071105957)\n",
      "(163, 31.642393112182617)\n",
      "(164, 30.175939559936523)\n",
      "(165, 28.778276443481445)\n",
      "(166, 27.447044372558594)\n",
      "(167, 26.17791175842285)\n",
      "(168, 24.968767166137695)\n",
      "(169, 23.816282272338867)\n",
      "(170, 22.717674255371094)\n",
      "(171, 21.670408248901367)\n",
      "(172, 20.671926498413086)\n",
      "(173, 19.720947265625)\n",
      "(174, 18.814128875732422)\n",
      "(175, 17.949378967285156)\n",
      "(176, 17.12508773803711)\n",
      "(177, 16.339122772216797)\n",
      "(178, 15.589753150939941)\n",
      "(179, 14.875011444091797)\n",
      "(180, 14.19387149810791)\n",
      "(181, 13.544088363647461)\n",
      "(182, 12.924639701843262)\n",
      "(183, 12.333708763122559)\n",
      "(184, 11.770421981811523)\n",
      "(185, 11.233244895935059)\n",
      "(186, 10.720698356628418)\n",
      "(187, 10.231720924377441)\n",
      "(188, 9.765382766723633)\n",
      "(189, 9.320643424987793)\n",
      "(190, 8.896472930908203)\n",
      "(191, 8.491333961486816)\n",
      "(192, 8.10562801361084)\n",
      "(193, 7.737290859222412)\n",
      "(194, 7.386117458343506)\n",
      "(195, 7.050628185272217)\n",
      "(196, 6.73084020614624)\n",
      "(197, 6.425880432128906)\n",
      "(198, 6.134642601013184)\n",
      "(199, 5.85673713684082)\n",
      "(200, 5.591662883758545)\n",
      "(201, 5.33866024017334)\n",
      "(202, 5.097215175628662)\n",
      "(203, 4.866888523101807)\n",
      "(204, 4.647171974182129)\n",
      "(205, 4.437333106994629)\n",
      "(206, 4.236913681030273)\n",
      "(207, 4.04599666595459)\n",
      "(208, 3.8636207580566406)\n",
      "(209, 3.6895430088043213)\n",
      "(210, 3.5233912467956543)\n",
      "(211, 3.3646719455718994)\n",
      "(212, 3.2134175300598145)\n",
      "(213, 3.0689210891723633)\n",
      "(214, 2.930802345275879)\n",
      "(215, 2.799276828765869)\n",
      "(216, 2.6736960411071777)\n",
      "(217, 2.5535147190093994)\n",
      "(218, 2.438868761062622)\n",
      "(219, 2.3295466899871826)\n",
      "(220, 2.225229263305664)\n",
      "(221, 2.1255123615264893)\n",
      "(222, 2.030320882797241)\n",
      "(223, 1.9394423961639404)\n",
      "(224, 1.8526262044906616)\n",
      "(225, 1.7697488069534302)\n",
      "(226, 1.6906996965408325)\n",
      "(227, 1.6151176691055298)\n",
      "(228, 1.542982578277588)\n",
      "(229, 1.4740018844604492)\n",
      "(230, 1.4082469940185547)\n",
      "(231, 1.3455144166946411)\n",
      "(232, 1.2855380773544312)\n",
      "(233, 1.2281430959701538)\n",
      "(234, 1.1734278202056885)\n",
      "(235, 1.121157169342041)\n",
      "(236, 1.0712608098983765)\n",
      "(237, 1.0236351490020752)\n",
      "(238, 0.9780893325805664)\n",
      "(239, 0.9346583485603333)\n",
      "(240, 0.8929941058158875)\n",
      "(241, 0.8533740639686584)\n",
      "(242, 0.8154177069664001)\n",
      "(243, 0.7792536616325378)\n",
      "(244, 0.744634747505188)\n",
      "(245, 0.7115853428840637)\n",
      "(246, 0.6800750494003296)\n",
      "(247, 0.6499499678611755)\n",
      "(248, 0.6211708784103394)\n",
      "(249, 0.5936055183410645)\n",
      "(250, 0.5673224925994873)\n",
      "(251, 0.5421980023384094)\n",
      "(252, 0.5182113647460938)\n",
      "(253, 0.49530085921287537)\n",
      "(254, 0.47343727946281433)\n",
      "(255, 0.4524344503879547)\n",
      "(256, 0.4324800372123718)\n",
      "(257, 0.4134541153907776)\n",
      "(258, 0.39514318108558655)\n",
      "(259, 0.37769976258277893)\n",
      "(260, 0.36100032925605774)\n",
      "(261, 0.3451359272003174)\n",
      "(262, 0.3299032151699066)\n",
      "(263, 0.3153538703918457)\n",
      "(264, 0.3014816343784332)\n",
      "(265, 0.28819575905799866)\n",
      "(266, 0.2755075693130493)\n",
      "(267, 0.26338905096054077)\n",
      "(268, 0.25181716680526733)\n",
      "(269, 0.24072876572608948)\n",
      "(270, 0.23017950356006622)\n",
      "(271, 0.22004500031471252)\n",
      "(272, 0.21039409935474396)\n",
      "(273, 0.20117633044719696)\n",
      "(274, 0.1923152655363083)\n",
      "(275, 0.18386036157608032)\n",
      "(276, 0.17581744492053986)\n",
      "(277, 0.1681162416934967)\n",
      "(278, 0.16074824333190918)\n",
      "(279, 0.15375083684921265)\n",
      "(280, 0.1469758152961731)\n",
      "(281, 0.14055079221725464)\n",
      "(282, 0.1344069540500641)\n",
      "(283, 0.12852661311626434)\n",
      "(284, 0.12290602177381516)\n",
      "(285, 0.11752457171678543)\n",
      "(286, 0.11238262057304382)\n",
      "(287, 0.10748320817947388)\n",
      "(288, 0.10281703621149063)\n",
      "(289, 0.0983145534992218)\n",
      "(290, 0.09401947259902954)\n",
      "(291, 0.08991419523954391)\n",
      "(292, 0.0860077366232872)\n",
      "(293, 0.08227987587451935)\n",
      "(294, 0.07868260145187378)\n",
      "(295, 0.07527081668376923)\n",
      "(296, 0.07196313887834549)\n",
      "(297, 0.06883322447538376)\n",
      "(298, 0.06584513932466507)\n",
      "(299, 0.06300459802150726)\n",
      "(300, 0.060259390622377396)\n",
      "(301, 0.05763355642557144)\n",
      "(302, 0.05514567345380783)\n",
      "(303, 0.05275826156139374)\n",
      "(304, 0.050454072654247284)\n",
      "(305, 0.0482763797044754)\n",
      "(306, 0.046185124665498734)\n",
      "(307, 0.044192515313625336)\n",
      "(308, 0.042279861867427826)\n",
      "(309, 0.040456902235746384)\n",
      "(310, 0.03870250657200813)\n",
      "(311, 0.037031181156635284)\n",
      "(312, 0.03543058782815933)\n",
      "(313, 0.03390578180551529)\n",
      "(314, 0.032450415194034576)\n",
      "(315, 0.031045639887452126)\n",
      "(316, 0.02971712313592434)\n",
      "(317, 0.02844153344631195)\n",
      "(318, 0.027219239622354507)\n",
      "(319, 0.0260511115193367)\n",
      "(320, 0.024931462481617928)\n",
      "(321, 0.023870861157774925)\n",
      "(322, 0.022847013548016548)\n",
      "(323, 0.02186073362827301)\n",
      "(324, 0.02094162441790104)\n",
      "(325, 0.0200510136783123)\n",
      "(326, 0.019196785986423492)\n",
      "(327, 0.01837354339659214)\n",
      "(328, 0.017592430114746094)\n",
      "(329, 0.0168515183031559)\n",
      "(330, 0.016132818534970284)\n",
      "(331, 0.015457301400601864)\n",
      "(332, 0.014796080999076366)\n",
      "(333, 0.01416764035820961)\n",
      "(334, 0.013570984825491905)\n",
      "(335, 0.012999847531318665)\n",
      "(336, 0.012447554618120193)\n",
      "(337, 0.011920633725821972)\n",
      "(338, 0.011427298188209534)\n",
      "(339, 0.010943137109279633)\n",
      "(340, 0.010486972518265247)\n",
      "(341, 0.010050554759800434)\n",
      "(342, 0.00963317509740591)\n",
      "(343, 0.009226709604263306)\n",
      "(344, 0.008845211938023567)\n",
      "(345, 0.008477775380015373)\n",
      "(346, 0.00813023466616869)\n",
      "(347, 0.007798508275300264)\n",
      "(348, 0.007475479040294886)\n",
      "(349, 0.007171817123889923)\n",
      "(350, 0.006878523621708155)\n",
      "(351, 0.00659435847774148)\n",
      "(352, 0.006323224399238825)\n",
      "(353, 0.0060666087083518505)\n",
      "(354, 0.005822652019560337)\n",
      "(355, 0.00557852303609252)\n",
      "(356, 0.005355206783860922)\n",
      "(357, 0.005144502501934767)\n",
      "(358, 0.004934079945087433)\n",
      "(359, 0.00473690964281559)\n",
      "(360, 0.004549859091639519)\n",
      "(361, 0.004369297530502081)\n",
      "(362, 0.0041982391849160194)\n",
      "(363, 0.004028423223644495)\n",
      "(364, 0.0038696331903338432)\n",
      "(365, 0.003717661602422595)\n",
      "(366, 0.0035740092862397432)\n",
      "(367, 0.0034344051964581013)\n",
      "(368, 0.003300595795735717)\n",
      "(369, 0.0031734369695186615)\n",
      "(370, 0.0030543978791683912)\n",
      "(371, 0.002933676354587078)\n",
      "(372, 0.0028200389351695776)\n",
      "(373, 0.002714666770771146)\n",
      "(374, 0.00261488975957036)\n",
      "(375, 0.0025131041184067726)\n",
      "(376, 0.0024172174744307995)\n",
      "(377, 0.002327635185793042)\n",
      "(378, 0.0022405178751796484)\n",
      "(379, 0.0021583903580904007)\n",
      "(380, 0.0020806186366826296)\n",
      "(381, 0.0020022557582706213)\n",
      "(382, 0.0019288796465843916)\n",
      "(383, 0.00185969949234277)\n",
      "(384, 0.0017938821110874414)\n",
      "(385, 0.0017279471503570676)\n",
      "(386, 0.0016634694766253233)\n",
      "(387, 0.0016057241009548306)\n",
      "(388, 0.0015460895374417305)\n",
      "(389, 0.0014917306834831834)\n",
      "(390, 0.0014412084128707647)\n",
      "(391, 0.001389004522934556)\n",
      "(392, 0.0013403317425400019)\n",
      "(393, 0.0012964006746187806)\n",
      "(394, 0.0012511417735368013)\n",
      "(395, 0.0012090625241398811)\n",
      "(396, 0.0011666757054626942)\n",
      "(397, 0.001128006144426763)\n",
      "(398, 0.0010882728965952992)\n",
      "(399, 0.0010532011510804296)\n",
      "(400, 0.0010186547879129648)\n",
      "(401, 0.0009836646495386958)\n",
      "(402, 0.0009500083979219198)\n",
      "(403, 0.0009202727815136313)\n",
      "(404, 0.0008902839617803693)\n",
      "(405, 0.0008617015555500984)\n",
      "(406, 0.0008352447184734046)\n",
      "(407, 0.0008091701893135905)\n",
      "(408, 0.0007816906436346471)\n",
      "(409, 0.0007577327778562903)\n",
      "(410, 0.000734858273062855)\n",
      "(411, 0.0007116023916751146)\n",
      "(412, 0.0006895533879287541)\n",
      "(413, 0.0006689868168905377)\n",
      "(414, 0.0006485508056357503)\n",
      "(415, 0.0006295645725913346)\n",
      "(416, 0.000610409420914948)\n",
      "(417, 0.0005926243611611426)\n",
      "(418, 0.0005759106716141105)\n",
      "(419, 0.0005584894679486752)\n",
      "(420, 0.000542850757483393)\n",
      "(421, 0.0005262407357804477)\n",
      "(422, 0.0005107183242216706)\n",
      "(423, 0.0004966284614056349)\n",
      "(424, 0.00048249762039631605)\n",
      "(425, 0.00046879120054654777)\n",
      "(426, 0.00045629212399944663)\n",
      "(427, 0.0004438161267898977)\n",
      "(428, 0.00043123605428263545)\n",
      "(429, 0.0004190512408968061)\n",
      "(430, 0.0004072166921105236)\n",
      "(431, 0.00039728687261231244)\n",
      "(432, 0.0003856534312944859)\n",
      "(433, 0.0003758321690838784)\n",
      "(434, 0.00036589187220670283)\n",
      "(435, 0.00035691121593117714)\n",
      "(436, 0.00034717973903752863)\n",
      "(437, 0.0003388503391761333)\n",
      "(438, 0.0003303141565993428)\n",
      "(439, 0.0003212067240383476)\n",
      "(440, 0.0003132811689283699)\n",
      "(441, 0.00030559769948013127)\n",
      "(442, 0.000297586084343493)\n",
      "(443, 0.00029079618980176747)\n",
      "(444, 0.00028355102404020727)\n",
      "(445, 0.0002766873221844435)\n",
      "(446, 0.0002705283695831895)\n",
      "(447, 0.000264055939624086)\n",
      "(448, 0.0002573855163063854)\n",
      "(449, 0.0002513086365070194)\n",
      "(450, 0.0002456511138007045)\n",
      "(451, 0.00023960191174410284)\n",
      "(452, 0.0002341372601222247)\n",
      "(453, 0.00022894619905855507)\n",
      "(454, 0.00022366145276464522)\n",
      "(455, 0.00021922323503531516)\n",
      "(456, 0.0002137268311344087)\n",
      "(457, 0.00020927902369294316)\n",
      "(458, 0.00020420220971573144)\n",
      "(459, 0.00019996403716504574)\n",
      "(460, 0.00019581324886530638)\n",
      "(461, 0.00019141880329698324)\n",
      "(462, 0.00018710247240960598)\n",
      "(463, 0.00018385416478849947)\n",
      "(464, 0.0001787510555004701)\n",
      "(465, 0.0001762103638611734)\n",
      "(466, 0.00017276234575547278)\n",
      "(467, 0.00016810606757644564)\n",
      "(468, 0.00016541800869163126)\n",
      "(469, 0.000161826130351983)\n",
      "(470, 0.00015875673852860928)\n",
      "(471, 0.0001559261727379635)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(472, 0.00015297491336241364)\n",
      "(473, 0.00014961855777073652)\n",
      "(474, 0.00014698725135531276)\n",
      "(475, 0.00014411259326152503)\n",
      "(476, 0.0001412991841789335)\n",
      "(477, 0.00013888302783016115)\n",
      "(478, 0.00013601838145405054)\n",
      "(479, 0.0001334167754976079)\n",
      "(480, 0.00013086288527119905)\n",
      "(481, 0.0001284064637729898)\n",
      "(482, 0.0001258545962627977)\n",
      "(483, 0.00012370322656352073)\n",
      "(484, 0.00012138210149714723)\n",
      "(485, 0.00011915469076484442)\n",
      "(486, 0.00011722226918209344)\n",
      "(487, 0.00011515793448779732)\n",
      "(488, 0.00011292377894278616)\n",
      "(489, 0.0001109398654079996)\n",
      "(490, 0.00010871604172280058)\n",
      "(491, 0.00010697369725676253)\n",
      "(492, 0.00010509673302294686)\n",
      "(493, 0.00010335187107557431)\n",
      "(494, 0.00010144435509573668)\n",
      "(495, 0.00010018905595643446)\n",
      "(496, 9.816091187531129e-05)\n",
      "(497, 9.666335972724482e-05)\n",
      "(498, 9.517376747680828e-05)\n",
      "(499, 9.386440069647506e-05)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:0') # run on GPU\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Set requires_grad=False to compute gradients automatically \n",
    "# backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PyTorch: Defining new autograd functions\n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The **forward** function computes output Tensors from input Tensors. The **backward** function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that *same* scalar value.\n",
    "\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclass of `torch.autograd.Function` and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:\n",
    "\n",
    "![Status: **torch.autograd.Function**](http://placehold.it/350x65/FF0000/FFFF00.png&text=torch.autograd.Function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 34719584.0)\n",
      "(1, 31848042.0)\n",
      "(2, 30749452.0)\n",
      "(3, 26891780.0)\n",
      "(4, 19848780.0)\n",
      "(5, 12351649.0)\n",
      "(6, 6938094.5)\n",
      "(7, 3843415.5)\n",
      "(8, 2278105.25)\n",
      "(9, 1495133.75)\n",
      "(10, 1081121.5)\n",
      "(11, 839503.8125)\n",
      "(12, 682381.0)\n",
      "(13, 570623.4375)\n",
      "(14, 485478.0625)\n",
      "(15, 417756.4375)\n",
      "(16, 362346.65625)\n",
      "(17, 316230.9375)\n",
      "(18, 277368.8125)\n",
      "(19, 244332.1875)\n",
      "(20, 216058.109375)\n",
      "(21, 191701.15625)\n",
      "(22, 170605.5625)\n",
      "(23, 152292.25)\n",
      "(24, 136309.8125)\n",
      "(25, 122331.6953125)\n",
      "(26, 110049.4765625)\n",
      "(27, 99204.4921875)\n",
      "(28, 89596.25)\n",
      "(29, 81059.8984375)\n",
      "(30, 73458.7265625)\n",
      "(31, 66676.84375)\n",
      "(32, 60611.87890625)\n",
      "(33, 55186.5234375)\n",
      "(34, 50314.66796875)\n",
      "(35, 45931.91015625)\n",
      "(36, 41986.0546875)\n",
      "(37, 38428.57421875)\n",
      "(38, 35209.08984375)\n",
      "(39, 32296.388671875)\n",
      "(40, 29655.884765625)\n",
      "(41, 27260.560546875)\n",
      "(42, 25085.412109375)\n",
      "(43, 23105.65234375)\n",
      "(44, 21300.591796875)\n",
      "(45, 19653.740234375)\n",
      "(46, 18149.908203125)\n",
      "(47, 16776.34765625)\n",
      "(48, 15521.6416015625)\n",
      "(49, 14374.3037109375)\n",
      "(50, 13321.4853515625)\n",
      "(51, 12354.7568359375)\n",
      "(52, 11466.267578125)\n",
      "(53, 10649.037109375)\n",
      "(54, 9896.7998046875)\n",
      "(55, 9204.0458984375)\n",
      "(56, 8565.2109375)\n",
      "(57, 7976.3349609375)\n",
      "(58, 7432.162109375)\n",
      "(59, 6929.265625)\n",
      "(60, 6464.21875)\n",
      "(61, 6034.10205078125)\n",
      "(62, 5635.60009765625)\n",
      "(63, 5266.22607421875)\n",
      "(64, 4923.98046875)\n",
      "(65, 4606.5673828125)\n",
      "(66, 4311.7060546875)\n",
      "(67, 4037.675537109375)\n",
      "(68, 3783.0615234375)\n",
      "(69, 3546.630859375)\n",
      "(70, 3326.849853515625)\n",
      "(71, 3122.22314453125)\n",
      "(72, 2931.6328125)\n",
      "(73, 2754.063232421875)\n",
      "(74, 2588.580078125)\n",
      "(75, 2434.1201171875)\n",
      "(76, 2289.8359375)\n",
      "(77, 2155.094482421875)\n",
      "(78, 2029.1575927734375)\n",
      "(79, 1911.4296875)\n",
      "(80, 1801.3062744140625)\n",
      "(81, 1698.3057861328125)\n",
      "(82, 1601.860107421875)\n",
      "(83, 1511.56591796875)\n",
      "(84, 1426.868408203125)\n",
      "(85, 1347.4609375)\n",
      "(86, 1272.9794921875)\n",
      "(87, 1203.1258544921875)\n",
      "(88, 1137.57470703125)\n",
      "(89, 1075.994384765625)\n",
      "(90, 1018.1146240234375)\n",
      "(91, 963.7324829101562)\n",
      "(92, 912.5624389648438)\n",
      "(93, 864.4169921875)\n",
      "(94, 819.126220703125)\n",
      "(95, 776.5204467773438)\n",
      "(96, 736.3499145507812)\n",
      "(97, 698.4950561523438)\n",
      "(98, 662.8330688476562)\n",
      "(99, 629.1893310546875)\n",
      "(100, 597.44140625)\n",
      "(101, 567.4861450195312)\n",
      "(102, 539.2062377929688)\n",
      "(103, 512.5089111328125)\n",
      "(104, 487.268798828125)\n",
      "(105, 463.4147644042969)\n",
      "(106, 440.88232421875)\n",
      "(107, 419.570068359375)\n",
      "(108, 399.39691162109375)\n",
      "(109, 380.2999267578125)\n",
      "(110, 362.23345947265625)\n",
      "(111, 345.1079406738281)\n",
      "(112, 328.8861389160156)\n",
      "(113, 313.50811767578125)\n",
      "(114, 298.9296875)\n",
      "(115, 285.10302734375)\n",
      "(116, 271.98431396484375)\n",
      "(117, 259.54571533203125)\n",
      "(118, 247.73129272460938)\n",
      "(119, 236.5135040283203)\n",
      "(120, 225.8560028076172)\n",
      "(121, 215.73599243164062)\n",
      "(122, 206.1131591796875)\n",
      "(123, 196.96168518066406)\n",
      "(124, 188.2606201171875)\n",
      "(125, 179.9865264892578)\n",
      "(126, 172.1147918701172)\n",
      "(127, 164.6197967529297)\n",
      "(128, 157.4842529296875)\n",
      "(129, 150.69393920898438)\n",
      "(130, 144.22015380859375)\n",
      "(131, 138.0535125732422)\n",
      "(132, 132.17271423339844)\n",
      "(133, 126.56787872314453)\n",
      "(134, 121.22654724121094)\n",
      "(135, 116.1312255859375)\n",
      "(136, 111.27080535888672)\n",
      "(137, 106.63412475585938)\n",
      "(138, 102.20589447021484)\n",
      "(139, 97.97843933105469)\n",
      "(140, 93.94264221191406)\n",
      "(141, 90.08792877197266)\n",
      "(142, 86.40294647216797)\n",
      "(143, 82.88406372070312)\n",
      "(144, 79.52139282226562)\n",
      "(145, 76.3043212890625)\n",
      "(146, 73.22897338867188)\n",
      "(147, 70.28865814208984)\n",
      "(148, 67.47520446777344)\n",
      "(149, 64.78496551513672)\n",
      "(150, 62.20924758911133)\n",
      "(151, 59.745357513427734)\n",
      "(152, 57.38435745239258)\n",
      "(153, 55.12547302246094)\n",
      "(154, 52.96345138549805)\n",
      "(155, 50.88986587524414)\n",
      "(156, 48.903385162353516)\n",
      "(157, 47.000572204589844)\n",
      "(158, 45.17688751220703)\n",
      "(159, 43.42882537841797)\n",
      "(160, 41.7535514831543)\n",
      "(161, 40.14730453491211)\n",
      "(162, 38.606746673583984)\n",
      "(163, 37.12943649291992)\n",
      "(164, 35.71133041381836)\n",
      "(165, 34.35140609741211)\n",
      "(166, 33.04671096801758)\n",
      "(167, 31.794265747070312)\n",
      "(168, 30.592098236083984)\n",
      "(169, 29.438499450683594)\n",
      "(170, 28.330394744873047)\n",
      "(171, 27.266769409179688)\n",
      "(172, 26.245166778564453)\n",
      "(173, 25.264944076538086)\n",
      "(174, 24.322406768798828)\n",
      "(175, 23.417156219482422)\n",
      "(176, 22.547819137573242)\n",
      "(177, 21.711732864379883)\n",
      "(178, 20.908578872680664)\n",
      "(179, 20.136402130126953)\n",
      "(180, 19.394351959228516)\n",
      "(181, 18.68088722229004)\n",
      "(182, 17.995264053344727)\n",
      "(183, 17.335647583007812)\n",
      "(184, 16.701255798339844)\n",
      "(185, 16.09163475036621)\n",
      "(186, 15.505566596984863)\n",
      "(187, 14.941059112548828)\n",
      "(188, 14.398377418518066)\n",
      "(189, 13.87603759765625)\n",
      "(190, 13.373499870300293)\n",
      "(191, 12.890016555786133)\n",
      "(192, 12.42439079284668)\n",
      "(193, 11.976263046264648)\n",
      "(194, 11.545279502868652)\n",
      "(195, 11.130158424377441)\n",
      "(196, 10.730488777160645)\n",
      "(197, 10.345541000366211)\n",
      "(198, 9.975250244140625)\n",
      "(199, 9.618636131286621)\n",
      "(200, 9.275195121765137)\n",
      "(201, 8.944308280944824)\n",
      "(202, 8.625685691833496)\n",
      "(203, 8.318872451782227)\n",
      "(204, 8.023539543151855)\n",
      "(205, 7.738499641418457)\n",
      "(206, 7.464446544647217)\n",
      "(207, 7.200068473815918)\n",
      "(208, 6.945291042327881)\n",
      "(209, 6.69996976852417)\n",
      "(210, 6.463449478149414)\n",
      "(211, 6.235538482666016)\n",
      "(212, 6.015721797943115)\n",
      "(213, 5.804046154022217)\n",
      "(214, 5.600057601928711)\n",
      "(215, 5.403375625610352)\n",
      "(216, 5.213974952697754)\n",
      "(217, 5.031174182891846)\n",
      "(218, 4.855141639709473)\n",
      "(219, 4.685153961181641)\n",
      "(220, 4.521426200866699)\n",
      "(221, 4.363490104675293)\n",
      "(222, 4.211484432220459)\n",
      "(223, 4.064480304718018)\n",
      "(224, 3.9227845668792725)\n",
      "(225, 3.786301851272583)\n",
      "(226, 3.6547739505767822)\n",
      "(227, 3.527696371078491)\n",
      "(228, 3.4051918983459473)\n",
      "(229, 3.287125825881958)\n",
      "(230, 3.1731691360473633)\n",
      "(231, 3.063051700592041)\n",
      "(232, 2.9569995403289795)\n",
      "(233, 2.854764699935913)\n",
      "(234, 2.7558791637420654)\n",
      "(235, 2.6606383323669434)\n",
      "(236, 2.5687944889068604)\n",
      "(237, 2.4802236557006836)\n",
      "(238, 2.3946056365966797)\n",
      "(239, 2.312082052230835)\n",
      "(240, 2.232455253601074)\n",
      "(241, 2.1556296348571777)\n",
      "(242, 2.0814990997314453)\n",
      "(243, 2.010035753250122)\n",
      "(244, 1.9410077333450317)\n",
      "(245, 1.8743610382080078)\n",
      "(246, 1.810090184211731)\n",
      "(247, 1.747992753982544)\n",
      "(248, 1.688054084777832)\n",
      "(249, 1.6302474737167358)\n",
      "(250, 1.574414610862732)\n",
      "(251, 1.5206356048583984)\n",
      "(252, 1.4686542749404907)\n",
      "(253, 1.4183299541473389)\n",
      "(254, 1.3698886632919312)\n",
      "(255, 1.3232039213180542)\n",
      "(256, 1.2779252529144287)\n",
      "(257, 1.2343854904174805)\n",
      "(258, 1.192200779914856)\n",
      "(259, 1.1516501903533936)\n",
      "(260, 1.1124311685562134)\n",
      "(261, 1.0744538307189941)\n",
      "(262, 1.0378936529159546)\n",
      "(263, 1.002577543258667)\n",
      "(264, 0.9685232639312744)\n",
      "(265, 0.9355543255805969)\n",
      "(266, 0.9038021564483643)\n",
      "(267, 0.8730533719062805)\n",
      "(268, 0.8433906435966492)\n",
      "(269, 0.8147546052932739)\n",
      "(270, 0.7871086001396179)\n",
      "(271, 0.7603393793106079)\n",
      "(272, 0.7346192598342896)\n",
      "(273, 0.7096794843673706)\n",
      "(274, 0.6856120228767395)\n",
      "(275, 0.6624549627304077)\n",
      "(276, 0.6400057077407837)\n",
      "(277, 0.6182856559753418)\n",
      "(278, 0.5973305702209473)\n",
      "(279, 0.5771656632423401)\n",
      "(280, 0.5576356053352356)\n",
      "(281, 0.5387601852416992)\n",
      "(282, 0.5205515027046204)\n",
      "(283, 0.5029328465461731)\n",
      "(284, 0.48592308163642883)\n",
      "(285, 0.4695686995983124)\n",
      "(286, 0.4537235498428345)\n",
      "(287, 0.4383890926837921)\n",
      "(288, 0.42355939745903015)\n",
      "(289, 0.40930041670799255)\n",
      "(290, 0.3954541087150574)\n",
      "(291, 0.38209962844848633)\n",
      "(292, 0.3692605495452881)\n",
      "(293, 0.3567844331264496)\n",
      "(294, 0.3447984457015991)\n",
      "(295, 0.3331460952758789)\n",
      "(296, 0.3219168782234192)\n",
      "(297, 0.3110990524291992)\n",
      "(298, 0.3005909025669098)\n",
      "(299, 0.2905287444591522)\n",
      "(300, 0.2807368338108063)\n",
      "(301, 0.2712480425834656)\n",
      "(302, 0.26217591762542725)\n",
      "(303, 0.2533244788646698)\n",
      "(304, 0.24480272829532623)\n",
      "(305, 0.23659071326255798)\n",
      "(306, 0.22862890362739563)\n",
      "(307, 0.22095052897930145)\n",
      "(308, 0.21354196965694427)\n",
      "(309, 0.20636463165283203)\n",
      "(310, 0.19943170249462128)\n",
      "(311, 0.19271959364414215)\n",
      "(312, 0.18627169728279114)\n",
      "(313, 0.180013045668602)\n",
      "(314, 0.17396673560142517)\n",
      "(315, 0.16813969612121582)\n",
      "(316, 0.16250233352184296)\n",
      "(317, 0.15705075860023499)\n",
      "(318, 0.1518060564994812)\n",
      "(319, 0.14672841131687164)\n",
      "(320, 0.14181607961654663)\n",
      "(321, 0.13704746961593628)\n",
      "(322, 0.13248905539512634)\n",
      "(323, 0.12803879380226135)\n",
      "(324, 0.12375032901763916)\n",
      "(325, 0.11961318552494049)\n",
      "(326, 0.11559596657752991)\n",
      "(327, 0.11174444109201431)\n",
      "(328, 0.10800660401582718)\n",
      "(329, 0.10438321530818939)\n",
      "(330, 0.10090778023004532)\n",
      "(331, 0.09753280878067017)\n",
      "(332, 0.09426451474428177)\n",
      "(333, 0.0911027193069458)\n",
      "(334, 0.0880810022354126)\n",
      "(335, 0.08513879776000977)\n",
      "(336, 0.08228851854801178)\n",
      "(337, 0.07953383028507233)\n",
      "(338, 0.07688639312982559)\n",
      "(339, 0.07431066781282425)\n",
      "(340, 0.07183600962162018)\n",
      "(341, 0.06944350153207779)\n",
      "(342, 0.06712955981492996)\n",
      "(343, 0.06490301340818405)\n",
      "(344, 0.06272546947002411)\n",
      "(345, 0.06063657999038696)\n",
      "(346, 0.05863235890865326)\n",
      "(347, 0.05667119473218918)\n",
      "(348, 0.054793935269117355)\n",
      "(349, 0.05297465622425079)\n",
      "(350, 0.051211729645729065)\n",
      "(351, 0.04950997233390808)\n",
      "(352, 0.04785572364926338)\n",
      "(353, 0.0462690070271492)\n",
      "(354, 0.044737473130226135)\n",
      "(355, 0.04325278103351593)\n",
      "(356, 0.04182612895965576)\n",
      "(357, 0.040428437292575836)\n",
      "(358, 0.039083003997802734)\n",
      "(359, 0.03780507668852806)\n",
      "(360, 0.03656017407774925)\n",
      "(361, 0.03533676639199257)\n",
      "(362, 0.034172412008047104)\n",
      "(363, 0.033048175275325775)\n",
      "(364, 0.03196476772427559)\n",
      "(365, 0.03089270554482937)\n",
      "(366, 0.029871346428990364)\n",
      "(367, 0.02889017015695572)\n",
      "(368, 0.027936168015003204)\n",
      "(369, 0.027020245790481567)\n",
      "(370, 0.026123367249965668)\n",
      "(371, 0.02526780776679516)\n",
      "(372, 0.024440087378025055)\n",
      "(373, 0.02363446354866028)\n",
      "(374, 0.022855183109641075)\n",
      "(375, 0.022113770246505737)\n",
      "(376, 0.021381527185440063)\n",
      "(377, 0.020678455010056496)\n",
      "(378, 0.019993256777524948)\n",
      "(379, 0.019348496571183205)\n",
      "(380, 0.018723176792263985)\n",
      "(381, 0.018110230565071106)\n",
      "(382, 0.01752152293920517)\n",
      "(383, 0.016945961862802505)\n",
      "(384, 0.016392435878515244)\n",
      "(385, 0.015863513574004173)\n",
      "(386, 0.015342919155955315)\n",
      "(387, 0.014845018275082111)\n",
      "(388, 0.014356054365634918)\n",
      "(389, 0.013888626359403133)\n",
      "(390, 0.01344721857458353)\n",
      "(391, 0.01301033329218626)\n",
      "(392, 0.012582671828567982)\n",
      "(393, 0.012179887853562832)\n",
      "(394, 0.01178121380507946)\n",
      "(395, 0.011406000703573227)\n",
      "(396, 0.01103825494647026)\n",
      "(397, 0.010682599619030952)\n",
      "(398, 0.010339850559830666)\n",
      "(399, 0.010004152543842793)\n",
      "(400, 0.009684651158750057)\n",
      "(401, 0.009376228787004948)\n",
      "(402, 0.009075485169887543)\n",
      "(403, 0.008786332793533802)\n",
      "(404, 0.008506960235536098)\n",
      "(405, 0.00823727436363697)\n",
      "(406, 0.007976080290973186)\n",
      "(407, 0.00771751394495368)\n",
      "(408, 0.007477166131138802)\n",
      "(409, 0.007242914754897356)\n",
      "(410, 0.007012804038822651)\n",
      "(411, 0.00679156044498086)\n",
      "(412, 0.006579079665243626)\n",
      "(413, 0.006369863171130419)\n",
      "(414, 0.006169057451188564)\n",
      "(415, 0.005979827605187893)\n",
      "(416, 0.005792261566966772)\n",
      "(417, 0.005611862987279892)\n",
      "(418, 0.00544077530503273)\n",
      "(419, 0.005270927678793669)\n",
      "(420, 0.005107678472995758)\n",
      "(421, 0.004954826086759567)\n",
      "(422, 0.004796986468136311)\n",
      "(423, 0.0046530235558748245)\n",
      "(424, 0.004511112347245216)\n",
      "(425, 0.004373142495751381)\n",
      "(426, 0.004239554516971111)\n",
      "(427, 0.004112585447728634)\n",
      "(428, 0.003989661578088999)\n",
      "(429, 0.003866248531267047)\n",
      "(430, 0.0037515959702432156)\n",
      "(431, 0.0036405036225914955)\n",
      "(432, 0.0035309523809701204)\n",
      "(433, 0.0034292975906282663)\n",
      "(434, 0.0033266814425587654)\n",
      "(435, 0.0032269677612930536)\n",
      "(436, 0.0031300943810492754)\n",
      "(437, 0.003041424322873354)\n",
      "(438, 0.0029490513261407614)\n",
      "(439, 0.0028649885207414627)\n",
      "(440, 0.0027822002302855253)\n",
      "(441, 0.0027004999574273825)\n",
      "(442, 0.0026223231106996536)\n",
      "(443, 0.002544567221775651)\n",
      "(444, 0.0024729256983846426)\n",
      "(445, 0.0024026946630328894)\n",
      "(446, 0.0023361267521977425)\n",
      "(447, 0.002271161414682865)\n",
      "(448, 0.002205857075750828)\n",
      "(449, 0.002143165562301874)\n",
      "(450, 0.0020826365798711777)\n",
      "(451, 0.002026550704613328)\n",
      "(452, 0.0019697032403200865)\n",
      "(453, 0.0019164234399795532)\n",
      "(454, 0.001861599856056273)\n",
      "(455, 0.001812303438782692)\n",
      "(456, 0.0017596181714907289)\n",
      "(457, 0.0017136330716311932)\n",
      "(458, 0.0016661541303619742)\n",
      "(459, 0.0016189448069781065)\n",
      "(460, 0.0015785639407113194)\n",
      "(461, 0.0015368140302598476)\n",
      "(462, 0.001495341770350933)\n",
      "(463, 0.0014531899942085147)\n",
      "(464, 0.0014155965764075518)\n",
      "(465, 0.0013783780159428716)\n",
      "(466, 0.0013428778620436788)\n",
      "(467, 0.0013081553624942899)\n",
      "(468, 0.001273406553082168)\n",
      "(469, 0.0012420715065672994)\n",
      "(470, 0.0012099394807592034)\n",
      "(471, 0.0011794938473030925)\n",
      "(472, 0.0011494795326143503)\n",
      "(473, 0.0011205687187612057)\n",
      "(474, 0.0010918198386207223)\n",
      "(475, 0.0010651899501681328)\n",
      "(476, 0.0010391168761998415)\n",
      "(477, 0.0010116677731275558)\n",
      "(478, 0.0009877309203147888)\n",
      "(479, 0.0009634224697947502)\n",
      "(480, 0.0009407204925082624)\n",
      "(481, 0.0009177616448141634)\n",
      "(482, 0.0008964308653958142)\n",
      "(483, 0.0008760389173403382)\n",
      "(484, 0.0008538240799680352)\n",
      "(485, 0.0008332866709679365)\n",
      "(486, 0.0008152327500283718)\n",
      "(487, 0.0007948673446662724)\n",
      "(488, 0.0007751283119432628)\n",
      "(489, 0.0007581943063996732)\n",
      "(490, 0.0007399650057777762)\n",
      "(491, 0.0007225777953863144)\n",
      "(492, 0.0007063719676807523)\n",
      "(493, 0.0006908893119543791)\n",
      "(494, 0.0006739242235198617)\n",
      "(495, 0.0006577947060577571)\n",
      "(496, 0.0006441615405492485)\n",
      "(497, 0.0006298905937001109)\n",
      "(498, 0.0006153382710181177)\n",
      "(499, 0.0006028197822161019)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, dtype=dtype, device=device)\n",
    "y = torch.randn(N, D_out, dtype=dtype, device=device)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, dtype=dtype, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, dtype=dtype, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow: Static Graphs\n",
    "PyTorch autograd looks a lot like TensorFlow: in both frameworks we define a computational graph, and use automatic differentiation to compute gradients. The biggest difference between the two is that TensorFlow’s computational graphs are **static** and PyTorch uses **dynamic** computational graphs.\n",
    "\n",
    "In TensorFlow, we define the computational graph once and then execute the same graph over and over again, possibly feeding different input data to the graph. In PyTorch, each forward pass defines a new computational graph.\n",
    "\n",
    "Static graphs are nice because you can optimize the graph up front; for example a framework might decide to fuse some graph operations for efficiency, or to come up with a strategy for distributing the graph across many GPUs or many machines. If you are reusing the same graph over and over, then this potentially costly up-front optimization can be amortized as the same graph is rerun over and over.\n",
    "\n",
    "One aspect where static and dynamic graphs differ is control flow. For some models we may wish to perform different computation for each data point; for example a recurrent network might be unrolled for different numbers of time steps for each data point; this unrolling can be implemented as a loop. With a static graph the loop construct needs to be a part of the graph; for this reason TensorFlow provides operators such as tf.scan for embedding loops into the graph. With dynamic graphs the situation is simpler: since we build graphs on-the-fly for each example, we can use normal imperative flow control to perform computation that differs for each input.\n",
    "\n",
    "To contrast with the PyTorch autograd example above, here we use TensorFlow to fit a simple two-layer net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32344976.0\n",
      "31963212.0\n",
      "39074896.0\n",
      "46020450.0\n",
      "44225544.0\n",
      "30476780.0\n",
      "15215793.0\n",
      "6268211.0\n",
      "2759659.0\n",
      "1523591.2\n",
      "1046365.3\n",
      "814004.94\n",
      "670358.94\n",
      "566447.6\n",
      "484932.25\n",
      "418534.88\n",
      "363445.8\n",
      "317182.9\n",
      "278007.06\n",
      "244703.95\n",
      "216189.94\n",
      "191655.48\n",
      "170463.03\n",
      "152056.38\n",
      "136000.44\n",
      "121944.766\n",
      "109608.445\n",
      "98744.27\n",
      "89137.234\n",
      "80610.95\n",
      "73031.63\n",
      "66284.09\n",
      "60256.6\n",
      "54860.0\n",
      "50031.047\n",
      "45693.824\n",
      "41785.355\n",
      "38260.227\n",
      "35075.766\n",
      "32195.062\n",
      "29584.082\n",
      "27213.129\n",
      "25056.81\n",
      "23093.531\n",
      "21305.79\n",
      "19675.312\n",
      "18185.846\n",
      "16822.977\n",
      "15573.881\n",
      "14428.966\n",
      "13379.152\n",
      "12414.1\n",
      "11527.108\n",
      "10711.538\n",
      "9959.78\n",
      "9266.346\n",
      "8626.755\n",
      "8035.881\n",
      "7489.941\n",
      "6984.858\n",
      "6517.1294\n",
      "6083.933\n",
      "5682.137\n",
      "5309.429\n",
      "4963.59\n",
      "4642.217\n",
      "4343.585\n",
      "4065.8936\n",
      "3807.5698\n",
      "3567.1826\n",
      "3343.2686\n",
      "3134.4705\n",
      "2939.809\n",
      "2758.2163\n",
      "2589.0186\n",
      "2431.0723\n",
      "2283.5632\n",
      "2145.709\n",
      "2016.7976\n",
      "1896.1699\n",
      "1783.2942\n",
      "1677.6671\n",
      "1578.747\n",
      "1486.057\n",
      "1399.154\n",
      "1317.7329\n",
      "1241.3392\n",
      "1169.6642\n",
      "1102.3892\n",
      "1039.2488\n",
      "979.9245\n",
      "924.1896\n",
      "871.82635\n",
      "822.61816\n",
      "776.33716\n",
      "732.7919\n",
      "691.83734\n",
      "653.2936\n",
      "617.00116\n",
      "582.83405\n",
      "550.66016\n",
      "520.3466\n",
      "491.78342\n",
      "464.8728\n",
      "439.50955\n",
      "415.58807\n",
      "393.0376\n",
      "371.76392\n",
      "351.68588\n",
      "332.74368\n",
      "314.8692\n",
      "297.9903\n",
      "282.05133\n",
      "267.00565\n",
      "252.80121\n",
      "239.37224\n",
      "226.6874\n",
      "214.70053\n",
      "203.37369\n",
      "192.66269\n",
      "182.54102\n",
      "172.96463\n",
      "163.911\n",
      "155.35106\n",
      "147.25227\n",
      "139.5862\n",
      "132.33463\n",
      "125.4718\n",
      "118.975655\n",
      "112.82828\n",
      "107.00672\n",
      "101.49675\n",
      "96.27814\n",
      "91.33562\n",
      "86.65428\n",
      "82.2207\n",
      "78.0191\n",
      "74.03969\n",
      "70.267586\n",
      "66.69318\n",
      "63.30619\n",
      "60.093536\n",
      "57.048946\n",
      "54.16426\n",
      "51.426632\n",
      "48.832954\n",
      "46.37386\n",
      "44.041225\n",
      "41.828465\n",
      "39.72889\n",
      "37.73697\n",
      "35.847576\n",
      "34.055355\n",
      "32.3545\n",
      "30.739532\n",
      "29.207851\n",
      "27.75361\n",
      "26.373692\n",
      "25.062674\n",
      "23.819942\n",
      "22.638607\n",
      "21.51798\n",
      "20.45323\n",
      "19.44239\n",
      "18.482258\n",
      "17.571281\n",
      "16.705555\n",
      "15.882719\n",
      "15.10157\n",
      "14.359396\n",
      "13.654229\n",
      "12.984402\n",
      "12.348395\n",
      "11.743969\n",
      "11.169191\n",
      "10.622909\n",
      "10.1043\n",
      "9.610709\n",
      "9.142509\n",
      "8.696834\n",
      "8.273434\n",
      "7.871407\n",
      "7.488412\n",
      "7.124943\n",
      "6.7788925\n",
      "6.4503055\n",
      "6.137673\n",
      "5.84059\n",
      "5.5577993\n",
      "5.2890677\n",
      "5.033432\n",
      "4.7906256\n",
      "4.559251\n",
      "4.339427\n",
      "4.1300645\n",
      "3.9313426\n",
      "3.742181\n",
      "3.5620737\n",
      "3.3910031\n",
      "3.2282\n",
      "3.07318\n",
      "2.9256434\n",
      "2.7852583\n",
      "2.6517239\n",
      "2.5249395\n",
      "2.4039884\n",
      "2.2889526\n",
      "2.1795275\n",
      "2.0753024\n",
      "1.9763792\n",
      "1.8820553\n",
      "1.7922127\n",
      "1.7067702\n",
      "1.625436\n",
      "1.5480652\n",
      "1.4742491\n",
      "1.4042004\n",
      "1.3373957\n",
      "1.2738019\n",
      "1.2132651\n",
      "1.1557546\n",
      "1.1008488\n",
      "1.0486889\n",
      "0.99884534\n",
      "0.9515511\n",
      "0.9064534\n",
      "0.86359423\n",
      "0.82267964\n",
      "0.78379744\n",
      "0.74675655\n",
      "0.71134853\n",
      "0.67783594\n",
      "0.64584535\n",
      "0.61535144\n",
      "0.5863054\n",
      "0.5586601\n",
      "0.53237844\n",
      "0.50725496\n",
      "0.48336178\n",
      "0.46054137\n",
      "0.4389174\n",
      "0.4182836\n",
      "0.3985635\n",
      "0.37986368\n",
      "0.3620335\n",
      "0.34501725\n",
      "0.32884285\n",
      "0.31344187\n",
      "0.29875484\n",
      "0.28472483\n",
      "0.27142373\n",
      "0.25870344\n",
      "0.24654667\n",
      "0.23502393\n",
      "0.22399305\n",
      "0.21348405\n",
      "0.2035581\n",
      "0.19406761\n",
      "0.18495145\n",
      "0.17630589\n",
      "0.1681009\n",
      "0.16021949\n",
      "0.1527504\n",
      "0.14563479\n",
      "0.13880044\n",
      "0.13235529\n",
      "0.12619382\n",
      "0.12029348\n",
      "0.11469749\n",
      "0.10936873\n",
      "0.10431238\n",
      "0.09944579\n",
      "0.094809026\n",
      "0.09038782\n",
      "0.08620043\n",
      "0.08217015\n",
      "0.07835181\n",
      "0.07471529\n",
      "0.07123718\n",
      "0.06792706\n",
      "0.06477836\n",
      "0.061776396\n",
      "0.058921803\n",
      "0.056201834\n",
      "0.053604078\n",
      "0.05112361\n",
      "0.048753478\n",
      "0.046505623\n",
      "0.044340193\n",
      "0.042301036\n",
      "0.04034538\n",
      "0.03850238\n",
      "0.036726248\n",
      "0.035008766\n",
      "0.03341884\n",
      "0.03186866\n",
      "0.030404013\n",
      "0.029014453\n",
      "0.0276818\n",
      "0.02640802\n",
      "0.025197204\n",
      "0.024025971\n",
      "0.022933288\n",
      "0.021886878\n",
      "0.020889662\n",
      "0.019935023\n",
      "0.019022092\n",
      "0.01815551\n",
      "0.01732615\n",
      "0.016531786\n",
      "0.015783768\n",
      "0.015072251\n",
      "0.014380183\n",
      "0.013727531\n",
      "0.013105413\n",
      "0.01251426\n",
      "0.011949336\n",
      "0.011411598\n",
      "0.010896663\n",
      "0.010407169\n",
      "0.009938009\n",
      "0.009491941\n",
      "0.009059281\n",
      "0.008653594\n",
      "0.008273585\n",
      "0.007904362\n",
      "0.0075455843\n",
      "0.007212035\n",
      "0.00689878\n",
      "0.006597969\n",
      "0.0063126213\n",
      "0.0060390113\n",
      "0.005773512\n",
      "0.0055252695\n",
      "0.005281008\n",
      "0.0050573996\n",
      "0.004841748\n",
      "0.0046322267\n",
      "0.004428451\n",
      "0.0042402856\n",
      "0.0040655993\n",
      "0.0038973743\n",
      "0.0037284205\n",
      "0.003574151\n",
      "0.0034210938\n",
      "0.0032778853\n",
      "0.0031396279\n",
      "0.0030105386\n",
      "0.002889054\n",
      "0.0027698898\n",
      "0.0026512677\n",
      "0.002545843\n",
      "0.0024421308\n",
      "0.002345896\n",
      "0.0022494006\n",
      "0.0021600793\n",
      "0.002076521\n",
      "0.0019944154\n",
      "0.0019161283\n",
      "0.0018417923\n",
      "0.0017732155\n",
      "0.0017052023\n",
      "0.0016414247\n",
      "0.0015774346\n",
      "0.0015170437\n",
      "0.0014585992\n",
      "0.0014045767\n",
      "0.0013532399\n",
      "0.0013050914\n",
      "0.0012568901\n",
      "0.0012102238\n",
      "0.0011678948\n",
      "0.0011251881\n",
      "0.0010870245\n",
      "0.0010473656\n",
      "0.0010111898\n",
      "0.00097625586\n",
      "0.000940961\n",
      "0.0009087818\n",
      "0.00087782706\n",
      "0.0008488834\n",
      "0.00081919273\n",
      "0.0007917343\n",
      "0.0007649887\n",
      "0.0007394642\n",
      "0.00071548775\n",
      "0.00069360423\n",
      "0.0006722545\n",
      "0.00065003725\n",
      "0.0006304198\n",
      "0.00060943473\n",
      "0.00059112895\n",
      "0.0005731498\n",
      "0.0005559618\n",
      "0.0005390393\n",
      "0.0005234565\n",
      "0.00050897733\n",
      "0.000493405\n",
      "0.0004784814\n",
      "0.0004651004\n",
      "0.0004517822\n",
      "0.00043957107\n",
      "0.0004267267\n",
      "0.00041436852\n",
      "0.00040326646\n",
      "0.0003922043\n",
      "0.00038091064\n",
      "0.000371184\n",
      "0.00036081212\n",
      "0.00035123815\n",
      "0.00034164672\n",
      "0.00033235562\n",
      "0.00032346032\n",
      "0.00031529923\n",
      "0.00030724998\n",
      "0.00029905688\n",
      "0.00029150373\n",
      "0.00028459277\n",
      "0.0002773567\n",
      "0.0002703282\n",
      "0.0002634189\n",
      "0.00025688094\n",
      "0.00025051524\n",
      "0.0002445317\n",
      "0.00023806636\n",
      "0.0002328053\n",
      "0.00022800469\n",
      "0.00022253918\n",
      "0.00021704166\n",
      "0.00021194312\n",
      "0.00020713126\n",
      "0.00020234966\n",
      "0.00019743326\n",
      "0.00019292298\n",
      "0.00018829963\n",
      "0.00018443505\n",
      "0.00017991032\n",
      "0.00017620248\n",
      "0.00017250329\n",
      "0.00016929628\n",
      "0.00016547063\n",
      "0.00016211756\n",
      "0.00015907781\n",
      "0.00015497401\n",
      "0.0001520844\n",
      "0.00014824147\n",
      "0.00014549735\n",
      "0.00014249107\n",
      "0.00014020845\n",
      "0.00013697402\n",
      "0.00013384447\n",
      "0.00013140828\n",
      "0.00012842123\n",
      "0.00012574942\n",
      "0.00012355133\n",
      "0.00012123002\n",
      "0.00011904912\n",
      "0.000116586154\n",
      "0.000114970935\n",
      "0.000112669106\n",
      "0.000110577756\n",
      "0.00010821633\n",
      "0.00010631645\n",
      "0.00010439902\n",
      "0.00010233985\n",
      "0.00010077036\n",
      "9.891612e-05\n",
      "9.7213044e-05\n",
      "9.5774485e-05\n",
      "9.37194e-05\n",
      "9.1950584e-05\n",
      "9.025412e-05\n",
      "8.839586e-05\n",
      "8.6955166e-05\n",
      "8.560882e-05\n",
      "8.412672e-05\n",
      "8.2634455e-05\n",
      "8.159603e-05\n",
      "8.021151e-05\n",
      "7.905895e-05\n",
      "7.746722e-05\n",
      "7.6097596e-05\n",
      "7.516933e-05\n",
      "7.396893e-05\n",
      "7.299401e-05\n",
      "7.185427e-05\n",
      "7.081763e-05\n",
      "6.9570495e-05\n",
      "6.826081e-05\n",
      "6.728736e-05\n",
      "6.626652e-05\n",
      "6.5118744e-05\n",
      "6.417136e-05\n",
      "6.334652e-05\n",
      "6.2311374e-05\n",
      "6.1506464e-05\n",
      "6.0627433e-05\n",
      "5.9495353e-05\n",
      "5.8727266e-05\n",
      "5.8074245e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind\n",
    "        # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        # Each time we execute the graph we want to compute the values for loss,\n",
    "        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "        # arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict = {x:x_value, y:y_value})\n",
    "        \n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN MODULE\n",
    "### PyTorch: nn\n",
    "\n",
    "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into **layers**, some of which have **learnable parameters** which will be optimized during learning.\n",
    "\n",
    "In TensorFlow, packages like [Keras](https://github.com/fchollet/keras), [TensorFlow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim), and [TFLearn](http://tflearn.org/) provide higher-level abstractions over raw computational graphs that are useful for building neural networks.\n",
    "\n",
    "In PyTorch, the `nn` package serves this same purpose. The `nn` package defines a set of **Modules**, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The `nn` package also defines a set of useful loss functions that are commonly used when training neural networks.\n",
    "\n",
    "In this example we use the `nn` package to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 694.1128540039062)\n",
      "(1, 640.18505859375)\n",
      "(2, 550.582275390625)\n",
      "(3, 450.6769714355469)\n",
      "(4, 357.48431396484375)\n",
      "(5, 279.3929748535156)\n",
      "(6, 217.30007934570312)\n",
      "(7, 171.32496643066406)\n",
      "(8, 139.98739624023438)\n",
      "(9, 121.37075805664062)\n",
      "(10, 118.09574890136719)\n",
      "(11, 139.26820373535156)\n",
      "(12, 188.1622314453125)\n",
      "(13, 235.66067504882812)\n",
      "(14, 233.3280029296875)\n",
      "(15, 178.70748901367188)\n",
      "(16, 132.3525390625)\n",
      "(17, 133.16220092773438)\n",
      "(18, 147.7489776611328)\n",
      "(19, 148.66209411621094)\n",
      "(20, 173.33229064941406)\n",
      "(21, 240.901611328125)\n",
      "(22, 283.0752258300781)\n",
      "(23, 249.7598114013672)\n",
      "(24, 205.88031005859375)\n",
      "(25, 207.874267578125)\n",
      "(26, 198.85772705078125)\n",
      "(27, 163.71678161621094)\n",
      "(28, 193.69155883789062)\n",
      "(29, 255.87147521972656)\n",
      "(30, 228.14585876464844)\n",
      "(31, 185.79563903808594)\n",
      "(32, 204.38009643554688)\n",
      "(33, 176.81723022460938)\n",
      "(34, 141.61204528808594)\n",
      "(35, 202.07936096191406)\n",
      "(36, 229.55462646484375)\n",
      "(37, 189.2101593017578)\n",
      "(38, 212.67835998535156)\n",
      "(39, 222.9515838623047)\n",
      "(40, 167.1463623046875)\n",
      "(41, 189.85801696777344)\n",
      "(42, 227.597412109375)\n",
      "(43, 177.83316040039062)\n",
      "(44, 179.40504455566406)\n",
      "(45, 208.64193725585938)\n",
      "(46, 160.6645965576172)\n",
      "(47, 165.19888305664062)\n",
      "(48, 217.4385223388672)\n",
      "(49, 183.97134399414062)\n",
      "(50, 164.94337463378906)\n",
      "(51, 205.00511169433594)\n",
      "(52, 178.26214599609375)\n",
      "(53, 146.6128387451172)\n",
      "(54, 189.9391632080078)\n",
      "(55, 187.98304748535156)\n",
      "(56, 143.8119354248047)\n",
      "(57, 167.95237731933594)\n",
      "(58, 186.60459899902344)\n",
      "(59, 143.29397583007812)\n",
      "(60, 142.22723388671875)\n",
      "(61, 181.19821166992188)\n",
      "(62, 165.21847534179688)\n",
      "(63, 137.08453369140625)\n",
      "(64, 163.96142578125)\n",
      "(65, 179.2227783203125)\n",
      "(66, 139.640380859375)\n",
      "(67, 118.3302230834961)\n",
      "(68, 143.33316040039062)\n",
      "(69, 148.4477081298828)\n",
      "(70, 119.1640396118164)\n",
      "(71, 116.71644592285156)\n",
      "(72, 148.1193389892578)\n",
      "(73, 154.3772735595703)\n",
      "(74, 125.27369689941406)\n",
      "(75, 112.36233520507812)\n",
      "(76, 132.87620544433594)\n",
      "(77, 145.80337524414062)\n",
      "(78, 126.85443115234375)\n",
      "(79, 105.44451141357422)\n",
      "(80, 111.16768646240234)\n",
      "(81, 129.07391357421875)\n",
      "(82, 127.4784164428711)\n",
      "(83, 107.15360260009766)\n",
      "(84, 97.10157012939453)\n",
      "(85, 110.21222686767578)\n",
      "(86, 126.7650146484375)\n",
      "(87, 122.88651275634766)\n",
      "(88, 101.79377746582031)\n",
      "(89, 86.61746978759766)\n",
      "(90, 90.20974731445312)\n",
      "(91, 102.718505859375)\n",
      "(92, 106.01387786865234)\n",
      "(93, 94.77161407470703)\n",
      "(94, 80.34868621826172)\n",
      "(95, 76.3989028930664)\n",
      "(96, 84.1866683959961)\n",
      "(97, 93.5402603149414)\n",
      "(98, 94.56917572021484)\n",
      "(99, 86.94840240478516)\n",
      "(100, 79.21836853027344)\n",
      "(101, 78.22074127197266)\n",
      "(102, 83.40609741210938)\n",
      "(103, 88.56695556640625)\n",
      "(104, 88.05591583251953)\n",
      "(105, 82.83051300048828)\n",
      "(106, 77.23372650146484)\n",
      "(107, 75.11892700195312)\n",
      "(108, 77.38041687011719)\n",
      "(109, 80.76431274414062)\n",
      "(110, 82.30384063720703)\n",
      "(111, 80.90750122070312)\n",
      "(112, 77.32087707519531)\n",
      "(113, 74.01927185058594)\n",
      "(114, 72.44448852539062)\n",
      "(115, 72.54491424560547)\n",
      "(116, 73.58236694335938)\n",
      "(117, 75.39283752441406)\n",
      "(118, 77.26658630371094)\n",
      "(119, 77.8113784790039)\n",
      "(120, 76.02140045166016)\n",
      "(121, 71.82880401611328)\n",
      "(122, 66.92831420898438)\n",
      "(123, 64.47962188720703)\n",
      "(124, 66.06389617919922)\n",
      "(125, 69.78429412841797)\n",
      "(126, 72.09706115722656)\n",
      "(127, 70.55900573730469)\n",
      "(128, 66.75982666015625)\n",
      "(129, 63.4969482421875)\n",
      "(130, 62.9079704284668)\n",
      "(131, 64.00064086914062)\n",
      "(132, 65.33203125)\n",
      "(133, 65.84536743164062)\n",
      "(134, 64.83380889892578)\n",
      "(135, 62.661521911621094)\n",
      "(136, 60.56599044799805)\n",
      "(137, 59.80774688720703)\n",
      "(138, 60.99993133544922)\n",
      "(139, 63.79136657714844)\n",
      "(140, 65.85388946533203)\n",
      "(141, 64.66773223876953)\n",
      "(142, 60.48625183105469)\n",
      "(143, 56.167911529541016)\n",
      "(144, 54.2332878112793)\n",
      "(145, 54.61308288574219)\n",
      "(146, 55.907020568847656)\n",
      "(147, 57.190616607666016)\n",
      "(148, 57.84746170043945)\n",
      "(149, 59.02403259277344)\n",
      "(150, 60.29255676269531)\n",
      "(151, 60.69451904296875)\n",
      "(152, 60.2257080078125)\n",
      "(153, 59.50677490234375)\n",
      "(154, 58.22346115112305)\n",
      "(155, 56.454341888427734)\n",
      "(156, 54.84675216674805)\n",
      "(157, 54.56140899658203)\n",
      "(158, 55.253841400146484)\n",
      "(159, 54.514686584472656)\n",
      "(160, 51.75433349609375)\n",
      "(161, 48.70342254638672)\n",
      "(162, 48.3826904296875)\n",
      "(163, 51.585205078125)\n",
      "(164, 55.031700134277344)\n",
      "(165, 55.7411003112793)\n",
      "(166, 52.74931716918945)\n",
      "(167, 49.034027099609375)\n",
      "(168, 47.732154846191406)\n",
      "(169, 49.23651123046875)\n",
      "(170, 51.03016662597656)\n",
      "(171, 51.49045181274414)\n",
      "(172, 51.021018981933594)\n",
      "(173, 50.12940216064453)\n",
      "(174, 48.70186233520508)\n",
      "(175, 46.95221710205078)\n",
      "(176, 46.562496185302734)\n",
      "(177, 48.51597595214844)\n",
      "(178, 51.33056640625)\n",
      "(179, 52.83115005493164)\n",
      "(180, 52.8909912109375)\n",
      "(181, 52.2890739440918)\n",
      "(182, 50.794185638427734)\n",
      "(183, 48.46707534790039)\n",
      "(184, 46.89825439453125)\n",
      "(185, 47.74968338012695)\n",
      "(186, 48.619529724121094)\n",
      "(187, 46.85707473754883)\n",
      "(188, 43.957008361816406)\n",
      "(189, 44.288909912109375)\n",
      "(190, 47.38713455200195)\n",
      "(191, 49.3078498840332)\n",
      "(192, 48.43686294555664)\n",
      "(193, 46.87830352783203)\n",
      "(194, 45.941917419433594)\n",
      "(195, 44.65541076660156)\n",
      "(196, 43.399959564208984)\n",
      "(197, 44.396995544433594)\n",
      "(198, 47.63169860839844)\n",
      "(199, 49.804718017578125)\n",
      "(200, 49.33494567871094)\n",
      "(201, 48.35828399658203)\n",
      "(202, 48.021602630615234)\n",
      "(203, 46.670013427734375)\n",
      "(204, 44.22044372558594)\n",
      "(205, 43.05470657348633)\n",
      "(206, 43.96352005004883)\n",
      "(207, 44.74494934082031)\n",
      "(208, 44.765411376953125)\n",
      "(209, 45.98603057861328)\n",
      "(210, 47.67316818237305)\n",
      "(211, 46.658443450927734)\n",
      "(212, 43.846187591552734)\n",
      "(213, 43.000343322753906)\n",
      "(214, 43.86174011230469)\n",
      "(215, 43.27309036254883)\n",
      "(216, 42.01860046386719)\n",
      "(217, 43.718570709228516)\n",
      "(218, 46.02989959716797)\n",
      "(219, 44.458900451660156)\n",
      "(220, 41.61237716674805)\n",
      "(221, 42.84416580200195)\n",
      "(222, 46.156227111816406)\n",
      "(223, 46.68067932128906)\n",
      "(224, 45.37431716918945)\n",
      "(225, 44.547271728515625)\n",
      "(226, 43.452362060546875)\n",
      "(227, 41.65924835205078)\n",
      "(228, 41.072853088378906)\n",
      "(229, 41.689701080322266)\n",
      "(230, 41.1710205078125)\n",
      "(231, 39.775733947753906)\n",
      "(232, 40.86376190185547)\n",
      "(233, 43.68259811401367)\n",
      "(234, 43.87398147583008)\n",
      "(235, 42.32300567626953)\n",
      "(236, 42.268470764160156)\n",
      "(237, 41.33197021484375)\n",
      "(238, 39.014076232910156)\n",
      "(239, 38.901466369628906)\n",
      "(240, 39.902530670166016)\n",
      "(241, 39.60612869262695)\n",
      "(242, 38.862483978271484)\n",
      "(243, 38.36699676513672)\n",
      "(244, 38.081600189208984)\n",
      "(245, 38.970767974853516)\n",
      "(246, 40.71921157836914)\n",
      "(247, 41.84775161743164)\n",
      "(248, 42.75725555419922)\n",
      "(249, 42.16368103027344)\n",
      "(250, 37.70228576660156)\n",
      "(251, 33.849205017089844)\n",
      "(252, 36.12950897216797)\n",
      "(253, 40.84006118774414)\n",
      "(254, 42.12557601928711)\n",
      "(255, 40.023033142089844)\n",
      "(256, 37.373897552490234)\n",
      "(257, 37.31733322143555)\n",
      "(258, 39.64809799194336)\n",
      "(259, 40.54764175415039)\n",
      "(260, 39.71171188354492)\n",
      "(261, 39.123756408691406)\n",
      "(262, 37.761878967285156)\n",
      "(263, 36.68073654174805)\n",
      "(264, 37.254302978515625)\n",
      "(265, 37.35890579223633)\n",
      "(266, 36.92937088012695)\n",
      "(267, 36.78947448730469)\n",
      "(268, 37.314720153808594)\n",
      "(269, 38.34975051879883)\n",
      "(270, 38.871036529541016)\n",
      "(271, 38.306800842285156)\n",
      "(272, 36.6931266784668)\n",
      "(273, 33.53550338745117)\n",
      "(274, 32.25914001464844)\n",
      "(275, 34.54716491699219)\n",
      "(276, 37.38703536987305)\n",
      "(277, 39.63310623168945)\n",
      "(278, 40.173851013183594)\n",
      "(279, 39.76763916015625)\n",
      "(280, 38.8297119140625)\n",
      "(281, 35.10675048828125)\n",
      "(282, 34.11898422241211)\n",
      "(283, 37.02988815307617)\n",
      "(284, 36.09083938598633)\n",
      "(285, 35.43148422241211)\n",
      "(286, 37.00424575805664)\n",
      "(287, 35.69486618041992)\n",
      "(288, 34.194644927978516)\n",
      "(289, 34.73789596557617)\n",
      "(290, 37.37957763671875)\n",
      "(291, 39.11320495605469)\n",
      "(292, 37.21685028076172)\n",
      "(293, 35.73814010620117)\n",
      "(294, 37.167335510253906)\n",
      "(295, 38.168800354003906)\n",
      "(296, 35.02265930175781)\n",
      "(297, 33.48463821411133)\n",
      "(298, 37.27009201049805)\n",
      "(299, 37.16543960571289)\n",
      "(300, 34.26027297973633)\n",
      "(301, 33.951087951660156)\n",
      "(302, 35.094093322753906)\n",
      "(303, 36.881351470947266)\n",
      "(304, 36.02190399169922)\n",
      "(305, 35.64499282836914)\n",
      "(306, 35.18646240234375)\n",
      "(307, 34.945064544677734)\n",
      "(308, 34.82265090942383)\n",
      "(309, 35.18702697753906)\n",
      "(310, 34.633968353271484)\n",
      "(311, 32.42741012573242)\n",
      "(312, 34.28399658203125)\n",
      "(313, 34.16722869873047)\n",
      "(314, 32.217193603515625)\n",
      "(315, 33.10846710205078)\n",
      "(316, 34.9874267578125)\n",
      "(317, 35.11448287963867)\n",
      "(318, 32.730751037597656)\n",
      "(319, 33.802677154541016)\n",
      "(320, 34.2617301940918)\n",
      "(321, 33.01331329345703)\n",
      "(322, 32.604583740234375)\n",
      "(323, 35.35720443725586)\n",
      "(324, 35.20132064819336)\n",
      "(325, 32.773441314697266)\n",
      "(326, 31.967575073242188)\n",
      "(327, 33.050559997558594)\n",
      "(328, 35.00153350830078)\n",
      "(329, 33.839012145996094)\n",
      "(330, 33.884376525878906)\n",
      "(331, 32.723941802978516)\n",
      "(332, 32.06863784790039)\n",
      "(333, 32.69590377807617)\n",
      "(334, 33.25339126586914)\n",
      "(335, 30.682132720947266)\n",
      "(336, 33.707576751708984)\n",
      "(337, 35.04383087158203)\n",
      "(338, 33.038761138916016)\n",
      "(339, 32.488643646240234)\n",
      "(340, 33.079959869384766)\n",
      "(341, 31.505027770996094)\n",
      "(342, 29.66712188720703)\n",
      "(343, 32.67533493041992)\n",
      "(344, 34.68482971191406)\n",
      "(345, 32.23786163330078)\n",
      "(346, 31.826313018798828)\n",
      "(347, 34.10649490356445)\n",
      "(348, 33.85682678222656)\n",
      "(349, 34.00091552734375)\n",
      "(350, 33.29598617553711)\n",
      "(351, 32.29433059692383)\n",
      "(352, 33.57102966308594)\n",
      "(353, 32.9967155456543)\n",
      "(354, 33.68656539916992)\n",
      "(355, 34.129825592041016)\n",
      "(356, 33.28740310668945)\n",
      "(357, 31.062341690063477)\n",
      "(358, 29.65216636657715)\n",
      "(359, 34.69001388549805)\n",
      "(360, 35.25966262817383)\n",
      "(361, 32.0895881652832)\n",
      "(362, 31.75063133239746)\n",
      "(363, 31.453718185424805)\n",
      "(364, 31.266605377197266)\n",
      "(365, 31.3829288482666)\n",
      "(366, 31.765396118164062)\n",
      "(367, 32.47170639038086)\n",
      "(368, 30.62179946899414)\n",
      "(369, 30.49130630493164)\n",
      "(370, 31.31349754333496)\n",
      "(371, 31.16227912902832)\n",
      "(372, 31.249460220336914)\n",
      "(373, 31.47883415222168)\n",
      "(374, 34.05643844604492)\n",
      "(375, 31.92725372314453)\n",
      "(376, 29.919641494750977)\n",
      "(377, 28.981327056884766)\n",
      "(378, 29.527597427368164)\n",
      "(379, 32.51094436645508)\n",
      "(380, 32.72996139526367)\n",
      "(381, 28.01374053955078)\n",
      "(382, 29.856233596801758)\n",
      "(383, 32.41706466674805)\n",
      "(384, 29.510929107666016)\n",
      "(385, 31.097026824951172)\n",
      "(386, 31.400236129760742)\n",
      "(387, 29.506755828857422)\n",
      "(388, 27.598102569580078)\n",
      "(389, 30.474348068237305)\n",
      "(390, 32.57722091674805)\n",
      "(391, 28.9249210357666)\n",
      "(392, 28.69258689880371)\n",
      "(393, 29.722904205322266)\n",
      "(394, 31.23549461364746)\n",
      "(395, 31.27072525024414)\n",
      "(396, 30.744657516479492)\n",
      "(397, 29.75019645690918)\n",
      "(398, 30.959609985351562)\n",
      "(399, 31.971088409423828)\n",
      "(400, 29.884910583496094)\n",
      "(401, 30.90875244140625)\n",
      "(402, 29.91558074951172)\n",
      "(403, 29.886398315429688)\n",
      "(404, 32.13124465942383)\n",
      "(405, 29.4747257232666)\n",
      "(406, 30.53960418701172)\n",
      "(407, 32.8235969543457)\n",
      "(408, 31.628803253173828)\n",
      "(409, 28.59681510925293)\n",
      "(410, 29.272735595703125)\n",
      "(411, 31.567895889282227)\n",
      "(412, 28.594337463378906)\n",
      "(413, 28.648494720458984)\n",
      "(414, 30.53948211669922)\n",
      "(415, 27.24974822998047)\n",
      "(416, 28.708322525024414)\n",
      "(417, 28.44729995727539)\n",
      "(418, 29.575403213500977)\n",
      "(419, 28.226455688476562)\n",
      "(420, 27.596269607543945)\n",
      "(421, 29.75578498840332)\n",
      "(422, 29.611539840698242)\n",
      "(423, 29.47818946838379)\n",
      "(424, 29.42294692993164)\n",
      "(425, 27.70477294921875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 28.127729415893555)\n",
      "(427, 30.728391647338867)\n",
      "(428, 30.931137084960938)\n",
      "(429, 30.750869750976562)\n",
      "(430, 30.92955780029297)\n",
      "(431, 30.933921813964844)\n",
      "(432, 29.153017044067383)\n",
      "(433, 27.616147994995117)\n",
      "(434, 30.155611038208008)\n",
      "(435, 30.80282211303711)\n",
      "(436, 28.818958282470703)\n",
      "(437, 26.5113582611084)\n",
      "(438, 30.335683822631836)\n",
      "(439, 30.265567779541016)\n",
      "(440, 28.298402786254883)\n",
      "(441, 30.54079818725586)\n",
      "(442, 28.15666389465332)\n",
      "(443, 28.182588577270508)\n",
      "(444, 29.00093650817871)\n",
      "(445, 26.8619327545166)\n",
      "(446, 28.400569915771484)\n",
      "(447, 31.238452911376953)\n",
      "(448, 28.85932731628418)\n",
      "(449, 28.795751571655273)\n",
      "(450, 29.51760482788086)\n",
      "(451, 28.49301528930664)\n",
      "(452, 29.56645393371582)\n",
      "(453, 29.374221801757812)\n",
      "(454, 29.153165817260742)\n",
      "(455, 29.445430755615234)\n",
      "(456, 28.09492301940918)\n",
      "(457, 29.186723709106445)\n",
      "(458, 31.38745880126953)\n",
      "(459, 30.934707641601562)\n",
      "(460, 27.527727127075195)\n",
      "(461, 29.951940536499023)\n",
      "(462, 32.035919189453125)\n",
      "(463, 29.5843448638916)\n",
      "(464, 30.328407287597656)\n",
      "(465, 30.375699996948242)\n",
      "(466, 29.63230323791504)\n",
      "(467, 31.162425994873047)\n",
      "(468, 31.77692985534668)\n",
      "(469, 29.357975006103516)\n",
      "(470, 30.866043090820312)\n",
      "(471, 30.95069694519043)\n",
      "(472, 26.812179565429688)\n",
      "(473, 30.003002166748047)\n",
      "(474, 29.387351989746094)\n",
      "(475, 29.894542694091797)\n",
      "(476, 31.73539161682129)\n",
      "(477, 29.28724479675293)\n",
      "(478, 29.751420974731445)\n",
      "(479, 30.061935424804688)\n",
      "(480, 30.585451126098633)\n",
      "(481, 30.842891693115234)\n",
      "(482, 31.734786987304688)\n",
      "(483, 31.88089370727539)\n",
      "(484, 31.14584732055664)\n",
      "(485, 33.57748794555664)\n",
      "(486, 35.34467697143555)\n",
      "(487, 36.107200622558594)\n",
      "(488, 34.570457458496094)\n",
      "(489, 33.56267166137695)\n",
      "(490, 33.82291793823242)\n",
      "(491, 33.10654830932617)\n",
      "(492, 36.039344787597656)\n",
      "(493, 36.37482452392578)\n",
      "(494, 34.49723815917969)\n",
      "(495, 34.98539352416992)\n",
      "(496, 35.22417449951172)\n",
      "(497, 33.52705764770508)\n",
      "(498, 32.78724670410156)\n",
      "(499, 32.48811721801758)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim\n",
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters (with `torch.no_grad()` or `.data` to avoid tracking history in autograd). This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we will optimize the model using the Adam algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, tensor(713.8069, grad_fn=<MseLossBackward>))\n",
      "(1, tensor(695.4018, grad_fn=<MseLossBackward>))\n",
      "(2, tensor(677.5189, grad_fn=<MseLossBackward>))\n",
      "(3, tensor(660.0837, grad_fn=<MseLossBackward>))\n",
      "(4, tensor(643.0918, grad_fn=<MseLossBackward>))\n",
      "(5, tensor(626.6547, grad_fn=<MseLossBackward>))\n",
      "(6, tensor(610.8483, grad_fn=<MseLossBackward>))\n",
      "(7, tensor(595.5851, grad_fn=<MseLossBackward>))\n",
      "(8, tensor(580.7399, grad_fn=<MseLossBackward>))\n",
      "(9, tensor(566.2960, grad_fn=<MseLossBackward>))\n",
      "(10, tensor(552.3088, grad_fn=<MseLossBackward>))\n",
      "(11, tensor(538.9022, grad_fn=<MseLossBackward>))\n",
      "(12, tensor(525.9811, grad_fn=<MseLossBackward>))\n",
      "(13, tensor(513.5200, grad_fn=<MseLossBackward>))\n",
      "(14, tensor(501.4097, grad_fn=<MseLossBackward>))\n",
      "(15, tensor(489.7232, grad_fn=<MseLossBackward>))\n",
      "(16, tensor(478.4750, grad_fn=<MseLossBackward>))\n",
      "(17, tensor(467.5736, grad_fn=<MseLossBackward>))\n",
      "(18, tensor(456.9418, grad_fn=<MseLossBackward>))\n",
      "(19, tensor(446.6033, grad_fn=<MseLossBackward>))\n",
      "(20, tensor(436.5732, grad_fn=<MseLossBackward>))\n",
      "(21, tensor(426.8274, grad_fn=<MseLossBackward>))\n",
      "(22, tensor(417.4140, grad_fn=<MseLossBackward>))\n",
      "(23, tensor(408.3019, grad_fn=<MseLossBackward>))\n",
      "(24, tensor(399.4283, grad_fn=<MseLossBackward>))\n",
      "(25, tensor(390.8217, grad_fn=<MseLossBackward>))\n",
      "(26, tensor(382.5197, grad_fn=<MseLossBackward>))\n",
      "(27, tensor(374.4665, grad_fn=<MseLossBackward>))\n",
      "(28, tensor(366.6303, grad_fn=<MseLossBackward>))\n",
      "(29, tensor(359.0154, grad_fn=<MseLossBackward>))\n",
      "(30, tensor(351.5665, grad_fn=<MseLossBackward>))\n",
      "(31, tensor(344.2892, grad_fn=<MseLossBackward>))\n",
      "(32, tensor(337.1668, grad_fn=<MseLossBackward>))\n",
      "(33, tensor(330.2333, grad_fn=<MseLossBackward>))\n",
      "(34, tensor(323.4570, grad_fn=<MseLossBackward>))\n",
      "(35, tensor(316.8251, grad_fn=<MseLossBackward>))\n",
      "(36, tensor(310.3273, grad_fn=<MseLossBackward>))\n",
      "(37, tensor(303.9416, grad_fn=<MseLossBackward>))\n",
      "(38, tensor(297.6779, grad_fn=<MseLossBackward>))\n",
      "(39, tensor(291.5670, grad_fn=<MseLossBackward>))\n",
      "(40, tensor(285.5821, grad_fn=<MseLossBackward>))\n",
      "(41, tensor(279.6932, grad_fn=<MseLossBackward>))\n",
      "(42, tensor(273.8919, grad_fn=<MseLossBackward>))\n",
      "(43, tensor(268.1895, grad_fn=<MseLossBackward>))\n",
      "(44, tensor(262.5886, grad_fn=<MseLossBackward>))\n",
      "(45, tensor(257.0930, grad_fn=<MseLossBackward>))\n",
      "(46, tensor(251.6872, grad_fn=<MseLossBackward>))\n",
      "(47, tensor(246.3611, grad_fn=<MseLossBackward>))\n",
      "(48, tensor(241.1200, grad_fn=<MseLossBackward>))\n",
      "(49, tensor(235.9793, grad_fn=<MseLossBackward>))\n",
      "(50, tensor(230.9310, grad_fn=<MseLossBackward>))\n",
      "(51, tensor(225.9669, grad_fn=<MseLossBackward>))\n",
      "(52, tensor(221.0760, grad_fn=<MseLossBackward>))\n",
      "(53, tensor(216.2590, grad_fn=<MseLossBackward>))\n",
      "(54, tensor(211.5386, grad_fn=<MseLossBackward>))\n",
      "(55, tensor(206.9051, grad_fn=<MseLossBackward>))\n",
      "(56, tensor(202.3389, grad_fn=<MseLossBackward>))\n",
      "(57, tensor(197.8525, grad_fn=<MseLossBackward>))\n",
      "(58, tensor(193.4406, grad_fn=<MseLossBackward>))\n",
      "(59, tensor(189.1122, grad_fn=<MseLossBackward>))\n",
      "(60, tensor(184.8587, grad_fn=<MseLossBackward>))\n",
      "(61, tensor(180.6647, grad_fn=<MseLossBackward>))\n",
      "(62, tensor(176.5278, grad_fn=<MseLossBackward>))\n",
      "(63, tensor(172.4546, grad_fn=<MseLossBackward>))\n",
      "(64, tensor(168.4387, grad_fn=<MseLossBackward>))\n",
      "(65, tensor(164.4837, grad_fn=<MseLossBackward>))\n",
      "(66, tensor(160.5994, grad_fn=<MseLossBackward>))\n",
      "(67, tensor(156.7855, grad_fn=<MseLossBackward>))\n",
      "(68, tensor(153.0382, grad_fn=<MseLossBackward>))\n",
      "(69, tensor(149.3511, grad_fn=<MseLossBackward>))\n",
      "(70, tensor(145.7285, grad_fn=<MseLossBackward>))\n",
      "(71, tensor(142.1624, grad_fn=<MseLossBackward>))\n",
      "(72, tensor(138.6552, grad_fn=<MseLossBackward>))\n",
      "(73, tensor(135.2202, grad_fn=<MseLossBackward>))\n",
      "(74, tensor(131.8579, grad_fn=<MseLossBackward>))\n",
      "(75, tensor(128.5545, grad_fn=<MseLossBackward>))\n",
      "(76, tensor(125.3194, grad_fn=<MseLossBackward>))\n",
      "(77, tensor(122.1441, grad_fn=<MseLossBackward>))\n",
      "(78, tensor(119.0247, grad_fn=<MseLossBackward>))\n",
      "(79, tensor(115.9695, grad_fn=<MseLossBackward>))\n",
      "(80, tensor(112.9782, grad_fn=<MseLossBackward>))\n",
      "(81, tensor(110.0428, grad_fn=<MseLossBackward>))\n",
      "(82, tensor(107.1680, grad_fn=<MseLossBackward>))\n",
      "(83, tensor(104.3507, grad_fn=<MseLossBackward>))\n",
      "(84, tensor(101.5896, grad_fn=<MseLossBackward>))\n",
      "(85, tensor(98.8863, grad_fn=<MseLossBackward>))\n",
      "(86, tensor(96.2379, grad_fn=<MseLossBackward>))\n",
      "(87, tensor(93.6451, grad_fn=<MseLossBackward>))\n",
      "(88, tensor(91.1118, grad_fn=<MseLossBackward>))\n",
      "(89, tensor(88.6282, grad_fn=<MseLossBackward>))\n",
      "(90, tensor(86.1921, grad_fn=<MseLossBackward>))\n",
      "(91, tensor(83.8043, grad_fn=<MseLossBackward>))\n",
      "(92, tensor(81.4729, grad_fn=<MseLossBackward>))\n",
      "(93, tensor(79.1926, grad_fn=<MseLossBackward>))\n",
      "(94, tensor(76.9641, grad_fn=<MseLossBackward>))\n",
      "(95, tensor(74.7896, grad_fn=<MseLossBackward>))\n",
      "(96, tensor(72.6692, grad_fn=<MseLossBackward>))\n",
      "(97, tensor(70.5914, grad_fn=<MseLossBackward>))\n",
      "(98, tensor(68.5622, grad_fn=<MseLossBackward>))\n",
      "(99, tensor(66.5838, grad_fn=<MseLossBackward>))\n",
      "(100, tensor(64.6499, grad_fn=<MseLossBackward>))\n",
      "(101, tensor(62.7632, grad_fn=<MseLossBackward>))\n",
      "(102, tensor(60.9248, grad_fn=<MseLossBackward>))\n",
      "(103, tensor(59.1314, grad_fn=<MseLossBackward>))\n",
      "(104, tensor(57.3775, grad_fn=<MseLossBackward>))\n",
      "(105, tensor(55.6700, grad_fn=<MseLossBackward>))\n",
      "(106, tensor(54.0070, grad_fn=<MseLossBackward>))\n",
      "(107, tensor(52.3828, grad_fn=<MseLossBackward>))\n",
      "(108, tensor(50.7977, grad_fn=<MseLossBackward>))\n",
      "(109, tensor(49.2553, grad_fn=<MseLossBackward>))\n",
      "(110, tensor(47.7494, grad_fn=<MseLossBackward>))\n",
      "(111, tensor(46.2827, grad_fn=<MseLossBackward>))\n",
      "(112, tensor(44.8568, grad_fn=<MseLossBackward>))\n",
      "(113, tensor(43.4673, grad_fn=<MseLossBackward>))\n",
      "(114, tensor(42.1130, grad_fn=<MseLossBackward>))\n",
      "(115, tensor(40.7952, grad_fn=<MseLossBackward>))\n",
      "(116, tensor(39.5138, grad_fn=<MseLossBackward>))\n",
      "(117, tensor(38.2669, grad_fn=<MseLossBackward>))\n",
      "(118, tensor(37.0535, grad_fn=<MseLossBackward>))\n",
      "(119, tensor(35.8717, grad_fn=<MseLossBackward>))\n",
      "(120, tensor(34.7199, grad_fn=<MseLossBackward>))\n",
      "(121, tensor(33.5973, grad_fn=<MseLossBackward>))\n",
      "(122, tensor(32.5043, grad_fn=<MseLossBackward>))\n",
      "(123, tensor(31.4402, grad_fn=<MseLossBackward>))\n",
      "(124, tensor(30.4065, grad_fn=<MseLossBackward>))\n",
      "(125, tensor(29.4013, grad_fn=<MseLossBackward>))\n",
      "(126, tensor(28.4259, grad_fn=<MseLossBackward>))\n",
      "(127, tensor(27.4767, grad_fn=<MseLossBackward>))\n",
      "(128, tensor(26.5555, grad_fn=<MseLossBackward>))\n",
      "(129, tensor(25.6612, grad_fn=<MseLossBackward>))\n",
      "(130, tensor(24.7942, grad_fn=<MseLossBackward>))\n",
      "(131, tensor(23.9515, grad_fn=<MseLossBackward>))\n",
      "(132, tensor(23.1345, grad_fn=<MseLossBackward>))\n",
      "(133, tensor(22.3427, grad_fn=<MseLossBackward>))\n",
      "(134, tensor(21.5738, grad_fn=<MseLossBackward>))\n",
      "(135, tensor(20.8287, grad_fn=<MseLossBackward>))\n",
      "(136, tensor(20.1073, grad_fn=<MseLossBackward>))\n",
      "(137, tensor(19.4078, grad_fn=<MseLossBackward>))\n",
      "(138, tensor(18.7306, grad_fn=<MseLossBackward>))\n",
      "(139, tensor(18.0748, grad_fn=<MseLossBackward>))\n",
      "(140, tensor(17.4380, grad_fn=<MseLossBackward>))\n",
      "(141, tensor(16.8212, grad_fn=<MseLossBackward>))\n",
      "(142, tensor(16.2254, grad_fn=<MseLossBackward>))\n",
      "(143, tensor(15.6495, grad_fn=<MseLossBackward>))\n",
      "(144, tensor(15.0932, grad_fn=<MseLossBackward>))\n",
      "(145, tensor(14.5539, grad_fn=<MseLossBackward>))\n",
      "(146, tensor(14.0325, grad_fn=<MseLossBackward>))\n",
      "(147, tensor(13.5284, grad_fn=<MseLossBackward>))\n",
      "(148, tensor(13.0407, grad_fn=<MseLossBackward>))\n",
      "(149, tensor(12.5687, grad_fn=<MseLossBackward>))\n",
      "(150, tensor(12.1121, grad_fn=<MseLossBackward>))\n",
      "(151, tensor(11.6714, grad_fn=<MseLossBackward>))\n",
      "(152, tensor(11.2451, grad_fn=<MseLossBackward>))\n",
      "(153, tensor(10.8332, grad_fn=<MseLossBackward>))\n",
      "(154, tensor(10.4354, grad_fn=<MseLossBackward>))\n",
      "(155, tensor(10.0510, grad_fn=<MseLossBackward>))\n",
      "(156, tensor(9.6794, grad_fn=<MseLossBackward>))\n",
      "(157, tensor(9.3201, grad_fn=<MseLossBackward>))\n",
      "(158, tensor(8.9732, grad_fn=<MseLossBackward>))\n",
      "(159, tensor(8.6382, grad_fn=<MseLossBackward>))\n",
      "(160, tensor(8.3147, grad_fn=<MseLossBackward>))\n",
      "(161, tensor(8.0020, grad_fn=<MseLossBackward>))\n",
      "(162, tensor(7.7000, grad_fn=<MseLossBackward>))\n",
      "(163, tensor(7.4084, grad_fn=<MseLossBackward>))\n",
      "(164, tensor(7.1272, grad_fn=<MseLossBackward>))\n",
      "(165, tensor(6.8558, grad_fn=<MseLossBackward>))\n",
      "(166, tensor(6.5938, grad_fn=<MseLossBackward>))\n",
      "(167, tensor(6.3413, grad_fn=<MseLossBackward>))\n",
      "(168, tensor(6.0975, grad_fn=<MseLossBackward>))\n",
      "(169, tensor(5.8624, grad_fn=<MseLossBackward>))\n",
      "(170, tensor(5.6359, grad_fn=<MseLossBackward>))\n",
      "(171, tensor(5.4173, grad_fn=<MseLossBackward>))\n",
      "(172, tensor(5.2068, grad_fn=<MseLossBackward>))\n",
      "(173, tensor(5.0039, grad_fn=<MseLossBackward>))\n",
      "(174, tensor(4.8083, grad_fn=<MseLossBackward>))\n",
      "(175, tensor(4.6200, grad_fn=<MseLossBackward>))\n",
      "(176, tensor(4.4386, grad_fn=<MseLossBackward>))\n",
      "(177, tensor(4.2641, grad_fn=<MseLossBackward>))\n",
      "(178, tensor(4.0959, grad_fn=<MseLossBackward>))\n",
      "(179, tensor(3.9339, grad_fn=<MseLossBackward>))\n",
      "(180, tensor(3.7780, grad_fn=<MseLossBackward>))\n",
      "(181, tensor(3.6279, grad_fn=<MseLossBackward>))\n",
      "(182, tensor(3.4835, grad_fn=<MseLossBackward>))\n",
      "(183, tensor(3.3445, grad_fn=<MseLossBackward>))\n",
      "(184, tensor(3.2107, grad_fn=<MseLossBackward>))\n",
      "(185, tensor(3.0821, grad_fn=<MseLossBackward>))\n",
      "(186, tensor(2.9584, grad_fn=<MseLossBackward>))\n",
      "(187, tensor(2.8395, grad_fn=<MseLossBackward>))\n",
      "(188, tensor(2.7251, grad_fn=<MseLossBackward>))\n",
      "(189, tensor(2.6152, grad_fn=<MseLossBackward>))\n",
      "(190, tensor(2.5093, grad_fn=<MseLossBackward>))\n",
      "(191, tensor(2.4077, grad_fn=<MseLossBackward>))\n",
      "(192, tensor(2.3100, grad_fn=<MseLossBackward>))\n",
      "(193, tensor(2.2160, grad_fn=<MseLossBackward>))\n",
      "(194, tensor(2.1258, grad_fn=<MseLossBackward>))\n",
      "(195, tensor(2.0392, grad_fn=<MseLossBackward>))\n",
      "(196, tensor(1.9559, grad_fn=<MseLossBackward>))\n",
      "(197, tensor(1.8760, grad_fn=<MseLossBackward>))\n",
      "(198, tensor(1.7992, grad_fn=<MseLossBackward>))\n",
      "(199, tensor(1.7254, grad_fn=<MseLossBackward>))\n",
      "(200, tensor(1.6545, grad_fn=<MseLossBackward>))\n",
      "(201, tensor(1.5864, grad_fn=<MseLossBackward>))\n",
      "(202, tensor(1.5210, grad_fn=<MseLossBackward>))\n",
      "(203, tensor(1.4583, grad_fn=<MseLossBackward>))\n",
      "(204, tensor(1.3981, grad_fn=<MseLossBackward>))\n",
      "(205, tensor(1.3402, grad_fn=<MseLossBackward>))\n",
      "(206, tensor(1.2846, grad_fn=<MseLossBackward>))\n",
      "(207, tensor(1.2313, grad_fn=<MseLossBackward>))\n",
      "(208, tensor(1.1800, grad_fn=<MseLossBackward>))\n",
      "(209, tensor(1.1308, grad_fn=<MseLossBackward>))\n",
      "(210, tensor(1.0836, grad_fn=<MseLossBackward>))\n",
      "(211, tensor(1.0382, grad_fn=<MseLossBackward>))\n",
      "(212, tensor(0.9947, grad_fn=<MseLossBackward>))\n",
      "(213, tensor(0.9529, grad_fn=<MseLossBackward>))\n",
      "(214, tensor(0.9129, grad_fn=<MseLossBackward>))\n",
      "(215, tensor(0.8745, grad_fn=<MseLossBackward>))\n",
      "(216, tensor(0.8376, grad_fn=<MseLossBackward>))\n",
      "(217, tensor(0.8023, grad_fn=<MseLossBackward>))\n",
      "(218, tensor(0.7685, grad_fn=<MseLossBackward>))\n",
      "(219, tensor(0.7360, grad_fn=<MseLossBackward>))\n",
      "(220, tensor(0.7049, grad_fn=<MseLossBackward>))\n",
      "(221, tensor(0.6750, grad_fn=<MseLossBackward>))\n",
      "(222, tensor(0.6464, grad_fn=<MseLossBackward>))\n",
      "(223, tensor(0.6190, grad_fn=<MseLossBackward>))\n",
      "(224, tensor(0.5927, grad_fn=<MseLossBackward>))\n",
      "(225, tensor(0.5675, grad_fn=<MseLossBackward>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, tensor(0.5434, grad_fn=<MseLossBackward>))\n",
      "(227, tensor(0.5203, grad_fn=<MseLossBackward>))\n",
      "(228, tensor(0.4981, grad_fn=<MseLossBackward>))\n",
      "(229, tensor(0.4768, grad_fn=<MseLossBackward>))\n",
      "(230, tensor(0.4565, grad_fn=<MseLossBackward>))\n",
      "(231, tensor(0.4370, grad_fn=<MseLossBackward>))\n",
      "(232, tensor(0.4183, grad_fn=<MseLossBackward>))\n",
      "(233, tensor(0.4004, grad_fn=<MseLossBackward>))\n",
      "(234, tensor(0.3832, grad_fn=<MseLossBackward>))\n",
      "(235, tensor(0.3668, grad_fn=<MseLossBackward>))\n",
      "(236, tensor(0.3510, grad_fn=<MseLossBackward>))\n",
      "(237, tensor(0.3359, grad_fn=<MseLossBackward>))\n",
      "(238, tensor(0.3215, grad_fn=<MseLossBackward>))\n",
      "(239, tensor(0.3076, grad_fn=<MseLossBackward>))\n",
      "(240, tensor(0.2943, grad_fn=<MseLossBackward>))\n",
      "(241, tensor(0.2816, grad_fn=<MseLossBackward>))\n",
      "(242, tensor(0.2695, grad_fn=<MseLossBackward>))\n",
      "(243, tensor(0.2578, grad_fn=<MseLossBackward>))\n",
      "(244, tensor(0.2466, grad_fn=<MseLossBackward>))\n",
      "(245, tensor(0.2360, grad_fn=<MseLossBackward>))\n",
      "(246, tensor(0.2257, grad_fn=<MseLossBackward>))\n",
      "(247, tensor(0.2159, grad_fn=<MseLossBackward>))\n",
      "(248, tensor(0.2065, grad_fn=<MseLossBackward>))\n",
      "(249, tensor(0.1976, grad_fn=<MseLossBackward>))\n",
      "(250, tensor(0.1890, grad_fn=<MseLossBackward>))\n",
      "(251, tensor(0.1807, grad_fn=<MseLossBackward>))\n",
      "(252, tensor(0.1728, grad_fn=<MseLossBackward>))\n",
      "(253, tensor(0.1653, grad_fn=<MseLossBackward>))\n",
      "(254, tensor(0.1581, grad_fn=<MseLossBackward>))\n",
      "(255, tensor(0.1512, grad_fn=<MseLossBackward>))\n",
      "(256, tensor(0.1445, grad_fn=<MseLossBackward>))\n",
      "(257, tensor(0.1382, grad_fn=<MseLossBackward>))\n",
      "(258, tensor(0.1322, grad_fn=<MseLossBackward>))\n",
      "(259, tensor(0.1263, grad_fn=<MseLossBackward>))\n",
      "(260, tensor(0.1208, grad_fn=<MseLossBackward>))\n",
      "(261, tensor(0.1155, grad_fn=<MseLossBackward>))\n",
      "(262, tensor(0.1104, grad_fn=<MseLossBackward>))\n",
      "(263, tensor(0.1055, grad_fn=<MseLossBackward>))\n",
      "(264, tensor(0.1009, grad_fn=<MseLossBackward>))\n",
      "(265, tensor(0.0964, grad_fn=<MseLossBackward>))\n",
      "(266, tensor(0.0922, grad_fn=<MseLossBackward>))\n",
      "(267, tensor(0.0881, grad_fn=<MseLossBackward>))\n",
      "(268, tensor(0.0842, grad_fn=<MseLossBackward>))\n",
      "(269, tensor(0.0805, grad_fn=<MseLossBackward>))\n",
      "(270, tensor(0.0769, grad_fn=<MseLossBackward>))\n",
      "(271, tensor(0.0735, grad_fn=<MseLossBackward>))\n",
      "(272, tensor(0.0703, grad_fn=<MseLossBackward>))\n",
      "(273, tensor(0.0671, grad_fn=<MseLossBackward>))\n",
      "(274, tensor(0.0642, grad_fn=<MseLossBackward>))\n",
      "(275, tensor(0.0613, grad_fn=<MseLossBackward>))\n",
      "(276, tensor(0.0586, grad_fn=<MseLossBackward>))\n",
      "(277, tensor(0.0560, grad_fn=<MseLossBackward>))\n",
      "(278, tensor(0.0535, grad_fn=<MseLossBackward>))\n",
      "(279, tensor(0.0511, grad_fn=<MseLossBackward>))\n",
      "(280, tensor(0.0488, grad_fn=<MseLossBackward>))\n",
      "(281, tensor(0.0466, grad_fn=<MseLossBackward>))\n",
      "(282, tensor(0.0446, grad_fn=<MseLossBackward>))\n",
      "(283, tensor(0.0426, grad_fn=<MseLossBackward>))\n",
      "(284, tensor(0.0407, grad_fn=<MseLossBackward>))\n",
      "(285, tensor(0.0388, grad_fn=<MseLossBackward>))\n",
      "(286, tensor(0.0371, grad_fn=<MseLossBackward>))\n",
      "(287, tensor(0.0354, grad_fn=<MseLossBackward>))\n",
      "(288, tensor(0.0338, grad_fn=<MseLossBackward>))\n",
      "(289, tensor(0.0323, grad_fn=<MseLossBackward>))\n",
      "(290, tensor(0.0309, grad_fn=<MseLossBackward>))\n",
      "(291, tensor(0.0295, grad_fn=<MseLossBackward>))\n",
      "(292, tensor(0.0282, grad_fn=<MseLossBackward>))\n",
      "(293, tensor(0.0269, grad_fn=<MseLossBackward>))\n",
      "(294, tensor(0.0257, grad_fn=<MseLossBackward>))\n",
      "(295, tensor(0.0245, grad_fn=<MseLossBackward>))\n",
      "(296, tensor(0.0234, grad_fn=<MseLossBackward>))\n",
      "(297, tensor(0.0224, grad_fn=<MseLossBackward>))\n",
      "(298, tensor(0.0213, grad_fn=<MseLossBackward>))\n",
      "(299, tensor(0.0204, grad_fn=<MseLossBackward>))\n",
      "(300, tensor(0.0195, grad_fn=<MseLossBackward>))\n",
      "(301, tensor(0.0186, grad_fn=<MseLossBackward>))\n",
      "(302, tensor(0.0177, grad_fn=<MseLossBackward>))\n",
      "(303, tensor(0.0169, grad_fn=<MseLossBackward>))\n",
      "(304, tensor(0.0162, grad_fn=<MseLossBackward>))\n",
      "(305, tensor(0.0154, grad_fn=<MseLossBackward>))\n",
      "(306, tensor(0.0147, grad_fn=<MseLossBackward>))\n",
      "(307, tensor(0.0141, grad_fn=<MseLossBackward>))\n",
      "(308, tensor(0.0134, grad_fn=<MseLossBackward>))\n",
      "(309, tensor(0.0128, grad_fn=<MseLossBackward>))\n",
      "(310, tensor(0.0122, grad_fn=<MseLossBackward>))\n",
      "(311, tensor(0.0117, grad_fn=<MseLossBackward>))\n",
      "(312, tensor(0.0111, grad_fn=<MseLossBackward>))\n",
      "(313, tensor(0.0106, grad_fn=<MseLossBackward>))\n",
      "(314, tensor(0.0101, grad_fn=<MseLossBackward>))\n",
      "(315, tensor(0.0097, grad_fn=<MseLossBackward>))\n",
      "(316, tensor(0.0092, grad_fn=<MseLossBackward>))\n",
      "(317, tensor(0.0088, grad_fn=<MseLossBackward>))\n",
      "(318, tensor(0.0084, grad_fn=<MseLossBackward>))\n",
      "(319, tensor(0.0080, grad_fn=<MseLossBackward>))\n",
      "(320, tensor(0.0076, grad_fn=<MseLossBackward>))\n",
      "(321, tensor(0.0073, grad_fn=<MseLossBackward>))\n",
      "(322, tensor(0.0070, grad_fn=<MseLossBackward>))\n",
      "(323, tensor(0.0066, grad_fn=<MseLossBackward>))\n",
      "(324, tensor(0.0063, grad_fn=<MseLossBackward>))\n",
      "(325, tensor(0.0060, grad_fn=<MseLossBackward>))\n",
      "(326, tensor(0.0058, grad_fn=<MseLossBackward>))\n",
      "(327, tensor(0.0055, grad_fn=<MseLossBackward>))\n",
      "(328, tensor(0.0052, grad_fn=<MseLossBackward>))\n",
      "(329, tensor(0.0050, grad_fn=<MseLossBackward>))\n",
      "(330, tensor(0.0048, grad_fn=<MseLossBackward>))\n",
      "(331, tensor(0.0046, grad_fn=<MseLossBackward>))\n",
      "(332, tensor(0.0043, grad_fn=<MseLossBackward>))\n",
      "(333, tensor(0.0041, grad_fn=<MseLossBackward>))\n",
      "(334, tensor(0.0039, grad_fn=<MseLossBackward>))\n",
      "(335, tensor(0.0038, grad_fn=<MseLossBackward>))\n",
      "(336, tensor(0.0036, grad_fn=<MseLossBackward>))\n",
      "(337, tensor(0.0034, grad_fn=<MseLossBackward>))\n",
      "(338, tensor(0.0033, grad_fn=<MseLossBackward>))\n",
      "(339, tensor(0.0031, grad_fn=<MseLossBackward>))\n",
      "(340, tensor(0.0030, grad_fn=<MseLossBackward>))\n",
      "(341, tensor(0.0028, grad_fn=<MseLossBackward>))\n",
      "(342, tensor(0.0027, grad_fn=<MseLossBackward>))\n",
      "(343, tensor(0.0026, grad_fn=<MseLossBackward>))\n",
      "(344, tensor(0.0024, grad_fn=<MseLossBackward>))\n",
      "(345, tensor(0.0023, grad_fn=<MseLossBackward>))\n",
      "(346, tensor(0.0022, grad_fn=<MseLossBackward>))\n",
      "(347, tensor(0.0021, grad_fn=<MseLossBackward>))\n",
      "(348, tensor(0.0020, grad_fn=<MseLossBackward>))\n",
      "(349, tensor(0.0019, grad_fn=<MseLossBackward>))\n",
      "(350, tensor(0.0018, grad_fn=<MseLossBackward>))\n",
      "(351, tensor(0.0017, grad_fn=<MseLossBackward>))\n",
      "(352, tensor(0.0017, grad_fn=<MseLossBackward>))\n",
      "(353, tensor(0.0016, grad_fn=<MseLossBackward>))\n",
      "(354, tensor(0.0015, grad_fn=<MseLossBackward>))\n",
      "(355, tensor(0.0014, grad_fn=<MseLossBackward>))\n",
      "(356, tensor(0.0014, grad_fn=<MseLossBackward>))\n",
      "(357, tensor(0.0013, grad_fn=<MseLossBackward>))\n",
      "(358, tensor(0.0012, grad_fn=<MseLossBackward>))\n",
      "(359, tensor(0.0012, grad_fn=<MseLossBackward>))\n",
      "(360, tensor(0.0011, grad_fn=<MseLossBackward>))\n",
      "(361, tensor(0.0011, grad_fn=<MseLossBackward>))\n",
      "(362, tensor(0.0010, grad_fn=<MseLossBackward>))\n",
      "(363, tensor(0.0010, grad_fn=<MseLossBackward>))\n",
      "(364, tensor(0.0009, grad_fn=<MseLossBackward>))\n",
      "(365, tensor(0.0009, grad_fn=<MseLossBackward>))\n",
      "(366, tensor(0.0008, grad_fn=<MseLossBackward>))\n",
      "(367, tensor(0.0008, grad_fn=<MseLossBackward>))\n",
      "(368, tensor(0.0008, grad_fn=<MseLossBackward>))\n",
      "(369, tensor(0.0007, grad_fn=<MseLossBackward>))\n",
      "(370, tensor(0.0007, grad_fn=<MseLossBackward>))\n",
      "(371, tensor(0.0007, grad_fn=<MseLossBackward>))\n",
      "(372, tensor(0.0006, grad_fn=<MseLossBackward>))\n",
      "(373, tensor(0.0006, grad_fn=<MseLossBackward>))\n",
      "(374, tensor(0.0006, grad_fn=<MseLossBackward>))\n",
      "(375, tensor(0.0005, grad_fn=<MseLossBackward>))\n",
      "(376, tensor(0.0005, grad_fn=<MseLossBackward>))\n",
      "(377, tensor(0.0005, grad_fn=<MseLossBackward>))\n",
      "(378, tensor(0.0005, grad_fn=<MseLossBackward>))\n",
      "(379, tensor(0.0004, grad_fn=<MseLossBackward>))\n",
      "(380, tensor(0.0004, grad_fn=<MseLossBackward>))\n",
      "(381, tensor(0.0004, grad_fn=<MseLossBackward>))\n",
      "(382, tensor(0.0004, grad_fn=<MseLossBackward>))\n",
      "(383, tensor(0.0004, grad_fn=<MseLossBackward>))\n",
      "(384, tensor(0.0003, grad_fn=<MseLossBackward>))\n",
      "(385, tensor(0.0003, grad_fn=<MseLossBackward>))\n",
      "(386, tensor(0.0003, grad_fn=<MseLossBackward>))\n",
      "(387, tensor(0.0003, grad_fn=<MseLossBackward>))\n",
      "(388, tensor(0.0003, grad_fn=<MseLossBackward>))\n",
      "(389, tensor(0.0003, grad_fn=<MseLossBackward>))\n",
      "(390, tensor(0.0003, grad_fn=<MseLossBackward>))\n",
      "(391, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(392, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(393, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(394, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(395, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(396, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(397, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(398, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(399, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(400, tensor(0.0002, grad_fn=<MseLossBackward>))\n",
      "(401, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(402, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(403, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(404, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(405, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(406, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(407, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(408, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(409, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(410, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(411, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(412, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(413, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(414, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(415, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(416, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(417, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(418, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(419, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(420, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(421, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(422, tensor(0.0001, grad_fn=<MseLossBackward>))\n",
      "(423, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(424, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(425, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(426, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(427, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(428, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(429, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(430, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(431, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(432, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(433, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(434, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(435, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(436, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(437, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(438, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(439, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(440, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(441, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(442, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(443, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(444, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(445, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(446, tensor(0.0000, grad_fn=<MseLossBackward>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(448, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(449, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(450, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(451, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(452, tensor(0.0000, grad_fn=<MseLossBackward>))\n",
      "(453, tensor(9.5682e-06, grad_fn=<MseLossBackward>))\n",
      "(454, tensor(9.0586e-06, grad_fn=<MseLossBackward>))\n",
      "(455, tensor(8.5753e-06, grad_fn=<MseLossBackward>))\n",
      "(456, tensor(8.1178e-06, grad_fn=<MseLossBackward>))\n",
      "(457, tensor(7.6837e-06, grad_fn=<MseLossBackward>))\n",
      "(458, tensor(7.2717e-06, grad_fn=<MseLossBackward>))\n",
      "(459, tensor(6.8821e-06, grad_fn=<MseLossBackward>))\n",
      "(460, tensor(6.5125e-06, grad_fn=<MseLossBackward>))\n",
      "(461, tensor(6.1617e-06, grad_fn=<MseLossBackward>))\n",
      "(462, tensor(5.8295e-06, grad_fn=<MseLossBackward>))\n",
      "(463, tensor(5.5147e-06, grad_fn=<MseLossBackward>))\n",
      "(464, tensor(5.2167e-06, grad_fn=<MseLossBackward>))\n",
      "(465, tensor(4.9342e-06, grad_fn=<MseLossBackward>))\n",
      "(466, tensor(4.6664e-06, grad_fn=<MseLossBackward>))\n",
      "(467, tensor(4.4130e-06, grad_fn=<MseLossBackward>))\n",
      "(468, tensor(4.1726e-06, grad_fn=<MseLossBackward>))\n",
      "(469, tensor(3.9455e-06, grad_fn=<MseLossBackward>))\n",
      "(470, tensor(3.7298e-06, grad_fn=<MseLossBackward>))\n",
      "(471, tensor(3.5262e-06, grad_fn=<MseLossBackward>))\n",
      "(472, tensor(3.3331e-06, grad_fn=<MseLossBackward>))\n",
      "(473, tensor(3.1505e-06, grad_fn=<MseLossBackward>))\n",
      "(474, tensor(2.9773e-06, grad_fn=<MseLossBackward>))\n",
      "(475, tensor(2.8135e-06, grad_fn=<MseLossBackward>))\n",
      "(476, tensor(2.6585e-06, grad_fn=<MseLossBackward>))\n",
      "(477, tensor(2.5117e-06, grad_fn=<MseLossBackward>))\n",
      "(478, tensor(2.3731e-06, grad_fn=<MseLossBackward>))\n",
      "(479, tensor(2.2416e-06, grad_fn=<MseLossBackward>))\n",
      "(480, tensor(2.1173e-06, grad_fn=<MseLossBackward>))\n",
      "(481, tensor(1.9995e-06, grad_fn=<MseLossBackward>))\n",
      "(482, tensor(1.8884e-06, grad_fn=<MseLossBackward>))\n",
      "(483, tensor(1.7834e-06, grad_fn=<MseLossBackward>))\n",
      "(484, tensor(1.6837e-06, grad_fn=<MseLossBackward>))\n",
      "(485, tensor(1.5898e-06, grad_fn=<MseLossBackward>))\n",
      "(486, tensor(1.5004e-06, grad_fn=<MseLossBackward>))\n",
      "(487, tensor(1.4162e-06, grad_fn=<MseLossBackward>))\n",
      "(488, tensor(1.3367e-06, grad_fn=<MseLossBackward>))\n",
      "(489, tensor(1.2612e-06, grad_fn=<MseLossBackward>))\n",
      "(490, tensor(1.1902e-06, grad_fn=<MseLossBackward>))\n",
      "(491, tensor(1.1232e-06, grad_fn=<MseLossBackward>))\n",
      "(492, tensor(1.0596e-06, grad_fn=<MseLossBackward>))\n",
      "(493, tensor(9.9960e-07, grad_fn=<MseLossBackward>))\n",
      "(494, tensor(9.4265e-07, grad_fn=<MseLossBackward>))\n",
      "(495, tensor(8.8905e-07, grad_fn=<MseLossBackward>))\n",
      "(496, tensor(8.3854e-07, grad_fn=<MseLossBackward>))\n",
      "(497, tensor(7.9084e-07, grad_fn=<MseLossBackward>))\n",
      "(498, tensor(7.4554e-07, grad_fn=<MseLossBackward>))\n",
      "(499, tensor(7.0283e-07, grad_fn=<MseLossBackward>))\n"
     ]
    }
   ],
   "source": [
    "# -*- codeing: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Custom nn Modules\n",
    "Sometimes you will want to specify models that are more complex than a sequence of existing Modules; for these cases you can define your own Modules by subclassing `nn.Module` and defining a forward which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors.\n",
    "\n",
    "In this example we implement our two-layer network as a custom Module subclass:\n",
    "![Status: **torch.nn.Module**](http://placehold.it/350x65/FF0000/FFFF00.png&text=torch.nn.Module) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, tensor(660.4787, grad_fn=<MseLossBackward>))\n",
      "(1, tensor(660.0216, grad_fn=<MseLossBackward>))\n",
      "(2, tensor(659.5650, grad_fn=<MseLossBackward>))\n",
      "(3, tensor(659.1095, grad_fn=<MseLossBackward>))\n",
      "(4, tensor(658.6545, grad_fn=<MseLossBackward>))\n",
      "(5, tensor(658.2000, grad_fn=<MseLossBackward>))\n",
      "(6, tensor(657.7459, grad_fn=<MseLossBackward>))\n",
      "(7, tensor(657.2924, grad_fn=<MseLossBackward>))\n",
      "(8, tensor(656.8394, grad_fn=<MseLossBackward>))\n",
      "(9, tensor(656.3868, grad_fn=<MseLossBackward>))\n",
      "(10, tensor(655.9346, grad_fn=<MseLossBackward>))\n",
      "(11, tensor(655.4830, grad_fn=<MseLossBackward>))\n",
      "(12, tensor(655.0319, grad_fn=<MseLossBackward>))\n",
      "(13, tensor(654.5811, grad_fn=<MseLossBackward>))\n",
      "(14, tensor(654.1308, grad_fn=<MseLossBackward>))\n",
      "(15, tensor(653.6810, grad_fn=<MseLossBackward>))\n",
      "(16, tensor(653.2316, grad_fn=<MseLossBackward>))\n",
      "(17, tensor(652.7827, grad_fn=<MseLossBackward>))\n",
      "(18, tensor(652.3341, grad_fn=<MseLossBackward>))\n",
      "(19, tensor(651.8865, grad_fn=<MseLossBackward>))\n",
      "(20, tensor(651.4396, grad_fn=<MseLossBackward>))\n",
      "(21, tensor(650.9933, grad_fn=<MseLossBackward>))\n",
      "(22, tensor(650.5476, grad_fn=<MseLossBackward>))\n",
      "(23, tensor(650.1024, grad_fn=<MseLossBackward>))\n",
      "(24, tensor(649.6577, grad_fn=<MseLossBackward>))\n",
      "(25, tensor(649.2134, grad_fn=<MseLossBackward>))\n",
      "(26, tensor(648.7695, grad_fn=<MseLossBackward>))\n",
      "(27, tensor(648.3264, grad_fn=<MseLossBackward>))\n",
      "(28, tensor(647.8839, grad_fn=<MseLossBackward>))\n",
      "(29, tensor(647.4418, grad_fn=<MseLossBackward>))\n",
      "(30, tensor(647.0001, grad_fn=<MseLossBackward>))\n",
      "(31, tensor(646.5589, grad_fn=<MseLossBackward>))\n",
      "(32, tensor(646.1183, grad_fn=<MseLossBackward>))\n",
      "(33, tensor(645.6786, grad_fn=<MseLossBackward>))\n",
      "(34, tensor(645.2400, grad_fn=<MseLossBackward>))\n",
      "(35, tensor(644.8021, grad_fn=<MseLossBackward>))\n",
      "(36, tensor(644.3648, grad_fn=<MseLossBackward>))\n",
      "(37, tensor(643.9280, grad_fn=<MseLossBackward>))\n",
      "(38, tensor(643.4919, grad_fn=<MseLossBackward>))\n",
      "(39, tensor(643.0563, grad_fn=<MseLossBackward>))\n",
      "(40, tensor(642.6212, grad_fn=<MseLossBackward>))\n",
      "(41, tensor(642.1866, grad_fn=<MseLossBackward>))\n",
      "(42, tensor(641.7526, grad_fn=<MseLossBackward>))\n",
      "(43, tensor(641.3189, grad_fn=<MseLossBackward>))\n",
      "(44, tensor(640.8857, grad_fn=<MseLossBackward>))\n",
      "(45, tensor(640.4531, grad_fn=<MseLossBackward>))\n",
      "(46, tensor(640.0209, grad_fn=<MseLossBackward>))\n",
      "(47, tensor(639.5893, grad_fn=<MseLossBackward>))\n",
      "(48, tensor(639.1581, grad_fn=<MseLossBackward>))\n",
      "(49, tensor(638.7275, grad_fn=<MseLossBackward>))\n",
      "(50, tensor(638.2972, grad_fn=<MseLossBackward>))\n",
      "(51, tensor(637.8676, grad_fn=<MseLossBackward>))\n",
      "(52, tensor(637.4383, grad_fn=<MseLossBackward>))\n",
      "(53, tensor(637.0096, grad_fn=<MseLossBackward>))\n",
      "(54, tensor(636.5812, grad_fn=<MseLossBackward>))\n",
      "(55, tensor(636.1532, grad_fn=<MseLossBackward>))\n",
      "(56, tensor(635.7261, grad_fn=<MseLossBackward>))\n",
      "(57, tensor(635.2994, grad_fn=<MseLossBackward>))\n",
      "(58, tensor(634.8731, grad_fn=<MseLossBackward>))\n",
      "(59, tensor(634.4471, grad_fn=<MseLossBackward>))\n",
      "(60, tensor(634.0217, grad_fn=<MseLossBackward>))\n",
      "(61, tensor(633.5967, grad_fn=<MseLossBackward>))\n",
      "(62, tensor(633.1722, grad_fn=<MseLossBackward>))\n",
      "(63, tensor(632.7480, grad_fn=<MseLossBackward>))\n",
      "(64, tensor(632.3244, grad_fn=<MseLossBackward>))\n",
      "(65, tensor(631.9017, grad_fn=<MseLossBackward>))\n",
      "(66, tensor(631.4800, grad_fn=<MseLossBackward>))\n",
      "(67, tensor(631.0588, grad_fn=<MseLossBackward>))\n",
      "(68, tensor(630.6379, grad_fn=<MseLossBackward>))\n",
      "(69, tensor(630.2176, grad_fn=<MseLossBackward>))\n",
      "(70, tensor(629.7975, grad_fn=<MseLossBackward>))\n",
      "(71, tensor(629.3779, grad_fn=<MseLossBackward>))\n",
      "(72, tensor(628.9592, grad_fn=<MseLossBackward>))\n",
      "(73, tensor(628.5407, grad_fn=<MseLossBackward>))\n",
      "(74, tensor(628.1227, grad_fn=<MseLossBackward>))\n",
      "(75, tensor(627.7054, grad_fn=<MseLossBackward>))\n",
      "(76, tensor(627.2885, grad_fn=<MseLossBackward>))\n",
      "(77, tensor(626.8717, grad_fn=<MseLossBackward>))\n",
      "(78, tensor(626.4555, grad_fn=<MseLossBackward>))\n",
      "(79, tensor(626.0399, grad_fn=<MseLossBackward>))\n",
      "(80, tensor(625.6254, grad_fn=<MseLossBackward>))\n",
      "(81, tensor(625.2113, grad_fn=<MseLossBackward>))\n",
      "(82, tensor(624.7977, grad_fn=<MseLossBackward>))\n",
      "(83, tensor(624.3848, grad_fn=<MseLossBackward>))\n",
      "(84, tensor(623.9722, grad_fn=<MseLossBackward>))\n",
      "(85, tensor(623.5599, grad_fn=<MseLossBackward>))\n",
      "(86, tensor(623.1481, grad_fn=<MseLossBackward>))\n",
      "(87, tensor(622.7372, grad_fn=<MseLossBackward>))\n",
      "(88, tensor(622.3268, grad_fn=<MseLossBackward>))\n",
      "(89, tensor(621.9167, grad_fn=<MseLossBackward>))\n",
      "(90, tensor(621.5071, grad_fn=<MseLossBackward>))\n",
      "(91, tensor(621.0978, grad_fn=<MseLossBackward>))\n",
      "(92, tensor(620.6893, grad_fn=<MseLossBackward>))\n",
      "(93, tensor(620.2817, grad_fn=<MseLossBackward>))\n",
      "(94, tensor(619.8746, grad_fn=<MseLossBackward>))\n",
      "(95, tensor(619.4677, grad_fn=<MseLossBackward>))\n",
      "(96, tensor(619.0615, grad_fn=<MseLossBackward>))\n",
      "(97, tensor(618.6558, grad_fn=<MseLossBackward>))\n",
      "(98, tensor(618.2507, grad_fn=<MseLossBackward>))\n",
      "(99, tensor(617.8459, grad_fn=<MseLossBackward>))\n",
      "(100, tensor(617.4419, grad_fn=<MseLossBackward>))\n",
      "(101, tensor(617.0381, grad_fn=<MseLossBackward>))\n",
      "(102, tensor(616.6349, grad_fn=<MseLossBackward>))\n",
      "(103, tensor(616.2324, grad_fn=<MseLossBackward>))\n",
      "(104, tensor(615.8302, grad_fn=<MseLossBackward>))\n",
      "(105, tensor(615.4285, grad_fn=<MseLossBackward>))\n",
      "(106, tensor(615.0270, grad_fn=<MseLossBackward>))\n",
      "(107, tensor(614.6260, grad_fn=<MseLossBackward>))\n",
      "(108, tensor(614.2253, grad_fn=<MseLossBackward>))\n",
      "(109, tensor(613.8251, grad_fn=<MseLossBackward>))\n",
      "(110, tensor(613.4252, grad_fn=<MseLossBackward>))\n",
      "(111, tensor(613.0258, grad_fn=<MseLossBackward>))\n",
      "(112, tensor(612.6268, grad_fn=<MseLossBackward>))\n",
      "(113, tensor(612.2281, grad_fn=<MseLossBackward>))\n",
      "(114, tensor(611.8297, grad_fn=<MseLossBackward>))\n",
      "(115, tensor(611.4316, grad_fn=<MseLossBackward>))\n",
      "(116, tensor(611.0340, grad_fn=<MseLossBackward>))\n",
      "(117, tensor(610.6368, grad_fn=<MseLossBackward>))\n",
      "(118, tensor(610.2399, grad_fn=<MseLossBackward>))\n",
      "(119, tensor(609.8434, grad_fn=<MseLossBackward>))\n",
      "(120, tensor(609.4474, grad_fn=<MseLossBackward>))\n",
      "(121, tensor(609.0518, grad_fn=<MseLossBackward>))\n",
      "(122, tensor(608.6568, grad_fn=<MseLossBackward>))\n",
      "(123, tensor(608.2627, grad_fn=<MseLossBackward>))\n",
      "(124, tensor(607.8691, grad_fn=<MseLossBackward>))\n",
      "(125, tensor(607.4758, grad_fn=<MseLossBackward>))\n",
      "(126, tensor(607.0837, grad_fn=<MseLossBackward>))\n",
      "(127, tensor(606.6926, grad_fn=<MseLossBackward>))\n",
      "(128, tensor(606.3025, grad_fn=<MseLossBackward>))\n",
      "(129, tensor(605.9127, grad_fn=<MseLossBackward>))\n",
      "(130, tensor(605.5231, grad_fn=<MseLossBackward>))\n",
      "(131, tensor(605.1340, grad_fn=<MseLossBackward>))\n",
      "(132, tensor(604.7451, grad_fn=<MseLossBackward>))\n",
      "(133, tensor(604.3566, grad_fn=<MseLossBackward>))\n",
      "(134, tensor(603.9684, grad_fn=<MseLossBackward>))\n",
      "(135, tensor(603.5805, grad_fn=<MseLossBackward>))\n",
      "(136, tensor(603.1929, grad_fn=<MseLossBackward>))\n",
      "(137, tensor(602.8057, grad_fn=<MseLossBackward>))\n",
      "(138, tensor(602.4189, grad_fn=<MseLossBackward>))\n",
      "(139, tensor(602.0323, grad_fn=<MseLossBackward>))\n",
      "(140, tensor(601.6462, grad_fn=<MseLossBackward>))\n",
      "(141, tensor(601.2606, grad_fn=<MseLossBackward>))\n",
      "(142, tensor(600.8754, grad_fn=<MseLossBackward>))\n",
      "(143, tensor(600.4904, grad_fn=<MseLossBackward>))\n",
      "(144, tensor(600.1058, grad_fn=<MseLossBackward>))\n",
      "(145, tensor(599.7217, grad_fn=<MseLossBackward>))\n",
      "(146, tensor(599.3380, grad_fn=<MseLossBackward>))\n",
      "(147, tensor(598.9548, grad_fn=<MseLossBackward>))\n",
      "(148, tensor(598.5717, grad_fn=<MseLossBackward>))\n",
      "(149, tensor(598.1895, grad_fn=<MseLossBackward>))\n",
      "(150, tensor(597.8076, grad_fn=<MseLossBackward>))\n",
      "(151, tensor(597.4267, grad_fn=<MseLossBackward>))\n",
      "(152, tensor(597.0470, grad_fn=<MseLossBackward>))\n",
      "(153, tensor(596.6680, grad_fn=<MseLossBackward>))\n",
      "(154, tensor(596.2891, grad_fn=<MseLossBackward>))\n",
      "(155, tensor(595.9105, grad_fn=<MseLossBackward>))\n",
      "(156, tensor(595.5323, grad_fn=<MseLossBackward>))\n",
      "(157, tensor(595.1545, grad_fn=<MseLossBackward>))\n",
      "(158, tensor(594.7772, grad_fn=<MseLossBackward>))\n",
      "(159, tensor(594.4006, grad_fn=<MseLossBackward>))\n",
      "(160, tensor(594.0242, grad_fn=<MseLossBackward>))\n",
      "(161, tensor(593.6482, grad_fn=<MseLossBackward>))\n",
      "(162, tensor(593.2724, grad_fn=<MseLossBackward>))\n",
      "(163, tensor(592.8972, grad_fn=<MseLossBackward>))\n",
      "(164, tensor(592.5221, grad_fn=<MseLossBackward>))\n",
      "(165, tensor(592.1475, grad_fn=<MseLossBackward>))\n",
      "(166, tensor(591.7733, grad_fn=<MseLossBackward>))\n",
      "(167, tensor(591.3995, grad_fn=<MseLossBackward>))\n",
      "(168, tensor(591.0263, grad_fn=<MseLossBackward>))\n",
      "(169, tensor(590.6533, grad_fn=<MseLossBackward>))\n",
      "(170, tensor(590.2807, grad_fn=<MseLossBackward>))\n",
      "(171, tensor(589.9088, grad_fn=<MseLossBackward>))\n",
      "(172, tensor(589.5371, grad_fn=<MseLossBackward>))\n",
      "(173, tensor(589.1658, grad_fn=<MseLossBackward>))\n",
      "(174, tensor(588.7949, grad_fn=<MseLossBackward>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, tensor(588.4253, grad_fn=<MseLossBackward>))\n",
      "(176, tensor(588.0566, grad_fn=<MseLossBackward>))\n",
      "(177, tensor(587.6882, grad_fn=<MseLossBackward>))\n",
      "(178, tensor(587.3203, grad_fn=<MseLossBackward>))\n",
      "(179, tensor(586.9526, grad_fn=<MseLossBackward>))\n",
      "(180, tensor(586.5853, grad_fn=<MseLossBackward>))\n",
      "(181, tensor(586.2185, grad_fn=<MseLossBackward>))\n",
      "(182, tensor(585.8521, grad_fn=<MseLossBackward>))\n",
      "(183, tensor(585.4858, grad_fn=<MseLossBackward>))\n",
      "(184, tensor(585.1200, grad_fn=<MseLossBackward>))\n",
      "(185, tensor(584.7544, grad_fn=<MseLossBackward>))\n",
      "(186, tensor(584.3890, grad_fn=<MseLossBackward>))\n",
      "(187, tensor(584.0242, grad_fn=<MseLossBackward>))\n",
      "(188, tensor(583.6599, grad_fn=<MseLossBackward>))\n",
      "(189, tensor(583.2960, grad_fn=<MseLossBackward>))\n",
      "(190, tensor(582.9324, grad_fn=<MseLossBackward>))\n",
      "(191, tensor(582.5691, grad_fn=<MseLossBackward>))\n",
      "(192, tensor(582.2067, grad_fn=<MseLossBackward>))\n",
      "(193, tensor(581.8450, grad_fn=<MseLossBackward>))\n",
      "(194, tensor(581.4839, grad_fn=<MseLossBackward>))\n",
      "(195, tensor(581.1232, grad_fn=<MseLossBackward>))\n",
      "(196, tensor(580.7628, grad_fn=<MseLossBackward>))\n",
      "(197, tensor(580.4027, grad_fn=<MseLossBackward>))\n",
      "(198, tensor(580.0428, grad_fn=<MseLossBackward>))\n",
      "(199, tensor(579.6833, grad_fn=<MseLossBackward>))\n",
      "(200, tensor(579.3238, grad_fn=<MseLossBackward>))\n",
      "(201, tensor(578.9645, grad_fn=<MseLossBackward>))\n",
      "(202, tensor(578.6057, grad_fn=<MseLossBackward>))\n",
      "(203, tensor(578.2474, grad_fn=<MseLossBackward>))\n",
      "(204, tensor(577.8895, grad_fn=<MseLossBackward>))\n",
      "(205, tensor(577.5320, grad_fn=<MseLossBackward>))\n",
      "(206, tensor(577.1747, grad_fn=<MseLossBackward>))\n",
      "(207, tensor(576.8177, grad_fn=<MseLossBackward>))\n",
      "(208, tensor(576.4610, grad_fn=<MseLossBackward>))\n",
      "(209, tensor(576.1046, grad_fn=<MseLossBackward>))\n",
      "(210, tensor(575.7486, grad_fn=<MseLossBackward>))\n",
      "(211, tensor(575.3928, grad_fn=<MseLossBackward>))\n",
      "(212, tensor(575.0374, grad_fn=<MseLossBackward>))\n",
      "(213, tensor(574.6823, grad_fn=<MseLossBackward>))\n",
      "(214, tensor(574.3274, grad_fn=<MseLossBackward>))\n",
      "(215, tensor(573.9728, grad_fn=<MseLossBackward>))\n",
      "(216, tensor(573.6189, grad_fn=<MseLossBackward>))\n",
      "(217, tensor(573.2654, grad_fn=<MseLossBackward>))\n",
      "(218, tensor(572.9119, grad_fn=<MseLossBackward>))\n",
      "(219, tensor(572.5585, grad_fn=<MseLossBackward>))\n",
      "(220, tensor(572.2066, grad_fn=<MseLossBackward>))\n",
      "(221, tensor(571.8552, grad_fn=<MseLossBackward>))\n",
      "(222, tensor(571.5042, grad_fn=<MseLossBackward>))\n",
      "(223, tensor(571.1536, grad_fn=<MseLossBackward>))\n",
      "(224, tensor(570.8035, grad_fn=<MseLossBackward>))\n",
      "(225, tensor(570.4539, grad_fn=<MseLossBackward>))\n",
      "(226, tensor(570.1048, grad_fn=<MseLossBackward>))\n",
      "(227, tensor(569.7562, grad_fn=<MseLossBackward>))\n",
      "(228, tensor(569.4082, grad_fn=<MseLossBackward>))\n",
      "(229, tensor(569.0604, grad_fn=<MseLossBackward>))\n",
      "(230, tensor(568.7130, grad_fn=<MseLossBackward>))\n",
      "(231, tensor(568.3661, grad_fn=<MseLossBackward>))\n",
      "(232, tensor(568.0195, grad_fn=<MseLossBackward>))\n",
      "(233, tensor(567.6732, grad_fn=<MseLossBackward>))\n",
      "(234, tensor(567.3275, grad_fn=<MseLossBackward>))\n",
      "(235, tensor(566.9819, grad_fn=<MseLossBackward>))\n",
      "(236, tensor(566.6364, grad_fn=<MseLossBackward>))\n",
      "(237, tensor(566.2913, grad_fn=<MseLossBackward>))\n",
      "(238, tensor(565.9466, grad_fn=<MseLossBackward>))\n",
      "(239, tensor(565.6021, grad_fn=<MseLossBackward>))\n",
      "(240, tensor(565.2581, grad_fn=<MseLossBackward>))\n",
      "(241, tensor(564.9144, grad_fn=<MseLossBackward>))\n",
      "(242, tensor(564.5712, grad_fn=<MseLossBackward>))\n",
      "(243, tensor(564.2284, grad_fn=<MseLossBackward>))\n",
      "(244, tensor(563.8859, grad_fn=<MseLossBackward>))\n",
      "(245, tensor(563.5436, grad_fn=<MseLossBackward>))\n",
      "(246, tensor(563.2017, grad_fn=<MseLossBackward>))\n",
      "(247, tensor(562.8599, grad_fn=<MseLossBackward>))\n",
      "(248, tensor(562.5186, grad_fn=<MseLossBackward>))\n",
      "(249, tensor(562.1773, grad_fn=<MseLossBackward>))\n",
      "(250, tensor(561.8364, grad_fn=<MseLossBackward>))\n",
      "(251, tensor(561.4957, grad_fn=<MseLossBackward>))\n",
      "(252, tensor(561.1554, grad_fn=<MseLossBackward>))\n",
      "(253, tensor(560.8154, grad_fn=<MseLossBackward>))\n",
      "(254, tensor(560.4756, grad_fn=<MseLossBackward>))\n",
      "(255, tensor(560.1362, grad_fn=<MseLossBackward>))\n",
      "(256, tensor(559.7970, grad_fn=<MseLossBackward>))\n",
      "(257, tensor(559.4579, grad_fn=<MseLossBackward>))\n",
      "(258, tensor(559.1197, grad_fn=<MseLossBackward>))\n",
      "(259, tensor(558.7819, grad_fn=<MseLossBackward>))\n",
      "(260, tensor(558.4445, grad_fn=<MseLossBackward>))\n",
      "(261, tensor(558.1073, grad_fn=<MseLossBackward>))\n",
      "(262, tensor(557.7704, grad_fn=<MseLossBackward>))\n",
      "(263, tensor(557.4338, grad_fn=<MseLossBackward>))\n",
      "(264, tensor(557.0977, grad_fn=<MseLossBackward>))\n",
      "(265, tensor(556.7616, grad_fn=<MseLossBackward>))\n",
      "(266, tensor(556.4260, grad_fn=<MseLossBackward>))\n",
      "(267, tensor(556.0909, grad_fn=<MseLossBackward>))\n",
      "(268, tensor(555.7560, grad_fn=<MseLossBackward>))\n",
      "(269, tensor(555.4212, grad_fn=<MseLossBackward>))\n",
      "(270, tensor(555.0869, grad_fn=<MseLossBackward>))\n",
      "(271, tensor(554.7529, grad_fn=<MseLossBackward>))\n",
      "(272, tensor(554.4191, grad_fn=<MseLossBackward>))\n",
      "(273, tensor(554.0854, grad_fn=<MseLossBackward>))\n",
      "(274, tensor(553.7522, grad_fn=<MseLossBackward>))\n",
      "(275, tensor(553.4192, grad_fn=<MseLossBackward>))\n",
      "(276, tensor(553.0864, grad_fn=<MseLossBackward>))\n",
      "(277, tensor(552.7538, grad_fn=<MseLossBackward>))\n",
      "(278, tensor(552.4215, grad_fn=<MseLossBackward>))\n",
      "(279, tensor(552.0894, grad_fn=<MseLossBackward>))\n",
      "(280, tensor(551.7575, grad_fn=<MseLossBackward>))\n",
      "(281, tensor(551.4258, grad_fn=<MseLossBackward>))\n",
      "(282, tensor(551.0946, grad_fn=<MseLossBackward>))\n",
      "(283, tensor(550.7634, grad_fn=<MseLossBackward>))\n",
      "(284, tensor(550.4324, grad_fn=<MseLossBackward>))\n",
      "(285, tensor(550.1017, grad_fn=<MseLossBackward>))\n",
      "(286, tensor(549.7713, grad_fn=<MseLossBackward>))\n",
      "(287, tensor(549.4410, grad_fn=<MseLossBackward>))\n",
      "(288, tensor(549.1111, grad_fn=<MseLossBackward>))\n",
      "(289, tensor(548.7818, grad_fn=<MseLossBackward>))\n",
      "(290, tensor(548.4530, grad_fn=<MseLossBackward>))\n",
      "(291, tensor(548.1246, grad_fn=<MseLossBackward>))\n",
      "(292, tensor(547.7964, grad_fn=<MseLossBackward>))\n",
      "(293, tensor(547.4686, grad_fn=<MseLossBackward>))\n",
      "(294, tensor(547.1409, grad_fn=<MseLossBackward>))\n",
      "(295, tensor(546.8135, grad_fn=<MseLossBackward>))\n",
      "(296, tensor(546.4866, grad_fn=<MseLossBackward>))\n",
      "(297, tensor(546.1602, grad_fn=<MseLossBackward>))\n",
      "(298, tensor(545.8343, grad_fn=<MseLossBackward>))\n",
      "(299, tensor(545.5092, grad_fn=<MseLossBackward>))\n",
      "(300, tensor(545.1843, grad_fn=<MseLossBackward>))\n",
      "(301, tensor(544.8596, grad_fn=<MseLossBackward>))\n",
      "(302, tensor(544.5353, grad_fn=<MseLossBackward>))\n",
      "(303, tensor(544.2115, grad_fn=<MseLossBackward>))\n",
      "(304, tensor(543.8881, grad_fn=<MseLossBackward>))\n",
      "(305, tensor(543.5651, grad_fn=<MseLossBackward>))\n",
      "(306, tensor(543.2424, grad_fn=<MseLossBackward>))\n",
      "(307, tensor(542.9203, grad_fn=<MseLossBackward>))\n",
      "(308, tensor(542.5984, grad_fn=<MseLossBackward>))\n",
      "(309, tensor(542.2769, grad_fn=<MseLossBackward>))\n",
      "(310, tensor(541.9556, grad_fn=<MseLossBackward>))\n",
      "(311, tensor(541.6349, grad_fn=<MseLossBackward>))\n",
      "(312, tensor(541.3145, grad_fn=<MseLossBackward>))\n",
      "(313, tensor(540.9944, grad_fn=<MseLossBackward>))\n",
      "(314, tensor(540.6749, grad_fn=<MseLossBackward>))\n",
      "(315, tensor(540.3553, grad_fn=<MseLossBackward>))\n",
      "(316, tensor(540.0363, grad_fn=<MseLossBackward>))\n",
      "(317, tensor(539.7182, grad_fn=<MseLossBackward>))\n",
      "(318, tensor(539.4003, grad_fn=<MseLossBackward>))\n",
      "(319, tensor(539.0825, grad_fn=<MseLossBackward>))\n",
      "(320, tensor(538.7650, grad_fn=<MseLossBackward>))\n",
      "(321, tensor(538.4477, grad_fn=<MseLossBackward>))\n",
      "(322, tensor(538.1307, grad_fn=<MseLossBackward>))\n",
      "(323, tensor(537.8138, grad_fn=<MseLossBackward>))\n",
      "(324, tensor(537.4974, grad_fn=<MseLossBackward>))\n",
      "(325, tensor(537.1820, grad_fn=<MseLossBackward>))\n",
      "(326, tensor(536.8668, grad_fn=<MseLossBackward>))\n",
      "(327, tensor(536.5516, grad_fn=<MseLossBackward>))\n",
      "(328, tensor(536.2369, grad_fn=<MseLossBackward>))\n",
      "(329, tensor(535.9226, grad_fn=<MseLossBackward>))\n",
      "(330, tensor(535.6084, grad_fn=<MseLossBackward>))\n",
      "(331, tensor(535.2945, grad_fn=<MseLossBackward>))\n",
      "(332, tensor(534.9807, grad_fn=<MseLossBackward>))\n",
      "(333, tensor(534.6675, grad_fn=<MseLossBackward>))\n",
      "(334, tensor(534.3553, grad_fn=<MseLossBackward>))\n",
      "(335, tensor(534.0435, grad_fn=<MseLossBackward>))\n",
      "(336, tensor(533.7317, grad_fn=<MseLossBackward>))\n",
      "(337, tensor(533.4202, grad_fn=<MseLossBackward>))\n",
      "(338, tensor(533.1089, grad_fn=<MseLossBackward>))\n",
      "(339, tensor(532.7980, grad_fn=<MseLossBackward>))\n",
      "(340, tensor(532.4875, grad_fn=<MseLossBackward>))\n",
      "(341, tensor(532.1774, grad_fn=<MseLossBackward>))\n",
      "(342, tensor(531.8674, grad_fn=<MseLossBackward>))\n",
      "(343, tensor(531.5576, grad_fn=<MseLossBackward>))\n",
      "(344, tensor(531.2481, grad_fn=<MseLossBackward>))\n",
      "(345, tensor(530.9387, grad_fn=<MseLossBackward>))\n",
      "(346, tensor(530.6295, grad_fn=<MseLossBackward>))\n",
      "(347, tensor(530.3206, grad_fn=<MseLossBackward>))\n",
      "(348, tensor(530.0119, grad_fn=<MseLossBackward>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349, tensor(529.7036, grad_fn=<MseLossBackward>))\n",
      "(350, tensor(529.3954, grad_fn=<MseLossBackward>))\n",
      "(351, tensor(529.0874, grad_fn=<MseLossBackward>))\n",
      "(352, tensor(528.7797, grad_fn=<MseLossBackward>))\n",
      "(353, tensor(528.4721, grad_fn=<MseLossBackward>))\n",
      "(354, tensor(528.1648, grad_fn=<MseLossBackward>))\n",
      "(355, tensor(527.8576, grad_fn=<MseLossBackward>))\n",
      "(356, tensor(527.5508, grad_fn=<MseLossBackward>))\n",
      "(357, tensor(527.2449, grad_fn=<MseLossBackward>))\n",
      "(358, tensor(526.9388, grad_fn=<MseLossBackward>))\n",
      "(359, tensor(526.6329, grad_fn=<MseLossBackward>))\n",
      "(360, tensor(526.3270, grad_fn=<MseLossBackward>))\n",
      "(361, tensor(526.0214, grad_fn=<MseLossBackward>))\n",
      "(362, tensor(525.7159, grad_fn=<MseLossBackward>))\n",
      "(363, tensor(525.4108, grad_fn=<MseLossBackward>))\n",
      "(364, tensor(525.1061, grad_fn=<MseLossBackward>))\n",
      "(365, tensor(524.8015, grad_fn=<MseLossBackward>))\n",
      "(366, tensor(524.4972, grad_fn=<MseLossBackward>))\n",
      "(367, tensor(524.1931, grad_fn=<MseLossBackward>))\n",
      "(368, tensor(523.8892, grad_fn=<MseLossBackward>))\n",
      "(369, tensor(523.5856, grad_fn=<MseLossBackward>))\n",
      "(370, tensor(523.2822, grad_fn=<MseLossBackward>))\n",
      "(371, tensor(522.9789, grad_fn=<MseLossBackward>))\n",
      "(372, tensor(522.6759, grad_fn=<MseLossBackward>))\n",
      "(373, tensor(522.3734, grad_fn=<MseLossBackward>))\n",
      "(374, tensor(522.0708, grad_fn=<MseLossBackward>))\n",
      "(375, tensor(521.7685, grad_fn=<MseLossBackward>))\n",
      "(376, tensor(521.4664, grad_fn=<MseLossBackward>))\n",
      "(377, tensor(521.1646, grad_fn=<MseLossBackward>))\n",
      "(378, tensor(520.8632, grad_fn=<MseLossBackward>))\n",
      "(379, tensor(520.5627, grad_fn=<MseLossBackward>))\n",
      "(380, tensor(520.2626, grad_fn=<MseLossBackward>))\n",
      "(381, tensor(519.9625, grad_fn=<MseLossBackward>))\n",
      "(382, tensor(519.6629, grad_fn=<MseLossBackward>))\n",
      "(383, tensor(519.3636, grad_fn=<MseLossBackward>))\n",
      "(384, tensor(519.0645, grad_fn=<MseLossBackward>))\n",
      "(385, tensor(518.7661, grad_fn=<MseLossBackward>))\n",
      "(386, tensor(518.4678, grad_fn=<MseLossBackward>))\n",
      "(387, tensor(518.1700, grad_fn=<MseLossBackward>))\n",
      "(388, tensor(517.8723, grad_fn=<MseLossBackward>))\n",
      "(389, tensor(517.5748, grad_fn=<MseLossBackward>))\n",
      "(390, tensor(517.2774, grad_fn=<MseLossBackward>))\n",
      "(391, tensor(516.9803, grad_fn=<MseLossBackward>))\n",
      "(392, tensor(516.6832, grad_fn=<MseLossBackward>))\n",
      "(393, tensor(516.3864, grad_fn=<MseLossBackward>))\n",
      "(394, tensor(516.0898, grad_fn=<MseLossBackward>))\n",
      "(395, tensor(515.7935, grad_fn=<MseLossBackward>))\n",
      "(396, tensor(515.4973, grad_fn=<MseLossBackward>))\n",
      "(397, tensor(515.2014, grad_fn=<MseLossBackward>))\n",
      "(398, tensor(514.9055, grad_fn=<MseLossBackward>))\n",
      "(399, tensor(514.6098, grad_fn=<MseLossBackward>))\n",
      "(400, tensor(514.3145, grad_fn=<MseLossBackward>))\n",
      "(401, tensor(514.0200, grad_fn=<MseLossBackward>))\n",
      "(402, tensor(513.7258, grad_fn=<MseLossBackward>))\n",
      "(403, tensor(513.4317, grad_fn=<MseLossBackward>))\n",
      "(404, tensor(513.1378, grad_fn=<MseLossBackward>))\n",
      "(405, tensor(512.8443, grad_fn=<MseLossBackward>))\n",
      "(406, tensor(512.5513, grad_fn=<MseLossBackward>))\n",
      "(407, tensor(512.2584, grad_fn=<MseLossBackward>))\n",
      "(408, tensor(511.9656, grad_fn=<MseLossBackward>))\n",
      "(409, tensor(511.6730, grad_fn=<MseLossBackward>))\n",
      "(410, tensor(511.3808, grad_fn=<MseLossBackward>))\n",
      "(411, tensor(511.0887, grad_fn=<MseLossBackward>))\n",
      "(412, tensor(510.7969, grad_fn=<MseLossBackward>))\n",
      "(413, tensor(510.5052, grad_fn=<MseLossBackward>))\n",
      "(414, tensor(510.2138, grad_fn=<MseLossBackward>))\n",
      "(415, tensor(509.9227, grad_fn=<MseLossBackward>))\n",
      "(416, tensor(509.6319, grad_fn=<MseLossBackward>))\n",
      "(417, tensor(509.3415, grad_fn=<MseLossBackward>))\n",
      "(418, tensor(509.0518, grad_fn=<MseLossBackward>))\n",
      "(419, tensor(508.7621, grad_fn=<MseLossBackward>))\n",
      "(420, tensor(508.4725, grad_fn=<MseLossBackward>))\n",
      "(421, tensor(508.1831, grad_fn=<MseLossBackward>))\n",
      "(422, tensor(507.8939, grad_fn=<MseLossBackward>))\n",
      "(423, tensor(507.6050, grad_fn=<MseLossBackward>))\n",
      "(424, tensor(507.3168, grad_fn=<MseLossBackward>))\n",
      "(425, tensor(507.0289, grad_fn=<MseLossBackward>))\n",
      "(426, tensor(506.7408, grad_fn=<MseLossBackward>))\n",
      "(427, tensor(506.4530, grad_fn=<MseLossBackward>))\n",
      "(428, tensor(506.1658, grad_fn=<MseLossBackward>))\n",
      "(429, tensor(505.8789, grad_fn=<MseLossBackward>))\n",
      "(430, tensor(505.5923, grad_fn=<MseLossBackward>))\n",
      "(431, tensor(505.3059, grad_fn=<MseLossBackward>))\n",
      "(432, tensor(505.0196, grad_fn=<MseLossBackward>))\n",
      "(433, tensor(504.7336, grad_fn=<MseLossBackward>))\n",
      "(434, tensor(504.4478, grad_fn=<MseLossBackward>))\n",
      "(435, tensor(504.1620, grad_fn=<MseLossBackward>))\n",
      "(436, tensor(503.8766, grad_fn=<MseLossBackward>))\n",
      "(437, tensor(503.5913, grad_fn=<MseLossBackward>))\n",
      "(438, tensor(503.3061, grad_fn=<MseLossBackward>))\n",
      "(439, tensor(503.0211, grad_fn=<MseLossBackward>))\n",
      "(440, tensor(502.7362, grad_fn=<MseLossBackward>))\n",
      "(441, tensor(502.4515, grad_fn=<MseLossBackward>))\n",
      "(442, tensor(502.1675, grad_fn=<MseLossBackward>))\n",
      "(443, tensor(501.8837, grad_fn=<MseLossBackward>))\n",
      "(444, tensor(501.6001, grad_fn=<MseLossBackward>))\n",
      "(445, tensor(501.3167, grad_fn=<MseLossBackward>))\n",
      "(446, tensor(501.0333, grad_fn=<MseLossBackward>))\n",
      "(447, tensor(500.7502, grad_fn=<MseLossBackward>))\n",
      "(448, tensor(500.4676, grad_fn=<MseLossBackward>))\n",
      "(449, tensor(500.1852, grad_fn=<MseLossBackward>))\n",
      "(450, tensor(499.9030, grad_fn=<MseLossBackward>))\n",
      "(451, tensor(499.6210, grad_fn=<MseLossBackward>))\n",
      "(452, tensor(499.3395, grad_fn=<MseLossBackward>))\n",
      "(453, tensor(499.0581, grad_fn=<MseLossBackward>))\n",
      "(454, tensor(498.7767, grad_fn=<MseLossBackward>))\n",
      "(455, tensor(498.4956, grad_fn=<MseLossBackward>))\n",
      "(456, tensor(498.2148, grad_fn=<MseLossBackward>))\n",
      "(457, tensor(497.9342, grad_fn=<MseLossBackward>))\n",
      "(458, tensor(497.6541, grad_fn=<MseLossBackward>))\n",
      "(459, tensor(497.3745, grad_fn=<MseLossBackward>))\n",
      "(460, tensor(497.0952, grad_fn=<MseLossBackward>))\n",
      "(461, tensor(496.8160, grad_fn=<MseLossBackward>))\n",
      "(462, tensor(496.5369, grad_fn=<MseLossBackward>))\n",
      "(463, tensor(496.2580, grad_fn=<MseLossBackward>))\n",
      "(464, tensor(495.9794, grad_fn=<MseLossBackward>))\n",
      "(465, tensor(495.7010, grad_fn=<MseLossBackward>))\n",
      "(466, tensor(495.4226, grad_fn=<MseLossBackward>))\n",
      "(467, tensor(495.1444, grad_fn=<MseLossBackward>))\n",
      "(468, tensor(494.8662, grad_fn=<MseLossBackward>))\n",
      "(469, tensor(494.5882, grad_fn=<MseLossBackward>))\n",
      "(470, tensor(494.3102, grad_fn=<MseLossBackward>))\n",
      "(471, tensor(494.0327, grad_fn=<MseLossBackward>))\n",
      "(472, tensor(493.7552, grad_fn=<MseLossBackward>))\n",
      "(473, tensor(493.4778, grad_fn=<MseLossBackward>))\n",
      "(474, tensor(493.2005, grad_fn=<MseLossBackward>))\n",
      "(475, tensor(492.9235, grad_fn=<MseLossBackward>))\n",
      "(476, tensor(492.6467, grad_fn=<MseLossBackward>))\n",
      "(477, tensor(492.3700, grad_fn=<MseLossBackward>))\n",
      "(478, tensor(492.0935, grad_fn=<MseLossBackward>))\n",
      "(479, tensor(491.8172, grad_fn=<MseLossBackward>))\n",
      "(480, tensor(491.5410, grad_fn=<MseLossBackward>))\n",
      "(481, tensor(491.2650, grad_fn=<MseLossBackward>))\n",
      "(482, tensor(490.9890, grad_fn=<MseLossBackward>))\n",
      "(483, tensor(490.7133, grad_fn=<MseLossBackward>))\n",
      "(484, tensor(490.4379, grad_fn=<MseLossBackward>))\n",
      "(485, tensor(490.1626, grad_fn=<MseLossBackward>))\n",
      "(486, tensor(489.8873, grad_fn=<MseLossBackward>))\n",
      "(487, tensor(489.6124, grad_fn=<MseLossBackward>))\n",
      "(488, tensor(489.3376, grad_fn=<MseLossBackward>))\n",
      "(489, tensor(489.0628, grad_fn=<MseLossBackward>))\n",
      "(490, tensor(488.7887, grad_fn=<MseLossBackward>))\n",
      "(491, tensor(488.5150, grad_fn=<MseLossBackward>))\n",
      "(492, tensor(488.2413, grad_fn=<MseLossBackward>))\n",
      "(493, tensor(487.9677, grad_fn=<MseLossBackward>))\n",
      "(494, tensor(487.6944, grad_fn=<MseLossBackward>))\n",
      "(495, tensor(487.4216, grad_fn=<MseLossBackward>))\n",
      "(496, tensor(487.1489, grad_fn=<MseLossBackward>))\n",
      "(497, tensor(486.8766, grad_fn=<MseLossBackward>))\n",
      "(498, tensor(486.6043, grad_fn=<MseLossBackward>))\n",
      "(499, tensor(486.3323, grad_fn=<MseLossBackward>))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-6)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Control Flow + Weight Sharing\n",
    "As an example of dynamic graphs and weight sharing, we implement a very strange model: a fully-connected ReLU network that on each forward pass chooses a random number between 1 and 4 and uses that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers.\n",
    "\n",
    "For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing among the innermost layers by simply reusing the same Module multiple times when defining the forward pass.\n",
    "\n",
    "We can easily implement this model as a Module subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, tensor(610.5097, grad_fn=<MseLossBackward>))\n",
      "(1, tensor(572.5293, grad_fn=<MseLossBackward>))\n",
      "(2, tensor(583.2917, grad_fn=<MseLossBackward>))\n",
      "(3, tensor(569.9171, grad_fn=<MseLossBackward>))\n",
      "(4, tensor(570.7463, grad_fn=<MseLossBackward>))\n",
      "(5, tensor(570.7705, grad_fn=<MseLossBackward>))\n",
      "(6, tensor(564.7770, grad_fn=<MseLossBackward>))\n",
      "(7, tensor(562.7483, grad_fn=<MseLossBackward>))\n",
      "(8, tensor(568.9363, grad_fn=<MseLossBackward>))\n",
      "(9, tensor(412.5173, grad_fn=<MseLossBackward>))\n",
      "(10, tensor(567.7312, grad_fn=<MseLossBackward>))\n",
      "(11, tensor(567.0626, grad_fn=<MseLossBackward>))\n",
      "(12, tensor(566.2966, grad_fn=<MseLossBackward>))\n",
      "(13, tensor(551.7401, grad_fn=<MseLossBackward>))\n",
      "(14, tensor(549.4773, grad_fn=<MseLossBackward>))\n",
      "(15, tensor(563.6799, grad_fn=<MseLossBackward>))\n",
      "(16, tensor(562.5636, grad_fn=<MseLossBackward>))\n",
      "(17, tensor(561.2402, grad_fn=<MseLossBackward>))\n",
      "(18, tensor(277.0592, grad_fn=<MseLossBackward>))\n",
      "(19, tensor(256.8362, grad_fn=<MseLossBackward>))\n",
      "(20, tensor(556.7158, grad_fn=<MseLossBackward>))\n",
      "(21, tensor(554.8585, grad_fn=<MseLossBackward>))\n",
      "(22, tensor(181.6461, grad_fn=<MseLossBackward>))\n",
      "(23, tensor(550.2783, grad_fn=<MseLossBackward>))\n",
      "(24, tensor(519.6934, grad_fn=<MseLossBackward>))\n",
      "(25, tensor(513.7929, grad_fn=<MseLossBackward>))\n",
      "(26, tensor(493.0638, grad_fn=<MseLossBackward>))\n",
      "(27, tensor(534.7250, grad_fn=<MseLossBackward>))\n",
      "(28, tensor(91.1896, grad_fn=<MseLossBackward>))\n",
      "(29, tensor(475.8786, grad_fn=<MseLossBackward>))\n",
      "(30, tensor(512.9148, grad_fn=<MseLossBackward>))\n",
      "(31, tensor(447.4369, grad_fn=<MseLossBackward>))\n",
      "(32, tensor(407.9834, grad_fn=<MseLossBackward>))\n",
      "(33, tensor(471.6087, grad_fn=<MseLossBackward>))\n",
      "(34, tensor(451.0277, grad_fn=<MseLossBackward>))\n",
      "(35, tensor(336.0818, grad_fn=<MseLossBackward>))\n",
      "(36, tensor(98.6566, grad_fn=<MseLossBackward>))\n",
      "(37, tensor(285.5066, grad_fn=<MseLossBackward>))\n",
      "(38, tensor(322.8109, grad_fn=<MseLossBackward>))\n",
      "(39, tensor(97.8599, grad_fn=<MseLossBackward>))\n",
      "(40, tensor(219.5222, grad_fn=<MseLossBackward>))\n",
      "(41, tensor(337.5646, grad_fn=<MseLossBackward>))\n",
      "(42, tensor(262.9110, grad_fn=<MseLossBackward>))\n",
      "(43, tensor(243.6913, grad_fn=<MseLossBackward>))\n",
      "(44, tensor(283.9049, grad_fn=<MseLossBackward>))\n",
      "(45, tensor(260.3055, grad_fn=<MseLossBackward>))\n",
      "(46, tensor(89.7402, grad_fn=<MseLossBackward>))\n",
      "(47, tensor(219.8510, grad_fn=<MseLossBackward>))\n",
      "(48, tensor(78.1524, grad_fn=<MseLossBackward>))\n",
      "(49, tensor(173.1394, grad_fn=<MseLossBackward>))\n",
      "(50, tensor(153.0776, grad_fn=<MseLossBackward>))\n",
      "(51, tensor(141.0658, grad_fn=<MseLossBackward>))\n",
      "(52, tensor(48.0088, grad_fn=<MseLossBackward>))\n",
      "(53, tensor(117.4905, grad_fn=<MseLossBackward>))\n",
      "(54, tensor(94.7078, grad_fn=<MseLossBackward>))\n",
      "(55, tensor(42.7221, grad_fn=<MseLossBackward>))\n",
      "(56, tensor(268.4906, grad_fn=<MseLossBackward>))\n",
      "(57, tensor(290.6705, grad_fn=<MseLossBackward>))\n",
      "(58, tensor(461.3640, grad_fn=<MseLossBackward>))\n",
      "(59, tensor(58.9190, grad_fn=<MseLossBackward>))\n",
      "(60, tensor(105.2958, grad_fn=<MseLossBackward>))\n",
      "(61, tensor(515.7073, grad_fn=<MseLossBackward>))\n",
      "(62, tensor(304.1127, grad_fn=<MseLossBackward>))\n",
      "(63, tensor(197.2434, grad_fn=<MseLossBackward>))\n",
      "(64, tensor(357.2399, grad_fn=<MseLossBackward>))\n",
      "(65, tensor(124.6138, grad_fn=<MseLossBackward>))\n",
      "(66, tensor(234.5418, grad_fn=<MseLossBackward>))\n",
      "(67, tensor(325.4742, grad_fn=<MseLossBackward>))\n",
      "(68, tensor(155.1624, grad_fn=<MseLossBackward>))\n",
      "(69, tensor(390.2730, grad_fn=<MseLossBackward>))\n",
      "(70, tensor(375.7283, grad_fn=<MseLossBackward>))\n",
      "(71, tensor(189.9558, grad_fn=<MseLossBackward>))\n",
      "(72, tensor(249.3341, grad_fn=<MseLossBackward>))\n",
      "(73, tensor(224.7155, grad_fn=<MseLossBackward>))\n",
      "(74, tensor(202.7965, grad_fn=<MseLossBackward>))\n",
      "(75, tensor(187.1019, grad_fn=<MseLossBackward>))\n",
      "(76, tensor(233.4442, grad_fn=<MseLossBackward>))\n",
      "(77, tensor(226.3779, grad_fn=<MseLossBackward>))\n",
      "(78, tensor(174.1743, grad_fn=<MseLossBackward>))\n",
      "(79, tensor(163.0467, grad_fn=<MseLossBackward>))\n",
      "(80, tensor(113.4854, grad_fn=<MseLossBackward>))\n",
      "(81, tensor(134.8156, grad_fn=<MseLossBackward>))\n",
      "(82, tensor(119.6725, grad_fn=<MseLossBackward>))\n",
      "(83, tensor(119.8756, grad_fn=<MseLossBackward>))\n",
      "(84, tensor(162.5975, grad_fn=<MseLossBackward>))\n",
      "(85, tensor(80.7545, grad_fn=<MseLossBackward>))\n",
      "(86, tensor(71.6026, grad_fn=<MseLossBackward>))\n",
      "(87, tensor(147.1535, grad_fn=<MseLossBackward>))\n",
      "(88, tensor(86.1413, grad_fn=<MseLossBackward>))\n",
      "(89, tensor(77.1197, grad_fn=<MseLossBackward>))\n",
      "(90, tensor(64.5127, grad_fn=<MseLossBackward>))\n",
      "(91, tensor(117.5562, grad_fn=<MseLossBackward>))\n",
      "(92, tensor(70.2523, grad_fn=<MseLossBackward>))\n",
      "(93, tensor(103.6945, grad_fn=<MseLossBackward>))\n",
      "(94, tensor(53.2947, grad_fn=<MseLossBackward>))\n",
      "(95, tensor(51.5340, grad_fn=<MseLossBackward>))\n",
      "(96, tensor(36.5156, grad_fn=<MseLossBackward>))\n",
      "(97, tensor(76.3658, grad_fn=<MseLossBackward>))\n",
      "(98, tensor(36.5636, grad_fn=<MseLossBackward>))\n",
      "(99, tensor(33.0325, grad_fn=<MseLossBackward>))\n",
      "(100, tensor(45.2161, grad_fn=<MseLossBackward>))\n",
      "(101, tensor(61.6345, grad_fn=<MseLossBackward>))\n",
      "(102, tensor(17.5721, grad_fn=<MseLossBackward>))\n",
      "(103, tensor(15.3294, grad_fn=<MseLossBackward>))\n",
      "(104, tensor(13.5725, grad_fn=<MseLossBackward>))\n",
      "(105, tensor(11.5645, grad_fn=<MseLossBackward>))\n",
      "(106, tensor(87.6401, grad_fn=<MseLossBackward>))\n",
      "(107, tensor(33.5203, grad_fn=<MseLossBackward>))\n",
      "(108, tensor(50.9302, grad_fn=<MseLossBackward>))\n",
      "(109, tensor(37.1711, grad_fn=<MseLossBackward>))\n",
      "(110, tensor(39.3271, grad_fn=<MseLossBackward>))\n",
      "(111, tensor(30.7430, grad_fn=<MseLossBackward>))\n",
      "(112, tensor(44.6236, grad_fn=<MseLossBackward>))\n",
      "(113, tensor(21.4785, grad_fn=<MseLossBackward>))\n",
      "(114, tensor(25.5799, grad_fn=<MseLossBackward>))\n",
      "(115, tensor(26.6912, grad_fn=<MseLossBackward>))\n",
      "(116, tensor(16.5513, grad_fn=<MseLossBackward>))\n",
      "(117, tensor(26.7599, grad_fn=<MseLossBackward>))\n",
      "(118, tensor(18.9054, grad_fn=<MseLossBackward>))\n",
      "(119, tensor(12.4669, grad_fn=<MseLossBackward>))\n",
      "(120, tensor(16.1333, grad_fn=<MseLossBackward>))\n",
      "(121, tensor(25.6562, grad_fn=<MseLossBackward>))\n",
      "(122, tensor(37.2553, grad_fn=<MseLossBackward>))\n",
      "(123, tensor(28.2329, grad_fn=<MseLossBackward>))\n",
      "(124, tensor(13.3249, grad_fn=<MseLossBackward>))\n",
      "(125, tensor(14.5472, grad_fn=<MseLossBackward>))\n",
      "(126, tensor(15.8795, grad_fn=<MseLossBackward>))\n",
      "(127, tensor(55.7833, grad_fn=<MseLossBackward>))\n",
      "(128, tensor(24.7122, grad_fn=<MseLossBackward>))\n",
      "(129, tensor(15.2650, grad_fn=<MseLossBackward>))\n",
      "(130, tensor(47.4351, grad_fn=<MseLossBackward>))\n",
      "(131, tensor(24.2864, grad_fn=<MseLossBackward>))\n",
      "(132, tensor(17.1051, grad_fn=<MseLossBackward>))\n",
      "(133, tensor(28.9953, grad_fn=<MseLossBackward>))\n",
      "(134, tensor(10.6970, grad_fn=<MseLossBackward>))\n",
      "(135, tensor(17.5185, grad_fn=<MseLossBackward>))\n",
      "(136, tensor(35.3226, grad_fn=<MseLossBackward>))\n",
      "(137, tensor(23.7526, grad_fn=<MseLossBackward>))\n",
      "(138, tensor(10.1877, grad_fn=<MseLossBackward>))\n",
      "(139, tensor(11.4685, grad_fn=<MseLossBackward>))\n",
      "(140, tensor(13.4767, grad_fn=<MseLossBackward>))\n",
      "(141, tensor(5.8640, grad_fn=<MseLossBackward>))\n",
      "(142, tensor(14.4377, grad_fn=<MseLossBackward>))\n",
      "(143, tensor(36.7359, grad_fn=<MseLossBackward>))\n",
      "(144, tensor(9.0408, grad_fn=<MseLossBackward>))\n",
      "(145, tensor(12.8237, grad_fn=<MseLossBackward>))\n",
      "(146, tensor(17.1262, grad_fn=<MseLossBackward>))\n",
      "(147, tensor(9.5657, grad_fn=<MseLossBackward>))\n",
      "(148, tensor(26.3146, grad_fn=<MseLossBackward>))\n",
      "(149, tensor(9.1278, grad_fn=<MseLossBackward>))\n",
      "(150, tensor(14.7088, grad_fn=<MseLossBackward>))\n",
      "(151, tensor(5.2837, grad_fn=<MseLossBackward>))\n",
      "(152, tensor(7.8844, grad_fn=<MseLossBackward>))\n",
      "(153, tensor(7.4595, grad_fn=<MseLossBackward>))\n",
      "(154, tensor(7.9142, grad_fn=<MseLossBackward>))\n",
      "(155, tensor(6.8668, grad_fn=<MseLossBackward>))\n",
      "(156, tensor(5.0255, grad_fn=<MseLossBackward>))\n",
      "(157, tensor(3.4523, grad_fn=<MseLossBackward>))\n",
      "(158, tensor(3.7197, grad_fn=<MseLossBackward>))\n",
      "(159, tensor(3.3396, grad_fn=<MseLossBackward>))\n",
      "(160, tensor(10.0859, grad_fn=<MseLossBackward>))\n",
      "(161, tensor(16.0971, grad_fn=<MseLossBackward>))\n",
      "(162, tensor(10.4759, grad_fn=<MseLossBackward>))\n",
      "(163, tensor(7.2854, grad_fn=<MseLossBackward>))\n",
      "(164, tensor(18.2591, grad_fn=<MseLossBackward>))\n",
      "(165, tensor(30.4099, grad_fn=<MseLossBackward>))\n",
      "(166, tensor(4.3642, grad_fn=<MseLossBackward>))\n",
      "(167, tensor(10.3331, grad_fn=<MseLossBackward>))\n",
      "(168, tensor(6.6051, grad_fn=<MseLossBackward>))\n",
      "(169, tensor(57.6029, grad_fn=<MseLossBackward>))\n",
      "(170, tensor(4.4218, grad_fn=<MseLossBackward>))\n",
      "(171, tensor(13.8185, grad_fn=<MseLossBackward>))\n",
      "(172, tensor(11.0752, grad_fn=<MseLossBackward>))\n",
      "(173, tensor(2.3526, grad_fn=<MseLossBackward>))\n",
      "(174, tensor(42.4421, grad_fn=<MseLossBackward>))\n",
      "(175, tensor(7.2566, grad_fn=<MseLossBackward>))\n",
      "(176, tensor(12.2521, grad_fn=<MseLossBackward>))\n",
      "(177, tensor(19.5522, grad_fn=<MseLossBackward>))\n",
      "(178, tensor(17.6776, grad_fn=<MseLossBackward>))\n",
      "(179, tensor(30.8720, grad_fn=<MseLossBackward>))\n",
      "(180, tensor(28.2311, grad_fn=<MseLossBackward>))\n",
      "(181, tensor(47.4329, grad_fn=<MseLossBackward>))\n",
      "(182, tensor(17.3269, grad_fn=<MseLossBackward>))\n",
      "(183, tensor(8.1769, grad_fn=<MseLossBackward>))\n",
      "(184, tensor(12.6731, grad_fn=<MseLossBackward>))\n",
      "(185, tensor(22.1436, grad_fn=<MseLossBackward>))\n",
      "(186, tensor(24.8206, grad_fn=<MseLossBackward>))\n",
      "(187, tensor(100.3268, grad_fn=<MseLossBackward>))\n",
      "(188, tensor(17.1951, grad_fn=<MseLossBackward>))\n",
      "(189, tensor(16.9588, grad_fn=<MseLossBackward>))\n",
      "(190, tensor(28.6716, grad_fn=<MseLossBackward>))\n",
      "(191, tensor(40.5483, grad_fn=<MseLossBackward>))\n",
      "(192, tensor(8.5803, grad_fn=<MseLossBackward>))\n",
      "(193, tensor(49.0690, grad_fn=<MseLossBackward>))\n",
      "(194, tensor(4.2954, grad_fn=<MseLossBackward>))\n",
      "(195, tensor(23.4428, grad_fn=<MseLossBackward>))\n",
      "(196, tensor(4.3727, grad_fn=<MseLossBackward>))\n",
      "(197, tensor(32.1527, grad_fn=<MseLossBackward>))\n",
      "(198, tensor(16.9158, grad_fn=<MseLossBackward>))\n",
      "(199, tensor(24.5307, grad_fn=<MseLossBackward>))\n",
      "(200, tensor(11.9117, grad_fn=<MseLossBackward>))\n",
      "(201, tensor(6.8898, grad_fn=<MseLossBackward>))\n",
      "(202, tensor(8.0174, grad_fn=<MseLossBackward>))\n",
      "(203, tensor(11.0655, grad_fn=<MseLossBackward>))\n",
      "(204, tensor(5.1582, grad_fn=<MseLossBackward>))\n",
      "(205, tensor(55.6957, grad_fn=<MseLossBackward>))\n",
      "(206, tensor(8.0698, grad_fn=<MseLossBackward>))\n",
      "(207, tensor(7.0645, grad_fn=<MseLossBackward>))\n",
      "(208, tensor(14.9559, grad_fn=<MseLossBackward>))\n",
      "(209, tensor(21.9952, grad_fn=<MseLossBackward>))\n",
      "(210, tensor(10.3312, grad_fn=<MseLossBackward>))\n",
      "(211, tensor(7.4973, grad_fn=<MseLossBackward>))\n",
      "(212, tensor(11.3300, grad_fn=<MseLossBackward>))\n",
      "(213, tensor(5.4924, grad_fn=<MseLossBackward>))\n",
      "(214, tensor(12.9281, grad_fn=<MseLossBackward>))\n",
      "(215, tensor(5.9859, grad_fn=<MseLossBackward>))\n",
      "(216, tensor(7.6203, grad_fn=<MseLossBackward>))\n",
      "(217, tensor(14.8922, grad_fn=<MseLossBackward>))\n",
      "(218, tensor(6.8540, grad_fn=<MseLossBackward>))\n",
      "(219, tensor(4.1694, grad_fn=<MseLossBackward>))\n",
      "(220, tensor(3.2454, grad_fn=<MseLossBackward>))\n",
      "(221, tensor(5.1803, grad_fn=<MseLossBackward>))\n",
      "(222, tensor(4.6009, grad_fn=<MseLossBackward>))\n",
      "(223, tensor(12.7717, grad_fn=<MseLossBackward>))\n",
      "(224, tensor(21.7546, grad_fn=<MseLossBackward>))\n",
      "(225, tensor(3.6480, grad_fn=<MseLossBackward>))\n",
      "(226, tensor(5.3174, grad_fn=<MseLossBackward>))\n",
      "(227, tensor(10.1775, grad_fn=<MseLossBackward>))\n",
      "(228, tensor(9.5948, grad_fn=<MseLossBackward>))\n",
      "(229, tensor(5.5621, grad_fn=<MseLossBackward>))\n",
      "(230, tensor(5.7812, grad_fn=<MseLossBackward>))\n",
      "(231, tensor(6.1211, grad_fn=<MseLossBackward>))\n",
      "(232, tensor(2.0139, grad_fn=<MseLossBackward>))\n",
      "(233, tensor(6.7488, grad_fn=<MseLossBackward>))\n",
      "(234, tensor(6.8915, grad_fn=<MseLossBackward>))\n",
      "(235, tensor(4.7268, grad_fn=<MseLossBackward>))\n",
      "(236, tensor(3.1039, grad_fn=<MseLossBackward>))\n",
      "(237, tensor(1.8186, grad_fn=<MseLossBackward>))\n",
      "(238, tensor(8.5636, grad_fn=<MseLossBackward>))\n",
      "(239, tensor(1.5530, grad_fn=<MseLossBackward>))\n",
      "(240, tensor(3.9840, grad_fn=<MseLossBackward>))\n",
      "(241, tensor(3.8741, grad_fn=<MseLossBackward>))\n",
      "(242, tensor(4.5543, grad_fn=<MseLossBackward>))\n",
      "(243, tensor(2.3517, grad_fn=<MseLossBackward>))\n",
      "(244, tensor(2.7199, grad_fn=<MseLossBackward>))\n",
      "(245, tensor(3.0254, grad_fn=<MseLossBackward>))\n",
      "(246, tensor(7.3427, grad_fn=<MseLossBackward>))\n",
      "(247, tensor(1.8178, grad_fn=<MseLossBackward>))\n",
      "(248, tensor(1.0942, grad_fn=<MseLossBackward>))\n",
      "(249, tensor(2.8948, grad_fn=<MseLossBackward>))\n",
      "(250, tensor(2.2140, grad_fn=<MseLossBackward>))\n",
      "(251, tensor(1.8004, grad_fn=<MseLossBackward>))\n",
      "(252, tensor(1.8491, grad_fn=<MseLossBackward>))\n",
      "(253, tensor(3.7225, grad_fn=<MseLossBackward>))\n",
      "(254, tensor(3.4588, grad_fn=<MseLossBackward>))\n",
      "(255, tensor(2.7630, grad_fn=<MseLossBackward>))\n",
      "(256, tensor(2.6307, grad_fn=<MseLossBackward>))\n",
      "(257, tensor(1.8012, grad_fn=<MseLossBackward>))\n",
      "(258, tensor(5.2708, grad_fn=<MseLossBackward>))\n",
      "(259, tensor(2.0239, grad_fn=<MseLossBackward>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, tensor(0.6219, grad_fn=<MseLossBackward>))\n",
      "(261, tensor(2.0048, grad_fn=<MseLossBackward>))\n",
      "(262, tensor(0.5717, grad_fn=<MseLossBackward>))\n",
      "(263, tensor(3.4727, grad_fn=<MseLossBackward>))\n",
      "(264, tensor(0.3729, grad_fn=<MseLossBackward>))\n",
      "(265, tensor(2.4041, grad_fn=<MseLossBackward>))\n",
      "(266, tensor(2.4119, grad_fn=<MseLossBackward>))\n",
      "(267, tensor(2.2901, grad_fn=<MseLossBackward>))\n",
      "(268, tensor(2.1943, grad_fn=<MseLossBackward>))\n",
      "(269, tensor(2.2494, grad_fn=<MseLossBackward>))\n",
      "(270, tensor(0.9426, grad_fn=<MseLossBackward>))\n",
      "(271, tensor(1.1624, grad_fn=<MseLossBackward>))\n",
      "(272, tensor(2.6328, grad_fn=<MseLossBackward>))\n",
      "(273, tensor(0.8579, grad_fn=<MseLossBackward>))\n",
      "(274, tensor(0.6745, grad_fn=<MseLossBackward>))\n",
      "(275, tensor(1.0851, grad_fn=<MseLossBackward>))\n",
      "(276, tensor(0.9333, grad_fn=<MseLossBackward>))\n",
      "(277, tensor(3.3892, grad_fn=<MseLossBackward>))\n",
      "(278, tensor(0.6431, grad_fn=<MseLossBackward>))\n",
      "(279, tensor(0.6612, grad_fn=<MseLossBackward>))\n",
      "(280, tensor(0.6281, grad_fn=<MseLossBackward>))\n",
      "(281, tensor(3.8207, grad_fn=<MseLossBackward>))\n",
      "(282, tensor(0.4496, grad_fn=<MseLossBackward>))\n",
      "(283, tensor(0.4846, grad_fn=<MseLossBackward>))\n",
      "(284, tensor(0.7267, grad_fn=<MseLossBackward>))\n",
      "(285, tensor(0.7557, grad_fn=<MseLossBackward>))\n",
      "(286, tensor(2.0869, grad_fn=<MseLossBackward>))\n",
      "(287, tensor(0.2764, grad_fn=<MseLossBackward>))\n",
      "(288, tensor(1.8957, grad_fn=<MseLossBackward>))\n",
      "(289, tensor(1.7311, grad_fn=<MseLossBackward>))\n",
      "(290, tensor(1.4594, grad_fn=<MseLossBackward>))\n",
      "(291, tensor(0.4137, grad_fn=<MseLossBackward>))\n",
      "(292, tensor(2.6562, grad_fn=<MseLossBackward>))\n",
      "(293, tensor(0.8203, grad_fn=<MseLossBackward>))\n",
      "(294, tensor(1.0628, grad_fn=<MseLossBackward>))\n",
      "(295, tensor(2.4813, grad_fn=<MseLossBackward>))\n",
      "(296, tensor(1.0494, grad_fn=<MseLossBackward>))\n",
      "(297, tensor(2.1571, grad_fn=<MseLossBackward>))\n",
      "(298, tensor(0.7459, grad_fn=<MseLossBackward>))\n",
      "(299, tensor(1.8660, grad_fn=<MseLossBackward>))\n",
      "(300, tensor(1.0591, grad_fn=<MseLossBackward>))\n",
      "(301, tensor(1.0277, grad_fn=<MseLossBackward>))\n",
      "(302, tensor(1.5279, grad_fn=<MseLossBackward>))\n",
      "(303, tensor(0.6239, grad_fn=<MseLossBackward>))\n",
      "(304, tensor(0.5439, grad_fn=<MseLossBackward>))\n",
      "(305, tensor(0.8179, grad_fn=<MseLossBackward>))\n",
      "(306, tensor(1.5366, grad_fn=<MseLossBackward>))\n",
      "(307, tensor(0.2971, grad_fn=<MseLossBackward>))\n",
      "(308, tensor(0.7438, grad_fn=<MseLossBackward>))\n",
      "(309, tensor(0.2382, grad_fn=<MseLossBackward>))\n",
      "(310, tensor(0.2145, grad_fn=<MseLossBackward>))\n",
      "(311, tensor(0.7503, grad_fn=<MseLossBackward>))\n",
      "(312, tensor(1.5653, grad_fn=<MseLossBackward>))\n",
      "(313, tensor(1.7342, grad_fn=<MseLossBackward>))\n",
      "(314, tensor(1.4493, grad_fn=<MseLossBackward>))\n",
      "(315, tensor(0.6537, grad_fn=<MseLossBackward>))\n",
      "(316, tensor(0.2330, grad_fn=<MseLossBackward>))\n",
      "(317, tensor(1.1871, grad_fn=<MseLossBackward>))\n",
      "(318, tensor(1.1259, grad_fn=<MseLossBackward>))\n",
      "(319, tensor(0.1992, grad_fn=<MseLossBackward>))\n",
      "(320, tensor(1.4487, grad_fn=<MseLossBackward>))\n",
      "(321, tensor(0.1812, grad_fn=<MseLossBackward>))\n",
      "(322, tensor(1.3067, grad_fn=<MseLossBackward>))\n",
      "(323, tensor(0.1731, grad_fn=<MseLossBackward>))\n",
      "(324, tensor(0.1691, grad_fn=<MseLossBackward>))\n",
      "(325, tensor(1.0439, grad_fn=<MseLossBackward>))\n",
      "(326, tensor(0.9428, grad_fn=<MseLossBackward>))\n",
      "(327, tensor(0.2062, grad_fn=<MseLossBackward>))\n",
      "(328, tensor(0.2289, grad_fn=<MseLossBackward>))\n",
      "(329, tensor(0.2260, grad_fn=<MseLossBackward>))\n",
      "(330, tensor(0.2014, grad_fn=<MseLossBackward>))\n",
      "(331, tensor(0.6534, grad_fn=<MseLossBackward>))\n",
      "(332, tensor(0.1563, grad_fn=<MseLossBackward>))\n",
      "(333, tensor(2.0922, grad_fn=<MseLossBackward>))\n",
      "(334, tensor(1.3169, grad_fn=<MseLossBackward>))\n",
      "(335, tensor(0.2107, grad_fn=<MseLossBackward>))\n",
      "(336, tensor(0.2722, grad_fn=<MseLossBackward>))\n",
      "(337, tensor(1.8326, grad_fn=<MseLossBackward>))\n",
      "(338, tensor(2.4296, grad_fn=<MseLossBackward>))\n",
      "(339, tensor(1.1107, grad_fn=<MseLossBackward>))\n",
      "(340, tensor(0.8728, grad_fn=<MseLossBackward>))\n",
      "(341, tensor(1.1002, grad_fn=<MseLossBackward>))\n",
      "(342, tensor(1.2183, grad_fn=<MseLossBackward>))\n",
      "(343, tensor(1.1713, grad_fn=<MseLossBackward>))\n",
      "(344, tensor(1.5978, grad_fn=<MseLossBackward>))\n",
      "(345, tensor(0.7308, grad_fn=<MseLossBackward>))\n",
      "(346, tensor(0.8297, grad_fn=<MseLossBackward>))\n",
      "(347, tensor(2.0325, grad_fn=<MseLossBackward>))\n",
      "(348, tensor(0.5125, grad_fn=<MseLossBackward>))\n",
      "(349, tensor(0.1616, grad_fn=<MseLossBackward>))\n",
      "(350, tensor(1.6459, grad_fn=<MseLossBackward>))\n",
      "(351, tensor(0.1693, grad_fn=<MseLossBackward>))\n",
      "(352, tensor(0.9308, grad_fn=<MseLossBackward>))\n",
      "(353, tensor(0.6380, grad_fn=<MseLossBackward>))\n",
      "(354, tensor(1.0868, grad_fn=<MseLossBackward>))\n",
      "(355, tensor(0.9154, grad_fn=<MseLossBackward>))\n",
      "(356, tensor(0.8103, grad_fn=<MseLossBackward>))\n",
      "(357, tensor(0.3957, grad_fn=<MseLossBackward>))\n",
      "(358, tensor(0.8498, grad_fn=<MseLossBackward>))\n",
      "(359, tensor(1.3268, grad_fn=<MseLossBackward>))\n",
      "(360, tensor(0.5401, grad_fn=<MseLossBackward>))\n",
      "(361, tensor(1.1286, grad_fn=<MseLossBackward>))\n",
      "(362, tensor(0.7392, grad_fn=<MseLossBackward>))\n",
      "(363, tensor(0.6712, grad_fn=<MseLossBackward>))\n",
      "(364, tensor(0.1486, grad_fn=<MseLossBackward>))\n",
      "(365, tensor(0.1285, grad_fn=<MseLossBackward>))\n",
      "(366, tensor(0.6288, grad_fn=<MseLossBackward>))\n",
      "(367, tensor(1.1440, grad_fn=<MseLossBackward>))\n",
      "(368, tensor(1.0292, grad_fn=<MseLossBackward>))\n",
      "(369, tensor(0.6375, grad_fn=<MseLossBackward>))\n",
      "(370, tensor(0.9454, grad_fn=<MseLossBackward>))\n",
      "(371, tensor(0.8915, grad_fn=<MseLossBackward>))\n",
      "(372, tensor(0.8415, grad_fn=<MseLossBackward>))\n",
      "(373, tensor(0.7913, grad_fn=<MseLossBackward>))\n",
      "(374, tensor(0.1562, grad_fn=<MseLossBackward>))\n",
      "(375, tensor(0.7173, grad_fn=<MseLossBackward>))\n",
      "(376, tensor(0.1567, grad_fn=<MseLossBackward>))\n",
      "(377, tensor(0.1357, grad_fn=<MseLossBackward>))\n",
      "(378, tensor(0.7064, grad_fn=<MseLossBackward>))\n",
      "(379, tensor(0.0897, grad_fn=<MseLossBackward>))\n",
      "(380, tensor(1.2513, grad_fn=<MseLossBackward>))\n",
      "(381, tensor(0.0906, grad_fn=<MseLossBackward>))\n",
      "(382, tensor(0.8796, grad_fn=<MseLossBackward>))\n",
      "(383, tensor(0.7376, grad_fn=<MseLossBackward>))\n",
      "(384, tensor(0.7774, grad_fn=<MseLossBackward>))\n",
      "(385, tensor(0.2646, grad_fn=<MseLossBackward>))\n",
      "(386, tensor(0.4910, grad_fn=<MseLossBackward>))\n",
      "(387, tensor(0.7347, grad_fn=<MseLossBackward>))\n",
      "(388, tensor(0.3740, grad_fn=<MseLossBackward>))\n",
      "(389, tensor(1.5337, grad_fn=<MseLossBackward>))\n",
      "(390, tensor(0.2582, grad_fn=<MseLossBackward>))\n",
      "(391, tensor(1.4215, grad_fn=<MseLossBackward>))\n",
      "(392, tensor(0.2068, grad_fn=<MseLossBackward>))\n",
      "(393, tensor(1.1215, grad_fn=<MseLossBackward>))\n",
      "(394, tensor(0.1322, grad_fn=<MseLossBackward>))\n",
      "(395, tensor(0.6118, grad_fn=<MseLossBackward>))\n",
      "(396, tensor(0.8756, grad_fn=<MseLossBackward>))\n",
      "(397, tensor(0.1285, grad_fn=<MseLossBackward>))\n",
      "(398, tensor(0.1300, grad_fn=<MseLossBackward>))\n",
      "(399, tensor(0.6956, grad_fn=<MseLossBackward>))\n",
      "(400, tensor(0.1227, grad_fn=<MseLossBackward>))\n",
      "(401, tensor(1.1989, grad_fn=<MseLossBackward>))\n",
      "(402, tensor(0.7176, grad_fn=<MseLossBackward>))\n",
      "(403, tensor(0.0871, grad_fn=<MseLossBackward>))\n",
      "(404, tensor(0.0806, grad_fn=<MseLossBackward>))\n",
      "(405, tensor(0.0753, grad_fn=<MseLossBackward>))\n",
      "(406, tensor(0.0700, grad_fn=<MseLossBackward>))\n",
      "(407, tensor(0.6923, grad_fn=<MseLossBackward>))\n",
      "(408, tensor(0.6242, grad_fn=<MseLossBackward>))\n",
      "(409, tensor(0.5932, grad_fn=<MseLossBackward>))\n",
      "(410, tensor(0.8224, grad_fn=<MseLossBackward>))\n",
      "(411, tensor(0.0682, grad_fn=<MseLossBackward>))\n",
      "(412, tensor(0.7053, grad_fn=<MseLossBackward>))\n",
      "(413, tensor(0.7767, grad_fn=<MseLossBackward>))\n",
      "(414, tensor(0.0721, grad_fn=<MseLossBackward>))\n",
      "(415, tensor(0.6620, grad_fn=<MseLossBackward>))\n",
      "(416, tensor(0.6387, grad_fn=<MseLossBackward>))\n",
      "(417, tensor(0.5522, grad_fn=<MseLossBackward>))\n",
      "(418, tensor(0.4508, grad_fn=<MseLossBackward>))\n",
      "(419, tensor(0.1540, grad_fn=<MseLossBackward>))\n",
      "(420, tensor(0.1790, grad_fn=<MseLossBackward>))\n",
      "(421, tensor(0.3143, grad_fn=<MseLossBackward>))\n",
      "(422, tensor(0.1831, grad_fn=<MseLossBackward>))\n",
      "(423, tensor(1.5015, grad_fn=<MseLossBackward>))\n",
      "(424, tensor(0.2208, grad_fn=<MseLossBackward>))\n",
      "(425, tensor(0.7230, grad_fn=<MseLossBackward>))\n",
      "(426, tensor(1.2007, grad_fn=<MseLossBackward>))\n",
      "(427, tensor(0.3411, grad_fn=<MseLossBackward>))\n",
      "(428, tensor(0.9325, grad_fn=<MseLossBackward>))\n",
      "(429, tensor(0.0892, grad_fn=<MseLossBackward>))\n",
      "(430, tensor(0.6204, grad_fn=<MseLossBackward>))\n",
      "(431, tensor(0.6080, grad_fn=<MseLossBackward>))\n",
      "(432, tensor(0.1027, grad_fn=<MseLossBackward>))\n",
      "(433, tensor(0.5527, grad_fn=<MseLossBackward>))\n",
      "(434, tensor(0.0847, grad_fn=<MseLossBackward>))\n",
      "(435, tensor(0.5722, grad_fn=<MseLossBackward>))\n",
      "(436, tensor(0.7537, grad_fn=<MseLossBackward>))\n",
      "(437, tensor(0.7110, grad_fn=<MseLossBackward>))\n",
      "(438, tensor(0.6856, grad_fn=<MseLossBackward>))\n",
      "(439, tensor(0.6484, grad_fn=<MseLossBackward>))\n",
      "(440, tensor(0.6255, grad_fn=<MseLossBackward>))\n",
      "(441, tensor(0.0817, grad_fn=<MseLossBackward>))\n",
      "(442, tensor(0.5217, grad_fn=<MseLossBackward>))\n",
      "(443, tensor(0.6572, grad_fn=<MseLossBackward>))\n",
      "(444, tensor(0.6400, grad_fn=<MseLossBackward>))\n",
      "(445, tensor(0.6210, grad_fn=<MseLossBackward>))\n",
      "(446, tensor(0.0673, grad_fn=<MseLossBackward>))\n",
      "(447, tensor(0.6390, grad_fn=<MseLossBackward>))\n",
      "(448, tensor(0.5803, grad_fn=<MseLossBackward>))\n",
      "(449, tensor(0.4799, grad_fn=<MseLossBackward>))\n",
      "(450, tensor(0.5681, grad_fn=<MseLossBackward>))\n",
      "(451, tensor(0.5190, grad_fn=<MseLossBackward>))\n",
      "(452, tensor(0.4843, grad_fn=<MseLossBackward>))\n",
      "(453, tensor(0.4294, grad_fn=<MseLossBackward>))\n",
      "(454, tensor(0.6208, grad_fn=<MseLossBackward>))\n",
      "(455, tensor(0.3237, grad_fn=<MseLossBackward>))\n",
      "(456, tensor(0.1479, grad_fn=<MseLossBackward>))\n",
      "(457, tensor(0.6191, grad_fn=<MseLossBackward>))\n",
      "(458, tensor(0.5936, grad_fn=<MseLossBackward>))\n",
      "(459, tensor(0.2733, grad_fn=<MseLossBackward>))\n",
      "(460, tensor(0.5188, grad_fn=<MseLossBackward>))\n",
      "(461, tensor(0.1513, grad_fn=<MseLossBackward>))\n",
      "(462, tensor(0.9408, grad_fn=<MseLossBackward>))\n",
      "(463, tensor(0.3281, grad_fn=<MseLossBackward>))\n",
      "(464, tensor(0.4274, grad_fn=<MseLossBackward>))\n",
      "(465, tensor(0.4126, grad_fn=<MseLossBackward>))\n",
      "(466, tensor(0.3904, grad_fn=<MseLossBackward>))\n",
      "(467, tensor(0.3635, grad_fn=<MseLossBackward>))\n",
      "(468, tensor(0.3232, grad_fn=<MseLossBackward>))\n",
      "(469, tensor(0.2856, grad_fn=<MseLossBackward>))\n",
      "(470, tensor(0.9308, grad_fn=<MseLossBackward>))\n",
      "(471, tensor(0.5687, grad_fn=<MseLossBackward>))\n",
      "(472, tensor(0.8894, grad_fn=<MseLossBackward>))\n",
      "(473, tensor(0.2718, grad_fn=<MseLossBackward>))\n",
      "(474, tensor(0.5421, grad_fn=<MseLossBackward>))\n",
      "(475, tensor(0.7093, grad_fn=<MseLossBackward>))\n",
      "(476, tensor(0.1384, grad_fn=<MseLossBackward>))\n",
      "(477, tensor(0.1336, grad_fn=<MseLossBackward>))\n",
      "(478, tensor(0.4593, grad_fn=<MseLossBackward>))\n",
      "(479, tensor(0.4992, grad_fn=<MseLossBackward>))\n",
      "(480, tensor(0.4126, grad_fn=<MseLossBackward>))\n",
      "(481, tensor(0.4187, grad_fn=<MseLossBackward>))\n",
      "(482, tensor(0.3792, grad_fn=<MseLossBackward>))\n",
      "(483, tensor(0.7733, grad_fn=<MseLossBackward>))\n",
      "(484, tensor(0.7399, grad_fn=<MseLossBackward>))\n",
      "(485, tensor(0.6703, grad_fn=<MseLossBackward>))\n",
      "(486, tensor(0.2625, grad_fn=<MseLossBackward>))\n",
      "(487, tensor(0.5819, grad_fn=<MseLossBackward>))\n",
      "(488, tensor(0.2529, grad_fn=<MseLossBackward>))\n",
      "(489, tensor(0.4928, grad_fn=<MseLossBackward>))\n",
      "(490, tensor(0.6312, grad_fn=<MseLossBackward>))\n",
      "(491, tensor(0.4673, grad_fn=<MseLossBackward>))\n",
      "(492, tensor(0.1677, grad_fn=<MseLossBackward>))\n",
      "(493, tensor(0.5755, grad_fn=<MseLossBackward>))\n",
      "(494, tensor(0.3195, grad_fn=<MseLossBackward>))\n",
      "(495, tensor(0.4864, grad_fn=<MseLossBackward>))\n",
      "(496, tensor(0.4933, grad_fn=<MseLossBackward>))\n",
      "(497, tensor(0.4008, grad_fn=<MseLossBackward>))\n",
      "(498, tensor(0.1248, grad_fn=<MseLossBackward>))\n",
      "(499, tensor(0.3306, grad_fn=<MseLossBackward>))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_layer = torch.nn.Linear(D_in, H)\n",
    "        self.middle_layer = torch.nn.Linear(H, H)\n",
    "        self.output_layer = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_layer(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_layer(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_layer(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
